var documenterSearchIndex = {"docs":
[{"location":"optimization/#optim","page":"Optimization","title":"Optimization","text":"","category":"section"},{"location":"optimization/#Parameter-Free-Optimization-Rules","page":"Optimization","title":"Parameter-Free Optimization Rules","text":"We provide custom optimization rules that are not provided out-of-the-box by Optimisers.jl. The main theme of the provided optimizers is that they are parameter-free. This means that these optimization rules shouldn't require (or barely) any tuning to obtain performance competitive with well-tuned alternatives.","category":"section"},{"location":"optimization/#Parameter-Averaging-Strategies","page":"Optimization","title":"Parameter Averaging Strategies","text":"In some cases, the best optimization performance is obtained by averaging the sequence of parameters generated by the optimization algorithm. For instance, the DoG[IHC2023] and DoWG[KMJ2024] papers report their best performance through averaging. The benefits of parameter averaging have been specifically confirmed for ELBO maximization[DCAMHV2020].\n\n[DCAMHV2020]: Dhaka, A. K., Catalina, A., Andersen, M. R., Magnusson, M., Huggins, J., & Vehtari, A. (2020). Robust, accurate stochastic optimization for variational inference. Advances in Neural Information Processing Systems, 33, 10961-10973.\n\n[KMJ2024]: Khaled, A., Mishchenko, K., & Jin, C. (2023). Dowg unleashed: An efficient universal parameter-free gradient descent method. Advances in Neural Information Processing Systems, 36, 6748-6769.\n\n[IHC2023]: Ivgi, M., Hinder, O., & Carmon, Y. (2023). Dog is sgd's best friend: A parameter-free dynamic step size schedule. In International Conference on Machine Learning (pp. 14465-14499). PMLR.","category":"section"},{"location":"optimization/#Operators","page":"Optimization","title":"Operators","text":"Depending on the variational family, variational objective, and optimization strategy, it might be necessary to modify the variational parameters after performing a gradient-based update. For this, an operator acting on the parameters can be supplied via the  operator keyword argument of AdvancedVI.optimize.","category":"section"},{"location":"optimization/#clipscale","page":"Optimization","title":"ClipScale","text":"For the location-scale family, it is often the case that optimization is stable only when the smallest eigenvalue of the scale matrix is strictly positive[D2020]. To ensure this, we provide the following projection operator:","category":"section"},{"location":"optimization/#proximalocationscaleentropy","page":"Optimization","title":"ProximalLocationScaleEntropy","text":"ELBO maximization with the location-scale family tends to be unstable when the scale has small eigenvalues or the stepsize is large. To remedy this, a proximal operator of the entropy[D2020] can be used.\n\n[D2020]: Domke, J. (2020). Provable smoothness guarantees for black-box variational inference. In International Conference on Machine Learning.","category":"section"},{"location":"optimization/#AdvancedVI.DoG","page":"Optimization","title":"AdvancedVI.DoG","text":"DoG(alpha)\n\nDistance over gradient (DoG[IHC2023]) optimizer. Its only parameter is the guess for the distance between the optimum and the initialization alpha, which shouldn't need much tuning.\n\nDoG works by starting from a AdaGrad-like update rule with a small step size, but then automatically increases the step size (\"warming up\") to be as large as possible. If alpha is too large, the optimzier can initially diverge, while if it is too small, the warm up period can be too long.\n\nParameters\n\nalpha: Scaling factor for initial guess (repsilon in the original paper) of the Euclidean distance between the initial point and the optimum. For the initial parameter lambda0, repsilon is calculated as repsilon = alpha*(1 + norm(lambda0)). (default value: 1e-6)\n\n\n\n\n\n","category":"type"},{"location":"optimization/#AdvancedVI.DoWG","page":"Optimization","title":"AdvancedVI.DoWG","text":"DoWG(alpha)\n\nDistance over weighted gradient (DoWG[KMJ2024]) optimizer. Its only parameter is the guess for the distance between the optimum and the initialization alpha, which shouldn't need much tuning.\n\nDoWG is a minor modification to DoG so that the step sizes are always provably larger than DoG. Similarly to DoG, it works by starting from a AdaGrad-like update rule with a small step size, but then automatically increases the step size (\"warming up\") to be as large as possible. If alpha is too large, the optimzier can initially diverge, while if it is too small, the warm up period can be too long. Depending on the problem, DoWG can be too aggressive and result in unstable behavior. If this is suspected, try using DoG instead.\n\nParameters\n\nalpha: Scaling factor for initial guess (repsilon in the original paper) of the Euclidean distance between the initial point and the optimum. For the initial parameter lambda0, repsilon is calculated as repsilon = alpha*(1 + norm(lambda0)). (default value: 1e-6)\n\n\n\n\n\n","category":"type"},{"location":"optimization/#AdvancedVI.COCOB","page":"Optimization","title":"AdvancedVI.COCOB","text":"COCOB(alpha)\n\nContinuous Coin Betting (COCOB[OT2017]) optimizer. We use the \"COCOB-Backprop\" variant, which is closer to the Adam optimizer. Its only parameter is the maximum change per parameter alpha, which shouldn't need much tuning.\n\nParameters\n\nalpha: Scaling parameter. (default value: 100)\n\n[OT2017]: Orabona, F., & Tommasi, T. (2017). Training deep networks without learning rates through coin betting. Advances in Neural Information Processing Systems, 30.\n\n\n\n\n\n","category":"type"},{"location":"optimization/#AdvancedVI.NoAveraging","page":"Optimization","title":"AdvancedVI.NoAveraging","text":"NoAveraging()\n\nNo averaging. This returns the last-iterate of the optimization rule.\n\n\n\n\n\n","category":"type"},{"location":"optimization/#AdvancedVI.PolynomialAveraging","page":"Optimization","title":"AdvancedVI.PolynomialAveraging","text":"PolynomialAveraging(eta)\n\nPolynomial averaging rule proposed Shamir and Zhang[SZ2013]. At iteration t, the parameter average $ \\bar{\\lambda}_t $ according to the polynomial averaging rule is given as\n\n    barlambda_t = (1 - w_t) barlambda_t-1 + w_t lambda_t  \n\nwhere the averaging weight is \n\n    w_t = fraceta + 1t + eta  \n\nHigher eta (eta) down-weights earlier iterations. When eta=0, this is equivalent to uniformly averaging the iterates in an online fashion. The DoG paper[IHC2023] suggests eta=8.\n\nParameters\n\neta: Regularization term. (default: 8)\n\n[SZ2013]: Shamir, O., & Zhang, T. (2013). Stochastic gradient descent for non-smooth optimization: Convergence results and optimal averaging schemes. In International conference on machine learning (pp. 71-79). PMLR.\n\n\n\n\n\n","category":"type"},{"location":"optimization/#AdvancedVI.ClipScale","page":"Optimization","title":"AdvancedVI.ClipScale","text":"ClipScale(ϵ = 1e-5)\n\nProjection operator ensuring that an MvLocationScale or MvLocationScaleLowRank has a scale with eigenvalues larger than ϵ. ClipScale also supports by operating on MvLocationScale and MvLocationScaleLowRank wrapped by a Bijectors.TransformedDistribution object. \n\n\n\n\n\n","category":"type"},{"location":"optimization/#AdvancedVI.ProximalLocationScaleEntropy","page":"Optimization","title":"AdvancedVI.ProximalLocationScaleEntropy","text":"ProximalLocationScaleEntropy()\n\nProximal operator for the entropy of a location-scale distribution, which is defined as\n\n    mathrmprox(lambda) = argmin_lambda^prime - mathbbH(q_lambda^prime) + frac12 gamma_t leftlVert lambda - lambda^prime rightrVert_2^2 \n\nwhere gamma_t is the stepsize the optimizer used with the proximal operator. This assumes the variational family is <:VILocationScale and the optimizer is one of the following:\n\nDoG\nDoWG\nDescent\n\nFor ELBO maximization, since this proximal operator handles the entropy, the gradient estimator for the ELBO must ignore the entropy term. That is, the entropy keyword argument of RepGradELBO muse be one of the following:\n\nClosedFormEntropyZeroGradient\nStickingTheLandingEntropyZeroGradient\n\n\n\n\n\n","category":"type"},{"location":"families/#families","page":"Variational Families","title":"Reparameterizable Variational Families","text":"Algorithms such as KLMinRepGradELBO assume that the members of the variational family have a differentiable sampling path. We provide multiple pre-packaged variational families that can be readily used.","category":"section"},{"location":"families/#locscale","page":"Variational Families","title":"The LocationScale Family","text":"The location-scale variational family is a family of probability distributions, where their sampling process can be represented as\n\nz sim  q_lambda qquadLeftrightarrowqquad\nz stackreld= C u + mquad u sim varphi\n\nwhere C is the scale, m is the location, and varphi is the base distribution. m and C form the variational parameters lambda = (m C) of q_lambda. The location-scale family encompasses many practical variational families, which can be instantiated by setting the base distribution of u and the structure of C.\n\nThe probability density is given by\n\n  q_lambda(z) = C^-1 varphi(C^-1(z - m))\n\nthe covariance is given as\n\n  mathrmVarleft(q_lambdaright) = C mathrmVar(varphi) C^top\n\nand the entropy is given as\n\n  mathbbH(q_lambda) = mathbbH(varphi) + log C\n\nwhere mathbbH(varphi) is the entropy of the base distribution. Notice that mathbbH(varphi) does not depend on the variational parameters lambda. The derivative of the entropy with respect to lambda is thus independent of the base distribution.","category":"section"},{"location":"families/#API","page":"Variational Families","title":"API","text":"note: Note\nFor stable convergence, the initial scale needs to be sufficiently large and well-conditioned. Initializing scale to have small eigenvalues will often result in initial divergences and numerical instabilities.\n\nThe following are specialized constructors for convenience:","category":"section"},{"location":"families/#Gaussian-Variational-Families","page":"Variational Families","title":"Gaussian Variational Families","text":"using AdvancedVI, LinearAlgebra, Distributions;\nμ = zeros(2);\n\nL = LowerTriangular(diagm(ones(2)));\nq = FullRankGaussian(μ, L)\n\nL = Diagonal(ones(2));\nq = MeanFieldGaussian(μ, L)","category":"section"},{"location":"families/#Student-t-Variational-Families","page":"Variational Families","title":"Student-t Variational Families","text":"using AdvancedVI, LinearAlgebra, Distributions;\nμ = zeros(2);\nν = 3;\n\n# Full-Rank \nL = LowerTriangular(diagm(ones(2)));\nq = MvLocationScale(μ, L, TDist(ν))\n\n# Mean-Field\nL = Diagonal(ones(2));\nq = MvLocationScale(μ, L, TDist(ν))","category":"section"},{"location":"families/#Laplace-Variational-families","page":"Variational Families","title":"Laplace Variational families","text":"using AdvancedVI, LinearAlgebra, Distributions;\nμ = zeros(2);\n\n# Full-Rank \nL = LowerTriangular(diagm(ones(2)));\nq = MvLocationScale(μ, L, Laplace())\n\n# Mean-Field\nL = Diagonal(ones(2));\nq = MvLocationScale(μ, L, Laplace())","category":"section"},{"location":"families/#The-LocationScaleLowRank-Family","page":"Variational Families","title":"The LocationScaleLowRank Family","text":"In practice, LocationScale families with full-rank scale matrices are known to converge slowly as they require a small SGD stepsize. Low-rank variational families can be an effective alternative[ONS2018]. LocationScaleLowRank generally represent any d-dimensional distribution which its sampling path can be represented as\n\nz sim  q_lambda qquadLeftrightarrowqquad\nz stackreld= D u_1 + U u_2  + mquad u_1 u_2 sim varphi\n\nwhere D in mathbbR^d times d is a diagonal matrix, U in mathbbR^d times r is a dense low-rank matrix for the rank r  0, m in mathbbR^d is the location, and varphi is the base distribution. m, D, and U form the variational parameters lambda = (m D U).\n\nThe covariance of this distribution is given as\n\n  mathrmVarleft(q_lambdaright) = D mathrmVar(varphi) D + U mathrmVar(varphi) U^top\n\nand the entropy is given by the matrix determinant lemma as\n\n  mathbbH(q_lambda) \n  = mathbbH(varphi) + log Sigma\n  = mathbbH(varphi) + 2 log D + log I + U^top D^-2 U\n\nwhere mathbbH(varphi) is the entropy of the base distribution.\n\nConsider a 30-dimensional Gaussian with a diagonal plus low-rank covariance structure, where the true rank is 3. Then, we can compare the convergence speed of LowRankGaussian versus FullRankGaussian:\n\n(Image: )\n\nAs we can see, LowRankGaussian converges faster than FullRankGaussian. While FullRankGaussian can converge to the true solution since it is a more expressive variational family, LowRankGaussian gets there faster.\n\ninfo: Info\nMvLocationScaleLowRank tend to work better with the Optimisers.Adam optimizer due to non-smoothness. Other optimisers may experience divergences.","category":"section"},{"location":"families/#API-2","page":"Variational Families","title":"API","text":"The logpdf of  MvLocationScaleLowRank has an optional argument non_differentiable::Bool (default: false). If set as true, a more efficient Oleft(r d^2right) implementation is used to evaluate the density. This, however, is not differentiable under most AD frameworks due to the use of Cholesky lowrankupdate. The default value is false, which uses a Oleft(d^3right) implementation, is differentiable and therefore compatible with the StickingTheLandingEntropy estimator.\n\nThe following is a specialized constructor for convenience:\n\n[ONS2018]: Ong, V. M. H., Nott, D. J., & Smith, M. S. (2018). Gaussian variational approximation with a factor covariance structure. Journal of Computational and Graphical Statistics, 27(3), 465-478.","category":"section"},{"location":"families/#AdvancedVI.MvLocationScale","page":"Variational Families","title":"AdvancedVI.MvLocationScale","text":"MvLocationScale(location, scale, dist)\n\nThe location scale variational family broadly represents various variational families using location and scale variational parameters.\n\nIt generally represents any distribution for which the sampling path can be represented as follows:\n\n  d = length(location)\n  u = rand(dist, d)\n  z = scale*u + location\n\n\n\n\n\n","category":"type"},{"location":"families/#AdvancedVI.FullRankGaussian","page":"Variational Families","title":"AdvancedVI.FullRankGaussian","text":"FullRankGaussian(μ, L)\n\nConstruct a Gaussian variational approximation with a dense covariance matrix.\n\nArguments\n\nμ::AbstractVector{T}: Mean of the Gaussian.\nL::LinearAlgebra.AbstractTriangular{T}: Cholesky factor of the covariance of the Gaussian.\n\n\n\n\n\n","category":"function"},{"location":"families/#AdvancedVI.MeanFieldGaussian","page":"Variational Families","title":"AdvancedVI.MeanFieldGaussian","text":"MeanFieldGaussian(μ, L)\n\nConstruct a Gaussian variational approximation with a diagonal covariance matrix.\n\nArguments\n\nμ::AbstractVector{T}: Mean of the Gaussian.\nL::Diagonal{T}: Diagonal Cholesky factor of the covariance of the Gaussian.\n\n\n\n\n\n","category":"function"},{"location":"families/#AdvancedVI.MvLocationScaleLowRank","page":"Variational Families","title":"AdvancedVI.MvLocationScaleLowRank","text":"MvLocationLowRankScale(location, scale_diag, scale_factors, dist)\n\nVariational family with a covariance in the form of a diagonal matrix plus a squared low-rank matrix. The rank is given by size(scale_factors, 2).\n\nIt generally represents any distribution for which the sampling path can be represented as follows:\n\n  d = length(location)\n  r = size(scale_factors, 2)\n  u_diag = rand(dist, d)\n  u_factors = rand(dist, r)\n  z = scale_diag.*u_diag + scale_factors*u_factors + location\n\n\n\n\n\n","category":"type"},{"location":"families/#AdvancedVI.LowRankGaussian","page":"Variational Families","title":"AdvancedVI.LowRankGaussian","text":"LowRankGaussian(μ, D, U)\n\nConstruct a Gaussian variational approximation with a diagonal plus low-rank covariance matrix.\n\nArguments\n\nμ::AbstractVector{T}: Mean of the Gaussian.\nD::Vector{T}: Diagonal of the scale.\nU::Matrix{T}: Low-rank factors of the scale, where size(U,2) is the rank.\n\n\n\n\n\n","category":"function"},{"location":"tutorials/stan/#Stan-Models","page":"Stan Models","title":"Stan Models","text":"Since AdvancedVI supports the LogDensityProblem interface, it can also be used with Stan models through StanLogDensityProblems interface. Specifically, StanLogDensityProblems wraps any Stan model into a LogDensityProblem using BridgeStan.","category":"section"},{"location":"tutorials/stan/#Problem-Setup","page":"Stan Models","title":"Problem Setup","text":"Recall the hierarchical logistic regression example in the Basic Example. Here, we will define the same model in Stan.\n\nmodel_src = \"\"\"\ndata {\n  int<lower=0> N;\n  int<lower=0> D;\n  matrix[N,D] X;\n  array[N] int<lower=0, upper=1> y;\n}\nparameters {\n  vector[D] beta;\n  real<lower=1e-4> sigma;\n}\nmodel {\n  sigma ~ lognormal(0, 1);\n  beta ~ normal(0, sigma);\n  y ~ bernoulli_logit(X * beta);\n}\n\"\"\"\nnothing\n\nWe also need to prepare the data.\n\nusing DataFrames: DataFrames\nusing OpenML: OpenML\nusing Statistics\n\ndata = Array(DataFrames.DataFrame(OpenML.load(40)))\n\nX = Matrix{Float64}(data[:, 1:(end - 1)])\nX = (X .- mean(X; dims=2)) ./ std(X; dims=2)\nX = hcat(X, ones(size(X, 1)))\ny = Vector{Int}(data[:, end] .== \"Mine\")\n\nstan_data = (X=transpose(X), y=y, N=size(X, 1), D=size(X, 2))\nnothing\n\nSince StanLogDensityProblems expects files for both the model and the data, we need to store both on the file system.\n\nusing JSON: JSON\n\nopen(\"logistic_model.stan\", \"w\") do io\n    println(io, model_src)\nend\nopen(\"logistic_data.json\", \"w\") do io\n    println(io, JSON.json(stan_data))\nend\nnothing","category":"section"},{"location":"tutorials/stan/#Inference-via-AdvancedVI","page":"Stan Models","title":"Inference via AdvancedVI","text":"We can now call StanLogDensityProblems to recieve a LogDensityProblem.\n\nusing StanLogDensityProblems: StanLogDensityProblems\n\nmodel = StanLogDensityProblems.StanProblem(\"logistic_model.stan\", \"logistic_data.json\")\nnothing\n\nThe rest is the same as all LogDensityProblem with the exception of how to deal with constrainted variables: Since StanLogDensityProblems automatically transforms the support of the target problem to be unconstrained, we do not need to involve Bijectors.\n\nusing ADTypes, ReverseDiff\nusing AdvancedVI\nusing LinearAlgebra\nusing LogDensityProblems\nusing Plots\n\nalg = KLMinRepGradDescent(ADTypes.AutoReverseDiff(); operator=ClipScale())\n\nd = LogDensityProblems.dimension(model)\nq = FullRankGaussian(zeros(d), LowerTriangular(Matrix{Float64}(0.37*I, d, d)))\n\nmax_iter = 10^4\nq_out, info, _ = AdvancedVI.optimize(alg, max_iter, model, q; show_progress=false)\n\nplot(\n    [i.iteration for i in info],\n    [i.elbo for i in info];\n    xlabel=\"Iteration\",\n    ylabel=\"ELBO\",\n    label=nothing,\n)\nsavefig(\"stan_example_elbo.svg\")\n\n(Image: )\n\nFrom variational posterior q_out we can draw samples from the unconstrained support of the model. To convert the samples back to the original (constrained) support of the model, it suffices to call BridgeStan.param_constrain.","category":"section"},{"location":"klminnaturalgraddescent/#klminnaturalgraddescent","page":"KLMinNaturalGradDescent","title":"KLMinNaturalGradDescent","text":"","category":"section"},{"location":"klminnaturalgraddescent/#Description","page":"KLMinNaturalGradDescent","title":"Description","text":"This algorithm aims to minimize the exclusive (or reverse) Kullback-Leibler (KL) divergence by running natural gradient descent. KLMinNaturalGradDescent is a specific implementation of natural gradient variational inference (NGVI) also known as variational online Newton[KR2023]. For nearly-Gaussian targets, NGVI tends to converge very quickly. If the ensure_posdef option is set to true (this is the default configuration), then the update rule of [LSK2020] is used, which guarantees the updated precision matrix is always positive definite. Since KLMinNaturalGradDescent is a measure-space algorithm, its use is restricted to full-rank Gaussian variational families (FullRankGaussian) that make the updates tractable.\n\nThe associated objective can be estimated through the following:\n\n[KR2023]: Khan, M. E., & Rue, H. (2023). The Bayesian learning rule. Journal of Machine Learning Research, 24(281), 1-46.\n\n[LSK2020]: Lin, W., Schmidt, M., & Khan, M. E. (2020). Handling the positive-definite constraint in the Bayesian learning rule. In International Conference on Machine Learning. PMLR.","category":"section"},{"location":"klminnaturalgraddescent/#klminnaturalgraddescent_method","page":"KLMinNaturalGradDescent","title":"Methodology","text":"This algorithm aims to solve the problem\n\n  mathrmminimize_q_lambda in mathcalQquad mathrmKLleft(q_lambda piright)\n\nwhere mathcalQ is some family of distributions, often called the variational family, by running stochastic gradient descent in the (Euclidean) space of parameters. That is, for all q_lambda in mathcalQ, we assume q_lambda there is a corresponding vector of parameters lambda in Lambda, where the space of parameters is Euclidean such that Lambda subset mathbbR^p.\n\nSince we usually only have access to the unnormalized densities of the target distribution pi, we don't have direct access to the KL divergence. Instead, the ELBO maximization strategy minimizes a surrogate objective, the negative evidence lower bound[JGJS1999]\n\n  mathcalLleft(qright) triangleq mathbbE_theta sim q -log pileft(thetaright) - mathbbHleft(qright)\n\nwhich is equivalent to the KL up to an additive constant (the evidence).\n\nSuppose we had access to the exact gradients nabla_lambda mathcalLleft(q_lambdaright). NGVI attempts to minimize mathcalL via natural gradient descent, which corresponds to iterating the mirror descent update\n\nlambda_t+1 =  argmin_lambda in Lambda langle nabla_lambda mathcalLleft(q_lambda_tright) lambda - lambda_t rangle + frac12 gamma_t mathrmKLleft(q q_lambda_tright) \n\nThis turns out to be equivalent to the update\n\nlambda_t+1 = lambda_t - gamma_t F(lambda_t)^-1 nabla_lambda mathcalL(q_lambda_t) \n\nwhere F(lambda_t) is the Fisher information matrix of q_lambda. That is, natural gradient descent can be viewed as gradient descent with an iterate-dependent preconditioning. Furthermore, F(lambda_t)^-1 nabla_lambda mathcalL(q_lambda_t) is refered to as the natural gradient of the KL divergence[A1998], hence natural gradient variational inference. Also note that the gradient is taken over the parameters of q_lambda. Therefore, NGVI is parametrization dependent: for the same variational family, different parametrizations will result in different behavior. However, the pseudo-metric mathrmKLleft(q q_lambda_tright) is over measures. Therefore, NGVI tend to behave as a measure-space algorithm, but technically speaking, not a fully measure-space algorithm.\n\nIn practice, we don't have access to nabla_lambda mathcalLleft(q_lambdaright) apart from its unbiased estimate. Regardless, the natural gradient descent/mirror descent updates involving the stochastic estimates have been derived for some variational families. For instance, Gaussian variational families[KR2023] and mixture of exponential families[LKS2019]. As of now, we only implement the Gaussian version.\n\n[LKS2019]: Lin, W., Khan, M. E., & Schmidt, M. (2019). Fast and simple natural-gradient variational inference with mixture of exponential-family approximations. In International Conference on Machine Learning. PMLR.\n\n[A1998]: Amari, S. I. (1998). Natural gradient works efficiently in learning. Neural computation, 10(2), 251-276.\n\n[JGJS1999]: Jordan, M. I., Ghahramani, Z., Jaakkola, T. S., & Saul, L. K. (1999). An introduction to variational methods for graphical models. Machine learning, 37, 183-233.","category":"section"},{"location":"klminnaturalgraddescent/#AdvancedVI.KLMinNaturalGradDescent","page":"KLMinNaturalGradDescent","title":"AdvancedVI.KLMinNaturalGradDescent","text":"KLMinNaturalGradDescent(stepsize, n_samples, ensure_posdef, subsampling)\nKLMinNaturalGradDescent(; stepsize, n_samples, ensure_posdef, subsampling)\n\nKL divergence minimization by running natural gradient descent[KL2017][KR2023], also called variational online Newton. This algorithm can be viewed as an instantiation of mirror descent, where the Bregman divergence is chosen to be the KL divergence.\n\nIf the ensure_posdef argument is true, the algorithm applies the technique by Lin et al.[LSK2020], where the precision matrix update includes an additional term that guarantees positive definiteness. This, however, involves an additional set of matrix-matrix multiplications that could be costly.\n\nThis algorithm requires second-order information about the target. If the target LogDensityProblem has second-order differentiation capability, Hessians are used. Otherwise, if the target has only first-order capability, it will use only gradients but ensure_posdef must be set to true. Also, compared to targets with second-order capability, convergence may be slower and the algorithm will be less robust.\n\n(Keyword) Arguments\n\nstepsize::Float64: Step size.\nn_samples::Int: Number of samples used to estimate the natural gradient. (default: 1)\nensure_posdef::Bool: Ensure that the updated precision preserves positive definiteness. (default: true)\nsubsampling::Union{Nothing,<:AbstractSubsampling}: Optional subsampling strategy.\n\nnote: Note\nThe subsampling strategy is only applied to the target LogDensityProblem but not to the variational approximation q. That is, KLMinNaturalGradDescent does not support amortization or structured variational families.\n\nOutput\n\nq: The last iterate of the algorithm.\n\nCallback Signature\n\nThe callback function supplied to optimize needs to have the following signature:\n\ncallback(; rng, iteration, q, info)\n\nThe keyword arguments are as follows:\n\nrng: Random number generator internally used by the algorithm.\niteration: The index of the current iteration.\nq: Current variational approximation.\ninfo: NamedTuple containing the information generated during the current iteration.\n\nRequirements\n\nThe variational family is FullRankGaussian.\nThe target distribution has unconstrained support.\nThe target LogDensityProblems.logdensity(prob, x) has at least first-order differentiation capability.\n\n\n\n\n\n","category":"type"},{"location":"klminnaturalgraddescent/#AdvancedVI.estimate_objective-Tuple{AbstractRNG, KLMinWassFwdBwd, MvLocationScale, Any}-klminnaturalgraddescent","page":"KLMinNaturalGradDescent","title":"AdvancedVI.estimate_objective","text":"estimate_objective([rng,] alg, q, prob; kwargs...)\n\nEstimate the variational objective to be minimized by the algorithm alg for approximating the target prob with the variational approximation q.\n\nArguments\n\nrng::Random.AbstractRNG: Random number generator.\nalg::AbstractVariationalAlgorithm: Variational inference algorithm.\nprob: The target log-joint likelihood implementing the LogDensityProblem interface.\nq: Variational approximation.\n\nKeyword Arguments\n\nDepending on the algorithm, additional keyword arguments may apply. Please refer to the respective documentation of each algorithm for more info.\n\nReturns\n\nobj_est: Estimate of the objective value.\n\n\n\n\n\n","category":"method"},{"location":"klminrepgradproxdescent/#klminrepgradproxdescent","page":"KLMinRepGradProxDescent","title":"KLMinRepGradProxDescent","text":"","category":"section"},{"location":"klminrepgradproxdescent/#Description","page":"KLMinRepGradProxDescent","title":"Description","text":"This algorithm is a slight variation of KLMinRepGradDescent specialized to location-scale families. Therefore, it also aims to minimize the exclusive (or reverse) Kullback-Leibler (KL) divergence over the space of parameters. But instead, it uses stochastic proximal gradient descent with the proximal operator of the entropy of location-scale variational families as discussed in: [D2020][KMG2024][DGG2023]. The remainder of the section will only discuss details specific to KLMinRepGradProxDescent. Thus, for general usage and additional details, please refer to the docs of KLMinRepGradDescent instead.\n\nThe associated objective value can be estimated through the following:","category":"section"},{"location":"klminrepgradproxdescent/#Methodology","page":"KLMinRepGradProxDescent","title":"Methodology","text":"Recall that KLMinRepGradDescent maximizes the ELBO. Now, the ELBO can be re-written as follows:\n\n  mathrmELBOleft(qright) triangleq mathcalEleft(qright) + mathbbHleft(qright)\n\nwhere\n\n  mathcalEleft(qright) = mathbbE_theta sim q log pileft(thetaright)\n\nis often referred to as the negative energy functional. KLMinRepGradProxDescent attempts to address the fact that minimizing the whole ELBO can be unstable due to non-smoothness of mathbbHleft(qright)[D2020]. For this, KLMinRepGradProxDescent relies on proximal stochastic gradient descent, where the problematic term mathbbHleft(qright) is separately handled via a proximal operator. Specifically, KLMinRepGradProxDescent first estimates the gradient of the energy mathcalEleft(qright) only via the reparameterization gradient estimator. Let us denote this as widehatnabla_lambda mathcalEleft(q_lambdaright). Then KLMinRepGradProxDescent iterates the step\n\n  lambda_t+1 = mathrmprox_-gamma_t mathbbHbig( \n      lambda_t + gamma_t widehatnabla_lambda mathcalE(q_lambda_t)\n  big)  \n\nwhere\n\nmathrmprox_h(lambda_t) \n= argmin_lambda in Lambdaleft \n    h(lambda) + lVert lambda - lambda_t rVert_2^2 \nright\n\nis a proximal operator for the entropy. As long as mathrmprox_-gamma_t mathbbH can be evaluated efficiently, this scheme can side-step the fact that mathbbH(lambda) is difficult to deal with via gradient descent. For location-scale families, it turns out the proximal operator of the entropy can be operated efficiently[D2020], which is implemented as ProximalLocationScaleEntropy. This has been empirically shown to be more robust[D2020][KMG2024].\n\n[D2020]: Domke, J. (2020). Provable smoothness guarantees for black-box variational inference. In International Conference on Machine Learning.\n\n[KMG2024]: Kim, K., Ma, Y., & Gardner, J. (2024). Linear Convergence of Black-Box Variational Inference: Should We Stick the Landing?. In International Conference on Artificial Intelligence and Statistics (pp. 235-243). PMLR.\n\n[DGG2023]: Domke, J., Gower, R., & Garrigos, G. (2023). Provable convergence guarantees for black-box variational inference. Advances in neural information processing systems, 36, 66289-66327.","category":"section"},{"location":"klminrepgradproxdescent/#AdvancedVI.KLMinRepGradProxDescent","page":"KLMinRepGradProxDescent","title":"AdvancedVI.KLMinRepGradProxDescent","text":"KLMinRepGradProxDescent(adtype; entropy_zerograd, optimizer, n_samples, averager)\n\nKL divergence minimization by running stochastic proximal gradient descent with the reparameterization gradient in the Euclidean space of variational parameters of a location-scale family.\n\nThis algorithm only supports subtypes of MvLocationScale. Also, since the stochastic proximal gradient descent does not use the entropy of the gradient, the entropy estimator to be used must have a zero-mean gradient. Thus, only the entropy estimators with a \"ZeroGradient\" suffix are allowed.\n\nArguments\n\nadtype: Automatic differentiation backend. \n\nKeyword Arguments\n\nentropy_zerograd: Estimator of the entropy with a zero-mean gradient to be used. Must be one of ClosedFormEntropyZeroGrad, StickingTheLandingEntropyZeroGrad. (default: ClosedFormEntropyZeroGrad())\noptimizer::Optimisers.AbstractRule: Optimization algorithm to be used. Only DoG, DoWG and Optimisers.Descent are supported. (default: DoWG())\nn_samples::Int: Number of Monte Carlo samples to be used for estimating each gradient.\naverager::AbstractAverager: Parameter averaging strategy. (default: PolynomialAveraging())\nsubsampling::Union{<:Nothing,<:AbstractSubsampling}: Data point subsampling strategy. If nothing, subsampling is not used. (default: nothing)\n\nOutput\n\nq_averaged: The variational approximation formed by the averaged SGD iterates.\n\nCallback Signature\n\nThe callback function supplied to optimize needs to have the following signature:\n\ncallback(; rng, iteration, restructure, params, averaged_params, restructure, gradient)\n\nThe keyword arguments are as follows:\n\nrng: Random number generator internally used by the algorithm.\niteration: The index of the current iteration.\nrestructure: Function that restructures the variational approximation from the variational parameters. Calling restructure(params) reconstructs the current variational approximation. \nparams: Current variational parameters.\naveraged_params: Variational parameters averaged according to the averaging strategy.\ngradient: The estimated (possibly stochastic) gradient.\n\nRequirements\n\nThe variational family is MvLocationScale.\nThe target distribution and the variational approximation have the same support.\nThe target LogDensityProblems.logdensity(prob, x) must be differentiable with respect to x by the selected AD backend.\nAdditonal requirements on q may apply depending on the choice of entropy_zerograd.\n\n\n\n\n\n","category":"type"},{"location":"klminrepgradproxdescent/#AdvancedVI.estimate_objective-Tuple{AbstractRNG, Union{KLMinRepGradDescent, KLMinRepGradProxDescent, KLMinScoreGradDescent}, Any, Any}-klminrepgradproxdescent","page":"KLMinRepGradProxDescent","title":"AdvancedVI.estimate_objective","text":"estimate_objective([rng,] alg, q, prob; n_samples, entropy)\n\nEstimate the negative ELBO of the variational approximation q against the target log-density prob.\n\nArguments\n\nrng::Random.AbstractRNG: Random number generator.\nalg::Union{<:KLMinRepGradDescent,<:KLMinRepGradProxDescent,<:KLMinScoreGradDescent}: Variational inference algorithm.\nq: Variational approximation.\nprob: The target log-joint likelihood implementing the LogDensityProblem interface.\n\nKeyword Arguments\n\nn_samples::Int: Number of Monte Carlo samples for estimating the objective. (default: Same as the the number of samples used for estimating the gradient during optimization.)\nentropy::AbstractEntropyEstimator: Entropy estimator. (default: MonteCarloEntropy())\n\nReturns\n\nobj_est: Estimate of the objective value.\n\n\n\n\n\n","category":"method"},{"location":"tutorials/subsampling/#Scaling-to-Large-Datasets-with-Subsampling","page":"Scaling to Large Datasets","title":"Scaling to Large Datasets with Subsampling","text":"In this tutorial, we will show how to use AdvancedVI on problems with large datasets. Variational inference (VI) has a long and successful history[HBWP2013][TL2014][HBB2010] in large scale inference using (minibatch) subsampling. In this tutorial, we will see how to perform subsampling with KLMinRepGradProxDescent, which was originally described in the paper by Titsias and Lázaro-Gredilla[TL2014]; Kucukelbir et al[KTRGB2017].\n\n[HBB2010]: Hoffman, M., Bach, F., & Blei, D. (2010). Online learning for latent Dirichlet allocation. In Advances in Neural Information Processing Systems, 23.\n\n[HBWP2013]: Hoffman, M. D., Blei, D. M., Wang, C., & Paisley, J. (2013). Stochastic variational inference. Journal of Machine Learning Research, 14(1), 1303-1347.\n\n[TL2014]: Titsias, M., & Lázaro-Gredilla, M. (2014, June). Doubly stochastic variational Bayes for non-conjugate inference. In Proceedings of the International Conference on Machine Learning (pp. 1971-1979). PMLR.\n\n[KTRGB2017]: Kucukelbir, A., Tran, D., Ranganath, R., Gelman, A., & Blei, D. M. (2017). Automatic differentiation variational inference. Journal of Machine Learning Research, 18(14), 1-45.","category":"section"},{"location":"tutorials/subsampling/#Setting-Up-Subsampling","page":"Scaling to Large Datasets","title":"Setting Up Subsampling","text":"We will consider the same hierarchical logistic regression example used in the Basic Example.\n\nusing LogDensityProblems: LogDensityProblems\nusing Distributions\nusing FillArrays\n\nstruct LogReg{XType,YType}\n    X::XType\n    y::YType\n    n_data::Int\nend\n\nfunction LogDensityProblems.logdensity(model::LogReg, θ)\n    (; X, y, n_data) = model\n    n, d = size(X)\n    β, σ = θ[1:size(X, 2)], θ[end]\n\n    logprior_β = logpdf(MvNormal(Zeros(d), σ), β)\n    logprior_σ = logpdf(LogNormal(0, 3), σ)\n\n    logit = X*β\n    loglike_y = mapreduce((li, yi) -> logpdf(BernoulliLogit(li), yi), +, logit, y)\n    return n_data/n*loglike_y + logprior_β + logprior_σ\nend\n\nfunction LogDensityProblems.dimension(model::LogReg)\n    return size(model.X, 2) + 1\nend\n\nfunction LogDensityProblems.capabilities(::Type{<:LogReg})\n    return LogDensityProblems.LogDensityOrder{0}()\nend\nnothing\n\nNotice that, to use subsampling, we need be able to rescale the likelihood strength. That is, for the gradient of the log-density with a batch of data points of size n to be an unbiased estimate of the gradient using the full dataset of size n_data, we need to scale the likelihood by n_data/n. This part is critical to ensure that the algorithm correctly approximates the posterior with the full dataset.\n\nAs usual, we will set up a bijector:\n\nusing Bijectors: Bijectors\n\nfunction Bijectors.bijector(model::LogReg)\n    d = size(model.X, 2)\n    return Bijectors.Stacked(\n        Bijectors.bijector.([MvNormal(Zeros(d), 1.0), LogNormal(0, 3)]),\n        [1:d, (d + 1):(d + 1)],\n    )\nend\nnothing\n\nFor the dataset, we will use one that is larger than that used in the Basic Example. This is to properly assess the advantage of subsampling. In particular, we will utilize the \"Phishing\" dataset[Tan2018], which consists of 10000 data points, each with 48 features. The goal is to predict whether the features of a specific website indicate whether it is a phishing website or a legitimate one. The dataset id on the OpenML repository is 46722.\n\n[Tan2018]: Tan, Choon Lin (2018), \"Phishing Dataset for Machine Learning: Feature Evaluation\", Mendeley Data, V1, doi: 10.17632/h3cgnj8hft.1]\n\nusing OpenML: OpenML\nusing DataFrames: DataFrames\n\ndata = Array(DataFrames.DataFrame(OpenML.load(46722)))\nX = Matrix{Float64}(data[:, 2:end])\ny = Vector{Bool}(data[:, end])\nnothing\n\nThe features start from the seoncd column, while the last column are the class labels.\n\nLet's also apply some basic pre-processing.\n\nusing Statistics\n\nX = (X .- mean(X; dims=2)) ./ std(X; dims=2)\nX = hcat(X, ones(size(X, 1)))\nnothing\n\nLet's now istantiate the model and set up automatic differentiation using LogDensityProblemsAD.\n\nusing ADTypes, ReverseDiff\nusing LogDensityProblemsAD\n\nmodel = LogReg(X, y, size(X, 1))\nmodel_ad = LogDensityProblemsAD.ADgradient(ADTypes.AutoReverseDiff(), model)\nnothing\n\nTo enable subsampling, LogReg has to implement the method AdvancedVI.subsample. For our model, this is fairly simple: We only need to select the rows of X and the elements of y corresponding to the batch of data points. As subtle point here is that we wrapped model with LogDensityProblemsAD.ADgradient into model_ad. Therefore, AdvancedVI sees model_ad and not model. This means we have to specialize AdvancedVI.subsample to typeof(model_ad) and not LogReg.\n\nusing Accessors\nusing AdvancedVI\n\nfunction AdvancedVI.subsample(model::typeof(model_ad), idx)\n    (; X, y, n_data) = parent(model)\n    model′ = @set model.ℓ.X = X[idx, :]\n    model′′ = @set model′.ℓ.y = y[idx]\n    return model′′\nend\nnothing\n\ninfo: Info\nThe default implementation of AdvancedVI.subsample is AdvancedVI.subsample(model, idx) = model. Therefore, if the specialization of AdvancedVI.subsample is not set up properly, AdvancedVI will silently use full-batch gradients instead of subsampling. It is thus useful to check whether the right specialization of AdvancedVI.subsample is being called.","category":"section"},{"location":"tutorials/subsampling/#Scalable-Inference-via-AdvancedVI","page":"Scaling to Large Datasets","title":"Scalable Inference via AdvancedVI","text":"In this example, we will compare the convergence speed of KLMinRepGradProxDescent with and without subsampling. Subsampling can be turned on by supplying a subsampling strategy. Here, we will use ReshufflingBatchSubsampling, which implements random reshuffling. We will us a batch size of 32, which results in 313 = length(subsampling) = ceil(Int, size(X,2)/32) steps per epoch.\n\ndataset = 1:size(model.X, 1)\nbatchsize = 32\nsubsampling = ReshufflingBatchSubsampling(dataset, batchsize)\nalg_sub = KLMinRepGradProxDescent(ADTypes.AutoReverseDiff(; compile=true); subsampling)\nnothing\n\nRecall that each epoch is 313 steps. When using ReshufflingBatchSubsampling, it is best to choose the number of iterations to be a multiple of the number of steps length(subsampling) in an epoch. This is due to a peculiar property of ReshufflingBatchSubsampling: the objective value tends to increase during an epoch, and come down nearing the end. (Theoretically, this is due to conditionally biased nature of random reshuffling[MKR2020].) Therefore, the objective value is minimized exactly after the last step of each epoch.\n\n[MKR2020]: Mishchenko, K., Khaled, A., & Richtárik, P. (2020). Random reshuffling: Simple analysis with vast improvements. Advances in Neural Information Processing Systems, 33, 17309-17320.\n\nnum_epochs = 10\nmax_iter = num_epochs * length(subsampling)\nnothing\n\nIf we don't supply a subsampling strategy to KLMinRepGradProxDescent, subsampling will not be used.\n\nalg_full = KLMinRepGradProxDescent(ADTypes.AutoReverseDiff(; compile=true))\nnothing\n\nThe variational family will be set up as follows:\n\nusing LinearAlgebra\n\nd = LogDensityProblems.dimension(model_ad)\nq = FullRankGaussian(zeros(d), LowerTriangular(Matrix{Float64}(0.37*I, d, d)))\nb = Bijectors.bijector(model)\nbinv = Bijectors.inverse(b)\nq_transformed = Bijectors.TransformedDistribution(q, binv)\nnothing\n\nIt now remains to run VI. For comparison, we will record both the ELBO (with a large number of Monte Carlo samples) and the prediction accuracy.\n\nusing StatsFuns: StatsFuns\n\nlogging_interval = 100\ntime_begin = nothing\n\n\"\"\"\n    logistic_prediction(X, μ_β, Σ_β)\n    \nApproximate the posterior predictive probability for a logistic link function using Mackay's approximation (Bishop p. 220).\n\"\"\"\nfunction logistic_prediction(X, μ_β, Σ_β)\n    xtΣx = sum((model.X*Σ_β) .* model.X; dims=2)[:, 1]\n    κ = @. 1/sqrt(1 + π/8*xtΣx)\n    return StatsFuns.logistic.(κ .* X*μ_β)\nend\n\nfunction callback(; iteration, averaged_params, restructure, kwargs...)\n    if mod(iteration, logging_interval) == 1\n\n        # Use the averaged parameters (the eventual output of the algorithm)\n        q_avg = restructure(averaged_params)\n\n        # Compute predictions using \n        μ_β = mean(q_avg.dist)[1:(end - 1)] # posterior mean of β\n        Σ_β = cov(q_avg.dist)[1:(end - 1), end - 1] # marginal posterior covariance of β\n        y_pred = logistic_prediction(X, μ_β, Σ_β) .> 0.5\n\n        # Prediction accuracy\n        acc = mean(y_pred .== model.y)\n\n        # Higher fidelity estimate of the ELBO on the averaged parameters\n        n_samples = 256\n        elbo_callback = -estimate_objective(alg_full, q_avg, model; n_samples)\n\n        (elbo_callback=elbo_callback, accuracy=acc, time_elapsed=time() - time_begin)\n    else\n        nothing\n    end\nend\n\ntime_begin = time()\n_, info_full, _ = AdvancedVI.optimize(\n    alg_full, max_iter, model_ad, q_transformed; show_progress=false, callback\n);\n\ntime_begin = time()\n_, info_sub, _ = AdvancedVI.optimize(\n    alg_sub, max_iter, model_ad, q_transformed; show_progress=false, callback\n);\nnothing\n\nLet's visualize the evolution of the ELBO.\n\nusing Plots\n\nt = 1:logging_interval:max_iter\nplot(\n    [i.iteration for i in info_full[t]],\n    [i.elbo_callback for i in info_full[t]];\n    xlabel=\"Iteration\",\n    ylabel=\"ELBO\",\n    label=\"Full Batch\",\n)\nplot!(\n    [i.iteration for i in info_sub[t]],\n    [i.elbo_callback for i in info_sub[t]];\n    label=\"Subsampling\",\n)\nsavefig(\"subsampling_example_iteration_elbo.svg\")\nnothing\n\n(Image: )\n\nAccording to this plot, it might seem like subsampling has no benefit (if not detrimental). This is, however, because we are plotting against the number of iterations. Subsampling generally converges slower (asymptotically) in terms of iterations. But in return, it reduces the time spent at each iteration. Therefore, we need to plot against the elapsed time:\n\nplot(\n    [i.time_elapsed for i in info_full[t]],\n    [i.elbo_callback for i in info_full[t]];\n    xlabel=\"Wallclock Time (sec)\",\n    ylabel=\"ELBO\",\n    label=\"Full Batch\",\n)\nplot!(\n    [i.time_elapsed for i in info_sub[t]],\n    [i.elbo_callback for i in info_sub[t]];\n    label=\"Subsampling\",\n)\nsavefig(\"subsampling_example_time_elbo.svg\")\nnothing\n\n(Image: )\n\nWe can now see the dramatic effect of subsampling. The picture is similar if we visualize the prediction accuracy over time.\n\nplot(\n    [i.time_elapsed for i in info_full[t]],\n    [i.accuracy for i in info_full[t]];\n    xlabel=\"Wallclock Time (sec)\",\n    ylabel=\"Prediction Accuracy\",\n    label=\"Full Batch\",\n)\nplot!(\n    [i.time_elapsed for i in info_sub[t]],\n    [i.accuracy for i in info_sub[t]];\n    label=\"Subsampling\",\n)\nsavefig(\"subsampling_example_time_accuracy.svg\")\nnothing\n\n(Image: )\n\nBut remember that subsampling will always be asymptotically slower than no subsampling. That is, as the number of iterations increase, there will be a point where no subsampling will overtake subsampling even in terms of wallclock time. Therefore, subsampling is most beneficial when a crude solution to the VI problem suffices.","category":"section"},{"location":"tutorials/flows/#Normalizing-Flows","page":"Normalizing Flows","title":"Normalizing Flows","text":"In this example, we will see how to use NormalizingFlows with AdvancedVI.","category":"section"},{"location":"tutorials/flows/#Problem-Setup","page":"Normalizing Flows","title":"Problem Setup","text":"For the problem, we will look into a toy problem where NormalizingFlows can be benficial. For a dataset of real valued data y_1 ldots y_n, consider the following generative model:\n\nbeginaligned\nalpha sim textLogNormal(0 1) \nbeta sim textNormalleft(0 10right) \ny_i sim textNormalleft(alpha beta 1right)\nendaligned\n\nNotice that the mean is predicted as the product alpha beta of two unknown parameters. This results in multiplicative unidentifiability of alpha and beta. As such, the posterior exhibits a \"banana\"-shaped degeneracy. Multiplicative degeneracy is not entirely made up and do come up in some models used in practice. For example, in the 3-parameter (3-PL) item-response theory model and the N-mixture model used for estimating animal population.\n\nusing Bijectors: Bijectors\nusing Distributions\nusing LogDensityProblems: LogDensityProblems\n\nstruct MultDegen{Y}\n    y::Y\nend\n\nfunction LogDensityProblems.logdensity(model::MultDegen, θ)\n    α, β = θ[1], θ[2]\n\n    logprior_α = logpdf(LogNormal(0, 1), α)\n    logprior_β = logpdf(Normal(0, 10), β)\n\n    loglike_y = mapreduce(+, model.y) do yi\n        logpdf(Normal(α * β, 1.0), yi)\n    end\n    return logprior_α + logprior_β + loglike_y\nend\n\nfunction LogDensityProblems.dimension(model::MultDegen)\n    return 2\nend\n\nfunction LogDensityProblems.capabilities(::Type{<:MultDegen})\n    return LogDensityProblems.LogDensityOrder{0}()\nend\nnothing\n\nDegenerate posteriors often indicate that there is not enough data to pin-point the right set of parameters. Therefore, for the purpose of illustration, we will use a single data point:\n\nmodel = MultDegen([3.0])\nnothing\n\nThe banana-shaped degeneracy of the posterior can be readily visualized:\n\nusing Plots\n\ncontour(\n    range(0, 4; length=64),\n    range(-3, 25; length=64),\n    (x, y) -> LogDensityProblems.logdensity(model, [x, y]);\n    xlabel=\"α\",\n    ylabel=\"β\",\n    clims=(-8, Inf),\n)\n\nsavefig(\"flow_example_posterior.svg\")\nnothing\n\n(Image: )\n\nNotice that the two ends of the \"banana\" run deep both horizontally and vertically. This sort of nonlinear correlation structure is difficult to model using only location-scale distributions.","category":"section"},{"location":"tutorials/flows/#Gaussian-Variational-Family","page":"Normalizing Flows","title":"Gaussian Variational Family","text":"As usual, let's try to fit a multivariate Gaussian to this posterior.\n\nusing ADTypes: ADTypes\nusing ReverseDiff: ReverseDiff\nusing DifferentiationInterface: DifferentiationInterface\nusing LogDensityProblemsAD: LogDensityProblemsAD\n\nmodel_ad = LogDensityProblemsAD.ADgradient(\n    ADTypes.AutoReverseDiff(; compile=true), model; x=[1.0, 1.0]\n)\nnothing\n\nSince alpha is constrained to the positive real half-space, we have to employ bijectors. For this, we use Bijectors:\n\nusing Bijectors: Bijectors\n\nfunction Bijectors.bijector(model::MultDegen)\n    return Bijectors.Stacked(\n        Bijectors.bijector.([LogNormal(0, 1), Normal(0, 10)]), [1:1, 2:2]\n    )\nend\nnothing\n\nFor the algorithm, we will use the KLMinRepGradProxDescent objective.\n\nusing AdvancedVI\nusing LinearAlgebra\n\nd = LogDensityProblems.dimension(model_ad)\nq = FullRankGaussian(zeros(d), LowerTriangular(Matrix{Float64}(I, d, d)))\n\nbinv = Bijectors.inverse(Bijectors.bijector(model))\nq_trans = Bijectors.TransformedDistribution(q, binv)\n\nmax_iter = 3*10^3\nalg = KLMinRepGradProxDescent(ADTypes.AutoReverseDiff(; compile=true))\nq_out, info, _ = AdvancedVI.optimize(alg, max_iter, model_ad, q_trans; show_progress=false)\nnothing\n\nThe resulting variational posterior can be visualized as follows:\n\nsamples = rand(q_out, 10000)\nhistogram2d(\n    samples[1, :],\n    samples[2, :];\n    normalize=:pdf,\n    nbins=32,\n    xlabel=\"α\",\n    ylabel=\"β\",\n    xlims=(0, 4),\n    ylims=(-3, 25),\n)\nsavefig(\"flow_example_locationscale.svg\")\nnothing\n\n(Image: )\n\nWe can see that the mode is closely matched, but the tails don't go as deep as the true posterior. For this, we will need a more \"expressive\" variational family that is capable of representing nonlinear correlations.","category":"section"},{"location":"tutorials/flows/#Normalizing-Flow-Variational-Family","page":"Normalizing Flows","title":"Normalizing Flow Variational Family","text":"Now, let's try to optimize over a variational family formed by normalizing flows. Normalizing flows, or flows for short, is a class of parametric models leveraging neural networks for density estimation. (For a detailed tutorial on flows, refer to the review by Papamakarios et al.[PNRML2021]) Within the Julia ecosystem, the package NormalizingFlows provides a collection of popular flow models. In this example, we will use the popular RealNVP[DSB2017]. We will use a standard Gaussian base distribution with three layers, each with 16 hidden units.\n\n[PNRML2021]: Papamakarios, G., Nalisnick, E., Rezende, D. J., Mohamed, S., & Lakshminarayanan, B. (2021). Normalizing flows for probabilistic modeling and inference. Journal of Machine Learning Research, 22(57), 1-64.\n\n[DSB2017]: Dinh, L., Sohl-Dickstein, J., & Bengio, S. (2016). Density estimation using real nvp. In Proceedings of the International Conference on Learning Representations.\n\nusing NormalizingFlows\nusing Functors\n\n@leaf MvNormal\n\nn_layers = 3\nhidden_dims = [16, 16]\nq_flow = realnvp(MvNormal(zeros(d), I), hidden_dims, n_layers; paramtype=Float64)\nnothing\n\nRecall that out posterior is constrained. In most cases, flows assume an unconstrained support. Therefore, just as with the Gaussian variational family, we can incorporate Bijectors to match the supports:\n\nq_flow_trans = Bijectors.TransformedDistribution(q_flow, binv)\nnothing\n\nFor the variational inference algorithms, we will similarly minimize the KL divergence with stochastic gradient descent as originally proposed by Rezende and Mohamed[RM2015]. For this, however, we need to be mindful of the requirements of the variational algorithm. The default entropy gradient estimator of KLMinRepGradDescent is ClosedFormEntropy(), which assumes that the entropy of the variational family entropy(q) is available. For flows, the entropy is (usually) not available. Instead, we can use any gradient estimator that only relies on the log-density of the variational family logpdf(q), StickingTheLandingEntropy() or MonteCarloEntropy(). Here, we will use StickingTheLandingEntropy()[RWD2017]. When the variational family is \"expressive,\" this gradient estimator has a variance reduction effect, resulting in faster convergence[ASD2020]. Furthermore, Agrawal et al.[AD2025] claim that using a larger number of Monte Carlo samples n_samples is beneficial.\n\n[RM2015]: Rezende, D., & Mohamed, S. (2015, June). Variational inference with normalizing flows. In Proceedings of the International conference on machine learning. PMLR.\n\n[RWD2017]: Roeder, G., Wu, Y., & Duvenaud, D. K. (2017). Sticking the landing: Simple, lower-variance gradient estimators for variational inference. In Advances in Neural Information Processing Systems, 30.\n\n[ASD2020]: Agrawal, A., Sheldon, D. R., & Domke, J. (2020). Advances in black-box VI: Normalizing flows, importance weighting, and optimization. In Advances in Neural Information Processing Systems, 33, 17358-17369.\n\n[AD2025]: Agrawal, A., & Domke, J. (2024). Disentangling impact of capacity, objective, batchsize, estimators, and step-size on flow VI. In Proceedings of the International Conference on Artificial Intelligence and Statistics.\n\nalg_flow = KLMinRepGradDescent(\n    ADTypes.AutoReverseDiff(; compile=true);\n    n_samples=8,\n    operator=IdentityOperator(),\n    entropy=StickingTheLandingEntropy(),\n)\nnothing\n\nWithout further due, let's now run VI:\n\nq_flow_out, info_flow, _ = AdvancedVI.optimize(\n    alg_flow, max_iter, model_ad, q_flow_trans; show_progress=false\n)\nnothing\n\nWe can do a quick visual diagnostic of whether the optimization went smoothly:\n\nplot([i.elbo for i in info_flow]; xlabel=\"Iteration\", ylabel=\"ELBO\", ylims=(-10, Inf))\nsavefig(\"flow_example_flow_elbo.svg\")\nnothing\n\n(Image: )\n\nFinally, let's visualize the variational posterior:\n\nsamples = rand(q_flow_out, 10000)\nhistogram2d(\n    samples[1, :],\n    samples[2, :];\n    normalize=:pdf,\n    nbins=64,\n    xlabel=\"α\",\n    ylabel=\"β\",\n    xlims=(0, 4),\n    ylims=(-3, 25),\n)\nsavefig(\"flow_example_flow.svg\")\nnothing\n\n(Image: )\n\nCompared to the Gaussian approximation, we can see that the tails go much deeper into vertical direction. This shows that, for this example with extreme nonlinear correlations, normalizing flows enable more accurate approximation.","category":"section"},{"location":"fisherminbatchmatch/#fisherminbatchmatch","page":"FisherMinBatchMatch","title":"FisherMinBatchMatch","text":"","category":"section"},{"location":"fisherminbatchmatch/#Description","page":"FisherMinBatchMatch","title":"Description","text":"This algorithm, known as batch-and-match (BaM) aims to minimize the covariance-weighted 2nd-order Fisher divergence by running a proximal point-type method[CMPMGBS24]. On certain low-dimensional problems, BaM can converge very quickly without any tuning. Since FisherMinBatchMatch is a measure-space algorithm, its use is restricted to full-rank Gaussian variational families (FullRankGaussian) that make the measure-valued operations tractable.\n\nThe associated objective value can be estimated through the following:\n\n[CMPMGBS24]: Cai, D., Modi, C., Pillaud-Vivien, L., Margossian, C. C., Gower, R. M., Blei, D. M., & Saul, L. K. (2024). Batch and match: black-box variational inference with a score-based divergence. In Proceedings of the International Conference on Machine Learning.","category":"section"},{"location":"fisherminbatchmatch/#fisherminbatchmatch_method","page":"FisherMinBatchMatch","title":"Methodology","text":"This algorithm aims to solve the problem\n\n  mathrmminimize_q in mathcalQquad mathrmF_mathrmcov(q pi)\n\nwhere mathcalQ is some family of distributions, often called the variational family, and mathrmF_mathrmcov is a divergence defined as\n\nmathrmF_mathrmcov(q pi) = mathbbE_z sim q leftlVert nabla log fracqpi (z) rightrVert_mathrmCov(q)^2 \n\nwhere lVert x rVert_A^2 = x^top A x  is a weighted norm mathrmF_mathrmcov can be viewed as a variant of the canonical 2nd-order Fisher divergence defined as\n\nmathrmF_2(q pi) = sqrt mathbbE_z sim q leftlVert nabla log fracqpi (z) rightrVert^2 \n\nThe use of the weighted norm lVert cdot rVert_mathrmCov(q)^2 facilitates the use of a proximal point-type method for minimizing mathrmF_2(q pi). In particular, BaM iterates the update\n\n  q_t+1 = argmin_q in mathcalQ left mathrmF_mathrmcov(q pi) + frac2lambda_t mathrmKLleft(q_t qright) right \n\nSince mathrmF(q pi) is intractable, it is replaced with a Monte Carlo approximation with a number of samples n_samples. Furthermore, by restricting mathcalQ to a Gaussian variational family, the update rule admits a closed form solution[CMPMGBS24]. Notice that the update does not involve the parameterization of q_t, which makes FisherMinBatchMatch a measure-space algorithm.\n\nHistorically, the idea of using a proximal point-type update for minimizing a Fisher divergence-like objective was initially coined as Gaussian score matching[MGMYBS23]. BaM can be viewed as a successor to this algorithm.\n\n[MGMYBS23]: Modi, C., Gower, R., Margossian, C., Yao, Y., Blei, D., & Saul, L. (2023). Variational inference with Gaussian score matching. In Advances in Neural Information Processing Systems, 36.","category":"section"},{"location":"fisherminbatchmatch/#AdvancedVI.FisherMinBatchMatch","page":"FisherMinBatchMatch","title":"AdvancedVI.FisherMinBatchMatch","text":"FisherMinBatchMatch(n_samples, subsampling)\nFisherMinBatchMatch(; n_samples, subsampling)\n\nCovariance-weighted Fisher divergence minimization via the batch-and-match algorithm, which is a proximal point-type optimization scheme.\n\n(Keyword) Arguments\n\nn_samples::Int: Number of samples (batchsize) used to compute the moments required for the batch-and-match update. (default: 32)\nsubsampling::Union{Nothing,<:AbstractSubsampling}: Optional subsampling strategy. (default: nothing)\n\nwarning: Warning\nFisherMinBatchMatch with subsampling enabled results in a biased algorithm and may not properly optimize the covariance-weighted Fisher divergence.\n\nnote: Note\nFisherMinBatchMatch requires a sufficiently large n_samples to converge quickly.\n\nnote: Note\nThe subsampling strategy is only applied to the target LogDensityProblem but not to the variational approximation q. That is, FisherMinBatchMatch does not support amortization or structured variational families.\n\nOutput\n\nq: The last iterate of the algorithm.\n\nCallback Signature\n\nThe callback function supplied to optimize needs to have the following signature:\n\ncallback(; rng, iteration, q, info)\n\nThe keyword arguments are as follows:\n\nrng: Random number generator internally used by the algorithm.\niteration: The index of the current iteration.\nq: Current variational approximation.\ninfo: NamedTuple containing the information generated during the current iteration.\n\nRequirements\n\nThe variational family is FullRankGaussian.\nThe target distribution has unconstrained support.\nThe target LogDensityProblems.logdensity(prob, x) has at least first-order differentiation capability.\n\n\n\n\n\n","category":"type"},{"location":"fisherminbatchmatch/#AdvancedVI.estimate_objective-Tuple{AbstractRNG, KLMinWassFwdBwd, MvLocationScale, Any}-fisherminbatchmatch","page":"FisherMinBatchMatch","title":"AdvancedVI.estimate_objective","text":"estimate_objective([rng,] alg, q, prob; kwargs...)\n\nEstimate the variational objective to be minimized by the algorithm alg for approximating the target prob with the variational approximation q.\n\nArguments\n\nrng::Random.AbstractRNG: Random number generator.\nalg::AbstractVariationalAlgorithm: Variational inference algorithm.\nprob: The target log-joint likelihood implementing the LogDensityProblem interface.\nq: Variational approximation.\n\nKeyword Arguments\n\nDepending on the algorithm, additional keyword arguments may apply. Please refer to the respective documentation of each algorithm for more info.\n\nReturns\n\nobj_est: Estimate of the objective value.\n\n\n\n\n\n","category":"method"},{"location":"klminrepgraddescent/#klminrepgraddescent","page":"KLMinRepGradDescent","title":"KLMinRepGradDescent","text":"","category":"section"},{"location":"klminrepgraddescent/#Description","page":"KLMinRepGradDescent","title":"Description","text":"This algorithm aims to minimize the exclusive (or reverse) Kullback-Leibler (KL) divergence via stochastic gradient descent in the space of parameters. Specifically, it uses the the reparameterization gradient estimator. As a result, this algorithm is best applicable when the target log-density is differentiable and the sampling process of the variational family is differentiable. (See the methodology section for more details.) This algorithm is also commonly referred to as automatic differentiation variational inference, black-box variational inference with the reparameterization gradient, and stochastic gradient variational inference. KLMinRepGradDescent is also an alias of ADVI .\n\nThe associated objective value can be estimated through the following:","category":"section"},{"location":"klminrepgraddescent/#klminrepgraddescent_method","page":"KLMinRepGradDescent","title":"Methodology","text":"This algorithm aims to solve the problem\n\n  mathrmminimize_q in mathcalQquad mathrmKLleft(q piright)\n\nwhere mathcalQ is some family of distributions, often called the variational family, by running stochastic gradient descent in the (Euclidean) space of parameters. That is, for all q_lambda in mathcalQ, we assume q_lambda there is a corresponding vector of parameters lambda in Lambda, where the space of parameters is Euclidean such that Lambda subset mathbbR^p.\n\nSince we usually only have access to the unnormalized densities of the target distribution pi, we don't have direct access to the KL divergence. Instead, the ELBO maximization strategy maximizes a surrogate objective, the evidence lower bound (ELBO; [JGJS1999])\n\n  mathrmELBOleft(qright) triangleq mathbbE_theta sim q log pileft(thetaright) + mathbbHleft(qright)\n\nwhich is equivalent to the KL up to an additive constant (the evidence).\n\nAlgorithmically, KLMinRepGradDescent iterates the step\n\n  lambda_t+1 = mathrmoperatorbig(\n      lambda_t + gamma_t widehatnabla_lambda mathrmELBO (q_lambda_t) \n  big)  \n\nwhere widehatnabla mathrmELBO(q_lambda) is the reparameterization gradient estimate[HC1983][G1991][R1992][P1996] of the ELBO gradient and mathrmoperator is an optional operator (e.g. projections, identity mapping).\n\nThe reparameterization gradient, also known as the push-in gradient or the pathwise gradient, was introduced to VI in [TL2014][RMW2014][KW2014]. For the variational family mathcalQ, suppose the process of sampling from q_lambda in mathcalQ can be described by some differentiable reparameterization function T_lambda and a base distribution varphi independent of lambda such that\n\nz sim  q_lambda qquadLeftrightarrowqquad\nz stackreld= T_lambdaleft(epsilonright)quad epsilon sim varphi  \n\nIn these cases, denoting the target log denstiy as log pi, we can effectively estimate the gradient of the ELBO by directly differentiating the stochastic estimate of the ELBO objective\n\n  widehatmathrmELBOleft(q_lambdaright) = frac1Msum^M_m=1 log pileft(T_lambdaleft(epsilon_mright)right) + mathbbHleft(q_lambdaright)\n\nwhere epsilon_m sim varphi are Monte Carlo samples.\n\n[JGJS1999]: Jordan, M. I., Ghahramani, Z., Jaakkola, T. S., & Saul, L. K. (1999). An introduction to variational methods for graphical models. Machine learning, 37, 183-233.\n\n[HC1983]: Ho, Y. C., & Cao, X. (1983). Perturbation analysis and optimization of queueing networks. Journal of optimization theory and Applications, 40(4), 559-582.\n\n[G1991]: Glasserman, P. (1991). Gradient estimation via perturbation analysis (Vol. 116). Springer Science & Business Media.\n\n[R1992]: Rubinstein, R. Y. (1992). Sensitivity analysis of discrete event systems by the “push out” method. Annals of Operations Research, 39(1), 229-250.\n\n[P1996]: Pflug, G. C. (1996). Optimization of stochastic models: the interface between simulation and optimization (Vol. 373). Springer Science & Business Media.\n\n[TL2014]: Titsias, M., & Lázaro-Gredilla, M. (2014). Doubly stochastic variational Bayes for non-conjugate inference. In International Conference on Machine Learning.\n\n[RMW2014]: Rezende, D. J., Mohamed, S., & Wierstra, D. (2014). Stochastic backpropagation and approximate inference in deep generative models. In International Conference on Machine Learning.\n\n[KW2014]: Kingma, D. P., & Welling, M. (2014). Auto-encoding variational bayes. In International Conference on Learning Representations.","category":"section"},{"location":"klminrepgraddescent/#bijectors","page":"KLMinRepGradDescent","title":"Handling Constraints with Bijectors","text":"As mentioned in the docstring, KLMinRepGradDescent assumes that the variational approximation q_lambda and the target distribution pi have the same support for all lambda in Lambda. However, in general, it is most convenient to use variational families that have the whole Euclidean space mathbbR^d as their support. This is the case for the location-scale distributions provided by AdvancedVI. For target distributions which the support is not the full mathbbR^d, we can apply some transformation b to q_lambda to match its support such that\n\nz sim  q_blambda qquadLeftrightarrowqquad\nz stackreld= b^-1left(etaright)quad eta sim q_lambda\n\nwhere b is often called a bijector, since it is often chosen among bijective transformations. This idea is known as automatic differentiation VI[KTRGB2017] and has subsequently been improved by Tensorflow Probability[DLTBV2017]. In Julia, Bijectors.jl[FXTYG2020] provides a comprehensive collection of bijections.\n\n[KTRGB2017]: Kucukelbir, A., Tran, D., Ranganath, R., Gelman, A., & Blei, D. M. (2017). Automatic differentiation variational inference. Journal of Machine Learning Research, 18(14), 1-45.\n\n[DLTBV2017]: Dillon, J. V., Langmore, I., Tran, D., Brevdo, E., Vasudevan, S., Moore, D., ... & Saurous, R. A. (2017). Tensorflow distributions. arXiv.\n\n[FXTYG2020]: Fjelde, T. E., Xu, K., Tarek, M., Yalburgi, S., & Ge, H. (2020,. Bijectors. jl: Flexible transformations for probability distributions. In Symposium on Advances in Approximate Bayesian Inference. One caveat of ADVI is that, after applying the bijection, a Jacobian adjustment needs to be applied. That is, the objective is now\n\nmathrmADVIleft(lambdaright)\ntriangleq\nmathbbE_eta sim q_lambdaleft\n  log pileft( b^-1left( eta right) right)\n  + log lvert J_b^-1left(etaright) rvert\nright\n+ mathbbHleft(q_lambdaright)\n\nThis is automatically handled by AdvancedVI through TransformedDistribution provided by Bijectors.jl. See the following example:\n\nusing Bijectors\nq = MeanFieldGaussian(μ, L)\nb = Bijectors.bijector(dist)\nbinv = inverse(b)\nq_transformed = Bijectors.TransformedDistribution(q, binv)\n\nBy passing q_transformed to optimize, the Jacobian adjustment for the bijector b is automatically applied. (See the Basic Example for a fully working example.)","category":"section"},{"location":"klminrepgraddescent/#entropygrad","page":"KLMinRepGradDescent","title":"Entropy Gradient Estimators","text":"For the gradient of the entropy term, we provide three choices with varying requirements. The user can select the entropy estimator by passing it as a keyword argument when constructing the algorithm object.\n\nEstimator entropy(q) logpdf(q) Type\nClosedFormEntropy required  Deterministic\nMonteCarloEntropy  required Monte Carlo\nStickingTheLandingEntropy  required Monte Carlo with control variate\n\nThe requirements mean that either Distributions.entropy or Distributions.logpdf need to be implemented for the choice of variational family. In general, the use of ClosedFormEntropy is recommended whenever possible. If entropy is not available, then StickingTheLandingEntropy is recommended. See the following section for more details.","category":"section"},{"location":"klminrepgraddescent/#The-StickingTheLandingEntropy-Estimator","page":"KLMinRepGradDescent","title":"The StickingTheLandingEntropy Estimator","text":"The StickingTheLandingEntropy, or STL estimator, is a control variate approach [RWD2017].\n\nIt occasionally results in lower variance when pi approx q_lambda, and higher variance when pi notapprox q_lambda. The conditions for which the STL estimator results in lower variance is still an active subject for research.\n\nThe main downside of the STL estimator is that it needs to evaluate and differentiate the log density of q_lambda, logpdf(q), in every iteration. Depending on the variational family, this might be computationally inefficient or even numerically unstable. For example, if q_lambda is a Gaussian with a full-rank covariance, a back-substitution must be performed at every step, making the per-iteration complexity mathcalO(d^3) and reducing numerical stability.\n\nIn this example, the true posterior is contained within the variational family. This setting is known as \"perfect variational family specification.\" In this case, KLMinRepGradDescent with StickingTheLandingEntropy is the only estimator known to converge exponentially fast (\"linear convergence\") to the true solution.\n\nRecall that the original ADVI objective with a closed-form entropy (CFE) is given as follows:\n\nn_montecarlo = 16;\nb = Bijectors.bijector(model);\nbinv = inverse(b)\n\nq0_trans = Bijectors.TransformedDistribution(q0, binv)\n\ncfe = KLMinRepGradDescent(\n    AutoReverseDiff();\n    entropy=ClosedFormEntropy(),\n    optimizer=Adam(1e-2),\n    operator=ClipScale(),\n)\nnothing\n\nThe repgradelbo estimator can instead be created as follows:\n\nstl = KLMinRepGradDescent(\n    AutoReverseDiff();\n    entropy=StickingTheLandingEntropy(),\n    optimizer=Adam(1e-2),\n    operator=ClipScale(),\n)\nnothing\n\n(Image: )\n\nWe can see that the noise of the repgradelbo estimator becomes smaller as VI converges. However, the speed of convergence may not always be significantly different. Also, due to noise, just looking at the ELBO may not be sufficient to judge which algorithm is better. This can be made apparent if we measure convergence through the distance to the optimum:\n\n(Image: )\n\nWe can see that STL kicks-in at later stages of optimization. Therefore, when STL \"works\", it yields a higher accuracy solution even on large stepsizes. However, whether STL works or not highly depends on the problem[KMG2024]. Furthermore, in a lot of cases, a low-accuracy solution may be sufficient.\n\n[RWD2017]: Roeder, G., Wu, Y., & Duvenaud, D. K. (2017). Sticking the landing: Simple, lower-variance gradient estimators for variational inference. Advances in Neural Information Processing Systems, 30.\n\n[KMG2024]: Kim, K., Ma, Y., & Gardner, J. (2024). Linear Convergence of Black-Box Variational Inference: Should We Stick the Landing?. In International Conference on Artificial Intelligence and Statistics (pp. 235-243). PMLR.","category":"section"},{"location":"klminrepgraddescent/#Advanced-Usage","page":"KLMinRepGradDescent","title":"Advanced Usage","text":"There are two major ways to customize the behavior of KLMinRepGradDescent\n\nCustomize the Distributions functions: rand(q), entropy(q), logpdf(q).\nCustomize AdvancedVI.reparam_with_entropy.\n\nIt is generally recommended to customize rand(q), entropy(q), logpdf(q), since it will easily compose with other functionalities provided by AdvancedVI.\n\nThe most advanced way is to customize AdvancedVI.reparam_with_entropy. In particular, reparam_with_entropy is the function that invokes rand(q), entropy(q), logpdf(q). Thus, it is the most general way to override the behavior of RepGradELBO.\n\nTo illustrate how we can customize the rand(q) function, we will implement quasi-Monte-Carlo variational inference[BWM2018]. Consider the case where we use the MeanFieldGaussian variational family. In this case, it suffices to override its rand specialization as follows:\n\nusing QuasiMonteCarlo\nusing StatsFuns\n\nqmcrng = SobolSample(; R=OwenScramble(; base=2, pad=32))\n\nfunction Distributions.rand(\n    rng::AbstractRNG, q::MvLocationScale{<:Diagonal,D,L}, num_samples::Int\n) where {L,D}\n    (; location, scale, dist) = q\n    n_dims = length(location)\n    scale_diag = diag(scale)\n    unif_samples = QuasiMonteCarlo.sample(num_samples, length(q), qmcrng)\n    std_samples = norminvcdf.(unif_samples)\n    return scale_diag .* std_samples .+ location\nend\nnothing\n\n(Note that this is a quick-and-dirty example, and there are more sophisticated ways to implement this.)\n\nBy plotting the ELBO, we can see the effect of quasi-Monte Carlo. (Image: ) We can see that quasi-Monte Carlo results in much lower variance than naive Monte Carlo. However, similarly to the STL example, just looking at the ELBO is often insufficient to really judge performance. Instead, let's look at the distance to the global optimum:\n\n(Image: )\n\nQMC yields an additional order of magnitude in accuracy. Also, unlike STL, it ever-so slightly accelerates convergence. This is because quasi-Monte Carlo uniformly reduces variance, unlike STL, which reduces variance only near the optimum.\n\n[BWM2018]: Buchholz, A., Wenzel, F., & Mandt, S. (2018). Quasi-monte carlo variational inference. In International Conference on Machine Learning.","category":"section"},{"location":"klminrepgraddescent/#AdvancedVI.KLMinRepGradDescent","page":"KLMinRepGradDescent","title":"AdvancedVI.KLMinRepGradDescent","text":"KLMinRepGradDescent(adtype; entropy, optimizer, n_samples, averager, operator)\n\nKL divergence minimization by running stochastic gradient descent with the reparameterization gradient in the Euclidean space of variational parameters.\n\nnote: Note\nFor a <:MvLocationScale variational family, IdentityOperator should be avoided for operator since optimization can result in a singular scale matrix. Instead, consider using ClipScale.\n\nArguments\n\nadtype::ADTypes.AbstractADType: Automatic differentiation backend. \n\nKeyword Arguments\n\nentropy: Entropy gradient estimator to be used. Must be one of ClosedFormEntropy, StickingTheLandingEntropy, MonteCarloEntropy. (default: ClosedFormEntropy())\noptimizer::Optimisers.AbstractRule: Optimization algorithm to be used. (default: DoWG())\nn_samples::Int: Number of Monte Carlo samples to be used for estimating each gradient. (default: 1)\naverager::AbstractAverager: Parameter averaging strategy. \noperator::AbstractOperator: Operator to be applied after each gradient descent step. (default: IdentityOperator())\nsubsampling::Union{<:Nothing,<:AbstractSubsampling}: Data point subsampling strategy. If nothing, subsampling is not used. (default: nothing)\n\nOutput\n\nq_averaged: The variational approximation formed by the averaged SGD iterates.\n\nCallback Signature\n\nThe callback function supplied to optimize needs to have the following signature:\n\ncallback(; rng, iteration, restructure, params, averaged_params, restructure, gradient)\n\nThe keyword arguments are as follows:\n\nrng: Random number generator internally used by the algorithm.\niteration: The index of the current iteration.\nrestructure: Function that restructures the variational approximation from the variational parameters. Calling restructure(params) reconstructs the current variational approximation. \nparams: Current variational parameters.\naveraged_params: Variational parameters averaged according to the averaging strategy.\ngradient: The estimated (possibly stochastic) gradient.\n\nRequirements\n\nThe trainable parameters in the variational approximation are expected to be extractable through Optimisers.destructure. This requires the variational approximation to be marked as a functor through Functors.@functor.\nThe variational approximation q_lambda implements rand.\nThe target distribution and the variational approximation have the same support.\nThe target LogDensityProblems.logdensity(prob, x) must be differentiable with respect to x by the selected AD backend.\nAdditonal requirements on q may apply depending on the choice of entropy.\n\n\n\n\n\n","category":"type"},{"location":"klminrepgraddescent/#AdvancedVI.estimate_objective-Tuple{AbstractRNG, Union{KLMinRepGradDescent, KLMinRepGradProxDescent, KLMinScoreGradDescent}, Any, Any}-klminrepgraddescent","page":"KLMinRepGradDescent","title":"AdvancedVI.estimate_objective","text":"estimate_objective([rng,] alg, q, prob; n_samples, entropy)\n\nEstimate the negative ELBO of the variational approximation q against the target log-density prob.\n\nArguments\n\nrng::Random.AbstractRNG: Random number generator.\nalg::Union{<:KLMinRepGradDescent,<:KLMinRepGradProxDescent,<:KLMinScoreGradDescent}: Variational inference algorithm.\nq: Variational approximation.\nprob: The target log-joint likelihood implementing the LogDensityProblem interface.\n\nKeyword Arguments\n\nn_samples::Int: Number of Monte Carlo samples for estimating the objective. (default: Same as the the number of samples used for estimating the gradient during optimization.)\nentropy::AbstractEntropyEstimator: Entropy estimator. (default: MonteCarloEntropy())\n\nReturns\n\nobj_est: Estimate of the objective value.\n\n\n\n\n\n","category":"method"},{"location":"klminrepgraddescent/#AdvancedVI.StickingTheLandingEntropy","page":"KLMinRepGradDescent","title":"AdvancedVI.StickingTheLandingEntropy","text":"StickingTheLandingEntropy()\n\nThe \"sticking the landing\" entropy estimator[RWD2017].\n\nRequirements\n\nThe variational approximation q implements logpdf.\nlogpdf(q, η) must be differentiable by the selected AD framework.\n\n\n\n\n\n","category":"type"},{"location":"klminrepgraddescent/#AdvancedVI.reparam_with_entropy","page":"KLMinRepGradDescent","title":"AdvancedVI.reparam_with_entropy","text":"reparam_with_entropy(rng, q, q_stop, n_samples, ent_est)\n\nDraw n_samples from q and compute its entropy.\n\nArguments\n\nrng::Random.AbstractRNG: Random number generator.\nq: Variational approximation.\nq_stop: Same as q, but held constant during differentiation. Should only be used for computing the entropy.\nn_samples::Int: Number of Monte Carlo samples \nent_est: The entropy estimation strategy. (See estimate_entropy.)\n\nReturns\n\nsamples: Monte Carlo samples generated through reparameterization. Their support matches that of the target distribution.\nentropy: An estimate (or exact value) of the differential entropy of q.\n\n\n\n\n\n","category":"function"},{"location":"klminwassfwdbwd/#klminwassfwdbwd","page":"KLMinWassFwdBwd","title":"KLMinWassFwdBwd","text":"","category":"section"},{"location":"klminwassfwdbwd/#Description","page":"KLMinWassFwdBwd","title":"Description","text":"This algorithm aims to minimize the exclusive (or reverse) Kullback-Leibler (KL) divergence by running proximal gradient descent (also known as forward-backward splitting) in Wasserstein space[DBCS2023]. (This algorithm is also sometimes referred to as \"Wasserstein VI\".) Since KLMinWassFwdBwd is a measure-space algorithm, its use is restricted to full-rank Gaussian variational families (FullRankGaussian) that makes the measure-valued operations tractable.\n\nThe associated objective value can be estimated through the following:\n\n[DBCS2023]: Diao, M. Z., Balasubramanian, K., Chewi, S., & Salim, A. (2023). Forward-backward Gaussian variational inference via JKO in the Bures-Wasserstein space. In International Conference on Machine Learning. PMLR.","category":"section"},{"location":"klminwassfwdbwd/#klminwassfwdbwd_method","page":"KLMinWassFwdBwd","title":"Methodology","text":"This algorithm aims to solve the problem\n\n  mathrmminimize_q in mathcalQquad mathrmKLleft(q piright)\n\nwhere mathcalQ is some family of distributions, often called the variational family. Since we usually only have access to the unnormalized densities of the target distribution pi, we don't have direct access to the KL divergence. Instead, we focus on minimizing a surrogate objective, the free energy functional, which corresponds to the negated evidence lower bound[JGJS1999], defined as\n\n  mathcalFleft(qright) triangleq mathcalUleft(qright) + mathcalHleft(qright)\n\nwhere\n\nbeginaligned\n  mathcalUleft(qright) = mathbbE_theta sim q -log pileft(thetaright)\n  text(potential energy)\n  \n  mathcalHleft(qright) = mathbbE_theta sim q log qleft(thetaright) \n  text(Boltzmann entropy)\nendaligned\n\nFor solving this problem, KLMinWassFwdBwd relies on proximal stochastic gradient descent (PSGD)–-also known as \"forward-backward splitting\"–-that iterates\n\n  q_t+1 = mathrmJKO_gamma_t mathcalHbig(\n      q_t - gamma_t widehatnabla_mathrmBW mathcalU (q_t) \n  big)  \n\nwhere widehatnabla_mathrmBW mathcalU is a stochastic estimate of the Bures-Wasserstein measure-valued gradient of mathcalU, the JKO (proximal) operator is defined as\n\nmathrmJKO_gamma_t mathcalH(mu)\n=\nargmin_nu in mathcalQ left mathcalH(nu) + frac12 gamma_t mathrmW_2 (mu nu)^2 right \n\nand mathrmW_2 is the Wasserstein-2 distance. When mathcalQ is set to be the Bures-Wasserstein space of mathbbR^d, this algorithm is referred to as the Jordan-Kinderlehrer-Otto (JKO) scheme[JKO1998], which was originally developed to study gradient flows under Wasserstein metrics. Within this context, KLMinWassFwdBwd can be viewed as a numerical realization of the JKO scheme by restricting mathcalQ to be a tractable parametric variational family. Specifically, Diao et al.[DBCS2023] derived the JKO update for multivariate Gaussians, which is implemented by KLMinWassFwdBwd. KLMinWassFwdBwd also exactly corresponds to the measure-space analog of KLMinRepGradProxDescent.\n\n[JKO1998]: Jordan, R., Kinderlehrer, D., & Otto, F. (1998). The variational formulation of the Fokker–Planck equation. SIAM Journal on Mathematical Analysis, 29(1).\n\n[JGJS1999]: Jordan, M. I., Ghahramani, Z., Jaakkola, T. S., & Saul, L. K. (1999). An introduction to variational methods for graphical models. Machine learning, 37, 183-233.","category":"section"},{"location":"klminwassfwdbwd/#AdvancedVI.KLMinWassFwdBwd","page":"KLMinWassFwdBwd","title":"AdvancedVI.KLMinWassFwdBwd","text":"KLMinWassFwdBwd(n_samples, stepsize, subsampling)\nKLMinWassFwdBwd(; n_samples, stepsize, subsampling)\n\nKL divergence minimization by running stochastic proximal gradient descent (forward-backward splitting) in Wasserstein space[DBCS2023].\n\nThis algorithm requires second-order information about the target. If the target LogDensityProblem has second-order differentiation capability, Hessians are used. Otherwise, if the target has only first-order capability, it will use only gradients but this will porbably result in slower convergence and less robust behavior.\n\n(Keyword) Arguments\n\nn_samples::Int: Number of samples used to estimate the Wasserstein gradient. (default: 1)\nstepsize::Float64: Step size of stochastic proximal gradient descent.\nsubsampling::Union{Nothing,<:AbstractSubsampling}: Optional subsampling strategy.\n\nnote: Note\nThe subsampling strategy is only applied to the target LogDensityProblem but not to the variational approximation q. That is, KLMinWassFwdBwd does not support amortization or structured variational families.\n\nOutput\n\nq: The last iterate of the algorithm.\n\nCallback Signature\n\nThe callback function supplied to optimize needs to have the following signature:\n\ncallback(; rng, iteration, q, info)\n\nThe keyword arguments are as follows:\n\nrng: Random number generator internally used by the algorithm.\niteration: The index of the current iteration.\nq: Current variational approximation.\ninfo: NamedTuple containing the information generated during the current iteration.\n\nRequirements\n\nThe variational family is FullRankGaussian.\nThe target distribution has unconstrained support.\nThe target LogDensityProblems.logdensity(prob, x) has at least first-order differentiation capability.\n\n\n\n\n\n","category":"type"},{"location":"klminwassfwdbwd/#AdvancedVI.estimate_objective-Tuple{AbstractRNG, KLMinWassFwdBwd, MvLocationScale, Any}-klminwassfwdbwd","page":"KLMinWassFwdBwd","title":"AdvancedVI.estimate_objective","text":"estimate_objective([rng,] alg, q, prob; kwargs...)\n\nEstimate the variational objective to be minimized by the algorithm alg for approximating the target prob with the variational approximation q.\n\nArguments\n\nrng::Random.AbstractRNG: Random number generator.\nalg::AbstractVariationalAlgorithm: Variational inference algorithm.\nprob: The target log-joint likelihood implementing the LogDensityProblem interface.\nq: Variational approximation.\n\nKeyword Arguments\n\nDepending on the algorithm, additional keyword arguments may apply. Please refer to the respective documentation of each algorithm for more info.\n\nReturns\n\nobj_est: Estimate of the objective value.\n\n\n\n\n\n","category":"method"},{"location":"klminscoregraddescent/#klminscoregraddescent","page":"KLMinScoreGradDescent","title":"KLMinScoreGradDescent","text":"","category":"section"},{"location":"klminscoregraddescent/#Description","page":"KLMinScoreGradDescent","title":"Description","text":"This algorithms aim to minimize the exclusive (or reverse) Kullback-Leibler (KL) divergence via stochastic gradient descent in the space of parameters. Specifically, it uses the the score gradient estimator, which is similar to the algorithm that was originally referred to as black-box variational inference (BBVI; [RGB2014][WW2013]). (The term BBVI has also recently been used to refer to the more general setup of maximizing the ELBO in parameter space. We are using the more narrow definition, which restricts to the use of the score gradient.) However, instead of using the vanilla score gradient estimator, we differentiate the \"VarGrad\" objective[RBNRA2020], which results in the score gradient variance-reduced by the leave-one-out control variate[SK2014][KvHW2019]. KLMinScoreGradDescent is also an alias of BBVI.\n\n[RBNRA2020]: Richter, L., Boustati, A., Nüsken, N., Ruiz, F., & Akyildiz, O. D. (2020). Vargrad: a low-variance gradient estimator for variational inference. Advances in Neural Information Processing Systems, 33, 13481-13492.\n\n[SK2014]: Salimans, T., & Knowles, D. A. (2014). On using control variates with stochastic approximation for variational bayes and its connection to stochastic linear regression. arXiv preprint arXiv:1401.1022.\n\nThe associated objective value can be estimated through the following:","category":"section"},{"location":"klminscoregraddescent/#Methodology","page":"KLMinScoreGradDescent","title":"Methodology","text":"This algorithm aims to solve the problem\n\n  mathrmminimize_q in mathcalQquad mathrmKLleft(q piright)\n\nwhere mathcalQ is some family of distributions, often called the variational family, by running stochastic gradient descent in the (Euclidean) space of parameters. That is, for all q_lambda in mathcalQ, we assume q_lambda there is a corresponding vector of parameters lambda in Lambda, where the space of parameters is Euclidean such that Lambda subset mathbbR^p.\n\nSince we usually only have access to the unnormalized densities of the target distribution pi, we don't have direct access to the KL divergence. Instead, the ELBO maximization strategy maximizes a surrogate objective, the evidence lower bound (ELBO; [JGJS1999])\n\n  mathrmELBOleft(qright) triangleq mathbbE_theta sim q log pileft(thetaright) + mathbbHleft(qright)\n\nwhich is equivalent to the KL up to an additive constant (the evidence).\n\nAlgorithmically, KLMinRepGradDescent iterates the step\n\n  lambda_t+1 = mathrmoperatorbig(\n      lambda_t + gamma_t widehatnabla_lambda mathrmELBO (q_lambda_t) \n  big)  \n\nwhere widehatnabla mathrmELBO(q_lambda) is the score gradient estimate[G1990][KR1996][RSU1996][W1992] of the ELBO gradient and mathrmoperator is an optional operator (e.g. projections, identity mapping).\n\nLet us describe the score gradient estimator[G1990][KR1996][RSU1996][W1992] of the ELBO gradient, also known as the score function method and the REINFORCE gradient. For variational inference, the use of the score gradient was proposed in [WW2013][RGB2014]. Unlike the reparameterization gradient, the score gradient does not require the target log density to be differentiable, and does not differentiate through the sampling process of the variational approximation q. Instead, it only requires gradients of the log density log q. For this reason, the score gradient is the standard method to deal with discrete variables and targets with discrete support. In more detail, the score gradient uses the Fisher log-derivative identity: For any regular f,\n\nnabla_lambda mathbbE_z sim q_lambda f\n=\nmathbbE_z sim q_lambdaleft f(z) log q_lambda(z) right  \n\nThe ELBO corresponds to the case where f = log pi  log q, where log pi is the target log density. Instead of implementing the canonical score gradient, KLMinScoreGradDescent internally uses the \"VarGrad\" objective[RBNRA2020]:\n\nwidehatmathrmVarGrad(lambda) \n=\nmathrmVarleft( log q_lambda(z_i) - log pileft(z_iright) right)  \n\nwhere the variance is computed over the samples z_1 ldots z_m sim q_lambda. Differentiating the VarGrad objective corresponds to the canonical score gradient combined with the \"leave-one-out\" control variate[SK2014][KvHW2019].\n\n[JGJS1999]: Jordan, M. I., Ghahramani, Z., Jaakkola, T. S., & Saul, L. K. (1999). An introduction to variational methods for graphical models. Machine learning, 37, 183-233.\n\n[G1990]: Glynn, P. W. (1990). Likelihood ratio gradient estimation for stochastic systems. Communications of the ACM, 33(10), 75-84.\n\n[KR1996]: Kleijnen, J. P., & Rubinstein, R. Y. (1996). Optimization and sensitivity analysis of computer simulation models by the score function method. European Journal of Operational Research, 88(3), 413-427.\n\n[RSU1996]: Rubinstein, R. Y., Shapiro, A., & Uryasev, S. (1996). The score function method. Encyclopedia of Management Sciences, 1363-1366.\n\n[W1992]: Williams, R. J. (1992). Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8, 229-256.\n\n[WW2013]: Wingate, D., & Weber, T. (2013). Automated variational inference in probabilistic programming. arXiv preprint arXiv:1301.1299.\n\n[RGB2014]: Ranganath, R., Gerrish, S., & Blei, D. (2014). Black box variational inference. In Artificial intelligence and statistics (pp. 814-822). PMLR.\n\n[KvHW2019]: Kool, W., van Hoof, H., & Welling, M. (2019). Buy 4 reinforce samples, get a baseline for free!.","category":"section"},{"location":"klminscoregraddescent/#AdvancedVI.KLMinScoreGradDescent","page":"KLMinScoreGradDescent","title":"AdvancedVI.KLMinScoreGradDescent","text":"KLMinScoreGradDescent(adtype; optimizer, n_samples, averager, operator)\n\nKL divergence minimization by running stochastic gradient descent with the score gradient in the Euclidean space of variational parameters.\n\nnote: Note\nIf a <:MvLocationScale variational family is used, for operator, IdentityOperator should be avoided since optimization can result in a singular scale matrix. Instead, consider using ClipScale.\n\nArguments\n\nadtype: Automatic differentiation backend. \n\nKeyword Arguments\n\noptimizer::Optimisers.AbstractRule: Optimization algorithm to be used. Only DoG, DoWG and Optimisers.Descent are supported. (default: DoWG())\nn_samples::Int: Number of Monte Carlo samples to be used for estimating each gradient.\naverager::AbstractAverager: Parameter averaging strategy. (default: PolynomialAveraging())\noperator::Union{<:IdentityOperator, <:ClipScale}: Operator to be applied after each gradient descent step. (default: IdentityOperator())\nsubsampling::Union{<:Nothing,<:AbstractSubsampling}: Data point subsampling strategy. If nothing, subsampling is not used. (default: nothing)\n\nOutput\n\nq_averaged: The variational approximation formed by the averaged SGD iterates.\n\nCallback\n\nThe callback function supplied to optimize needs to have the following signature:\n\ncallback(; rng, iteration, restructure, params, averaged_params, restructure, gradient)\n\nThe keyword arguments are as follows:\n\nrng: Random number generator internally used by the algorithm.\niteration: The index of the current iteration.\nrestructure: Function that restructures the variational approximation from the variational parameters. Calling restructure(params) reconstructs the current variational approximation. \nparams: Current variational parameters.\naveraged_params: Variational parameters averaged according to the averaging strategy.\ngradient: The estimated (possibly stochastic) gradient.\n\nRequirements\n\nThe trainable parameters in the variational approximation are expected to be extractable through Optimisers.destructure. This requires the variational approximation to be marked as a functor through Functors.@functor.\nThe variational approximation q_lambda implements rand.\nThe variational approximation q_lambda implements logpdf(q, x), which should also be differentiable with respect to x.\nThe target distribution and the variational approximation have the same support.\n\n\n\n\n\n","category":"type"},{"location":"klminscoregraddescent/#AdvancedVI.estimate_objective-Tuple{AbstractRNG, Union{KLMinRepGradDescent, KLMinRepGradProxDescent, KLMinScoreGradDescent}, Any, Any}-klminscoregraddescent","page":"KLMinScoreGradDescent","title":"AdvancedVI.estimate_objective","text":"estimate_objective([rng,] alg, q, prob; n_samples, entropy)\n\nEstimate the negative ELBO of the variational approximation q against the target log-density prob.\n\nArguments\n\nrng::Random.AbstractRNG: Random number generator.\nalg::Union{<:KLMinRepGradDescent,<:KLMinRepGradProxDescent,<:KLMinScoreGradDescent}: Variational inference algorithm.\nq: Variational approximation.\nprob: The target log-joint likelihood implementing the LogDensityProblem interface.\n\nKeyword Arguments\n\nn_samples::Int: Number of Monte Carlo samples for estimating the objective. (default: Same as the the number of samples used for estimating the gradient during optimization.)\nentropy::AbstractEntropyEstimator: Entropy estimator. (default: MonteCarloEntropy())\n\nReturns\n\nobj_est: Estimate of the objective value.\n\n\n\n\n\n","category":"method"},{"location":"tutorials/basic/#basic","page":"Basic Example","title":"Basic Example","text":"In this tutorial, we will demonstrate the basic usage of AdvancedVI with LogDensityProblem interface.","category":"section"},{"location":"tutorials/basic/#Problem-Setup","page":"Basic Example","title":"Problem Setup","text":"Let's consider a basic logistic regression example with a hierarchical prior. For a dataset (X y) with the design matrix X in mathbbR^n times d and the response variables y in 0 1^n, we assume the following data generating process:\n\nbeginaligned\nsigma sim textLogNormal(0 3) \nbeta sim textNormalleft(0_d sigma^2 mathrmI_dright) \ny sim mathrmBernoulliLogitleft(X betaright)\nendaligned\n\nThe LogDensityProblem corresponding to this model can be constructed as\n\nusing LogDensityProblems: LogDensityProblems\nusing Distributions\nusing FillArrays\n\nstruct LogReg{XType,YType}\n    X::XType\n    y::YType\nend\n\nfunction LogDensityProblems.logdensity(model::LogReg, θ)\n    (; X, y) = model\n    d = size(X, 2)\n    β, σ = θ[1:d], θ[end]\n\n    logprior_β = logpdf(MvNormal(Zeros(d), σ), β)\n    logprior_σ = logpdf(LogNormal(0, 3), σ)\n\n    logit = X*β\n    loglike_y = mapreduce((li, yi) -> logpdf(BernoulliLogit(li), yi), +, logit, y)\n    return loglike_y + logprior_β + logprior_σ\nend\n\nfunction LogDensityProblems.dimension(model::LogReg)\n    return size(model.X, 2) + 1\nend\n\nfunction LogDensityProblems.capabilities(::Type{<:LogReg})\n    return LogDensityProblems.LogDensityOrder{0}()\nend\nnothing\n\nSince the support of σ is constrained to be positive and most VI algorithms assume an unconstrained Euclidean support, we need to use a bijector to transform θ. We will use Bijectors for this purpose. This corresponds to the automatic differentiation variational inference (ADVI) formulation[KTRGB2017].\n\nIn our case, we need a bijector that applies an identity map for the first size(X,2) coordinates, and map the last coordinate to the support of LogNormal(0, 3). This can be done as  follows:\n\n[KTRGB2017]: Kucukelbir, A., Tran, D., Ranganath, R., Gelman, A., & Blei, D. M. (2017). Automatic differentiation variational inference. Journal of machine learning research.\n\nusing Bijectors: Bijectors\n\nfunction Bijectors.bijector(model::LogReg)\n    d = size(model.X, 2)\n    return Bijectors.Stacked(\n        Bijectors.bijector.([MvNormal(Zeros(d), 1.0), LogNormal(0, 3)]),\n        [1:d, (d + 1):(d + 1)],\n    )\nend\nnothing\n\nFor more details, please refer to the documentation of Bijectors.\n\nFor the dataset, we will use the popular sonar classification dataset from the UCI repository. This can be automatically downloaded using OpenML. The sonar dataset corresponds to the dataset id 40.\n\nusing OpenML: OpenML\nusing DataFrames: DataFrames\ndata = Array(DataFrames.DataFrame(OpenML.load(40)))\nX = Matrix{Float64}(data[:, 1:(end - 1)])\ny = Vector{Bool}(data[:, end] .== \"Mine\")\nnothing\n\nLet's apply some basic pre-processing and add an intercept column:\n\nusing Statistics\n\nX = (X .- mean(X; dims=2)) ./ std(X; dims=2)\nX = hcat(X, ones(size(X, 1)))\nnothing\n\nThe model can now be instantiated as follows:\n\nmodel = LogReg(X, y)\nnothing","category":"section"},{"location":"tutorials/basic/#Basic-Usage","page":"Basic Example","title":"Basic Usage","text":"For the VI algorithm, we will use KLMinRepGradDescent:\n\nusing ADTypes, ReverseDiff\nusing AdvancedVI\n\nalg = KLMinRepGradDescent(ADTypes.AutoReverseDiff(); operator=ClipScale());\nnothing\n\nThis algorithm minimizes the exclusive/reverse KL divergence via stochastic gradient descent in the (Euclidean) space of the parameters of the variational approximation with the reparametrization gradient[TL2014][RMW2014][KW2014]. This is also commonly referred as automatic differentiation VI, black-box VI, stochastic gradient VI, and so on.\n\nFor certain algorithms such as KLMinRepGradDescent, projection or proximal operators can be used through the keyword argument operator. For this example, we will use Gaussian variational family, which is part of the more broad location-scale family. Location-scale family distributions require the scale matrix to have strictly positive eigenvalues at all times. Here, the projection operator ClipScale ensures this.\n\nKLMinRepGradDescent, in particular, assumes that the target LogDensityProblem is differentiable. If the LogDensityProblem has a differentiation capability of at least first-order, we can take advantage of this. For this example, we will use LogDensityProblemsAD to equip our problem with a first-order capability:\n\n[TL2014]: Titsias, M., & Lázaro-Gredilla, M. (2014, June). Doubly stochastic variational Bayes for non-conjugate inference. In International Conference on Machine Learning. PMLR.\n\n[RMW2014]: Rezende, D. J., Mohamed, S., & Wierstra, D. (2014, June). Stochastic backpropagation and approximate inference in deep generative models. In International Conference on Machine Learning. PMLR.\n\n[KW2014]: Kingma, D. P., & Welling, M. (2014). Auto-encoding variational bayes. In International Conference on Learning Representations.\n\nusing DifferentiationInterface: DifferentiationInterface\nusing LogDensityProblemsAD: LogDensityProblemsAD\n\nmodel_ad = LogDensityProblemsAD.ADgradient(ADTypes.AutoReverseDiff(), model)\nnothing\n\nFor the variational family, we will consider a FullRankGaussian approximation:\n\nusing LinearAlgebra\n\nd = LogDensityProblems.dimension(model_ad)\nq = FullRankGaussian(zeros(d), LowerTriangular(Matrix{Float64}(0.37*I, d, d)))\nnothing\n\nNow, KLMinRepGradDescent requires the variational approximation and the target log-density to have the same support. Since y follows a log-normal prior, its support is bounded to be the positive half-space mathbbR_+. Thus, we will use Bijectors to match the support of our target posterior and the variational approximation. The bijector can now be applied to q to match the support of the target problem.\n\nb = Bijectors.bijector(model)\nbinv = Bijectors.inverse(b)\nq_transformed = Bijectors.TransformedDistribution(q, binv)\nnothing\n\nWe can now run VI:\n\nmax_iter = 10^4\nq_out, info, _ = AdvancedVI.optimize(\n    alg, max_iter, model_ad, q_transformed; show_progress=false\n)\nnothing\n\nLet's verify that the optimization procedure converged. For this, we will visually inspect that the maximization objective of KLMinRepGradDescent, the \"evidence lower bound\" (ELBO) increased. Since KLMinRepGradDescent stores the ELBO estimate at each iteration in info, we can visualize this as follows:\n\nusing Plots\n\nplot(\n    [i.iteration for i in info],\n    [i.elbo for i in info];\n    xlabel=\"Iteration\",\n    ylabel=\"ELBO\",\n    label=nothing,\n)\nsavefig(\"basic_example_elbo.svg\")\nnothing\n\n(Image: )","category":"section"},{"location":"tutorials/basic/#Custom-Callback","page":"Basic Example","title":"Custom Callback","text":"The ELBO estimates above however, use only a handful of Monte Carlo samples. Furthermore, the ELBO is evaluated on the iterates of the optimization procedure, which may not coincide with the actual output of the algorithm. (For instance, if parameter averaging is used.) Therefore, we may want to occasionally estimate higher resolution ELBO estimates. Also, depending on the problem, we may want to monitor some problem-specific diagnostics for monitoring the progress.\n\nFor both use cases above, defining a custom callback function can be useful. In this example, we will compute a more accurate estimate of the ELBO and the classification accuracy every logging_interval = 10 iterations.\n\nusing StatsFuns: StatsFuns\n\n\"\"\"\n    logistic_prediction(X, μ_β, Σ_β)\n    \nApproximate the posterior predictive probability for a logistic link function using Mackay's approximation (Bishop p. 220).\n\"\"\"\nfunction logistic_prediction(X, μ_β, Σ_β)\n    xtΣx = sum((model.X*Σ_β) .* model.X; dims=2)[:, 1]\n    κ = @. 1/sqrt(1 + π/8*xtΣx)\n    return StatsFuns.logistic.(κ .* X*μ_β)\nend\n\nlogging_interval = 100\nfunction callback(; iteration, averaged_params, restructure, kwargs...)\n    if mod(iteration, logging_interval) == 1\n\n        # Use the averaged parameters (the eventual output of the algorithm)\n        q_avg = restructure(averaged_params)\n\n        # Compute predictions\n        μ_β = mean(q_avg.dist)[1:(end - 1)] # posterior mean of β\n        Σ_β = cov(q_avg.dist)[1:(end - 1), end - 1] # marginal posterior covariance of β\n        y_pred = logistic_prediction(X, μ_β, Σ_β) .> 0.5\n\n        # Prediction accuracy\n        acc = mean(y_pred .== model.y)\n\n        # Higher fidelity estimate of the ELBO on the averaged parameters\n        n_samples = 256\n        elbo_callback = -estimate_objective(alg, q_avg, model; n_samples)\n\n        (elbo_callback=elbo_callback, accuracy=acc)\n    else\n        nothing\n    end\nend\nnothing\n\nNote that the interface for the callback function will depend on the VI algorithm being used. Therefore, please refer to the documentation of each VI algorithm.\n\nThe callback can be supplied to optimize:\n\nmax_iter = 10^4\nq_out, info, _ = AdvancedVI.optimize(\n    alg, max_iter, model_ad, q_transformed; show_progress=false, callback=callback\n)\nnothing\n\nFirst, let's compare the default estimate of the ELBO, which uses a small number of samples and is evaluated in the current iterate, versus the ELBO computed in the callback, which uses a large number of samples and is evaluated on the averaged iterate.\n\nt = 1:max_iter\nelbo = [i.elbo for i in info[t]]\n\nt_callback = 1:logging_interval:max_iter\nelbo_callback = [i.elbo_callback for i in info[t_callback]]\n\nplot(t, elbo; xlabel=\"Iteration\", ylabel=\"ELBO\", label=\"Default\")\nplot!(t_callback, elbo_callback; label=\"Callback\", ylims=(-300, Inf), linewidth=2)\n\nsavefig(\"basic_example_elbo_callback.svg\")\nnothing\n\n(Image: )\n\nWe can see that the default ELBO estimates are noisy compared to the higher fidelity estimates from the callback. After a few thousands of iterations, it is difficult to judge if we are still making progress or not. In contrast, the estimates from callback show that the objective is increasing smoothly.\n\nSimilarly, we can monitor the evolution of the prediction accuracy.\n\nacc_callback = [i.accuracy for i in info[t_callback]]\nplot(\n    t_callback,\n    acc_callback;\n    xlabel=\"Iteration\",\n    ylabel=\"Prediction Accuracy\",\n    label=nothing,\n)\nsavefig(\"basic_example_acc.svg\")\nnothing\n\n(Image: )\n\nClearly, the accuracy is improving over time.","category":"section"},{"location":"general/#general","page":"General Usage","title":"General Usage","text":"AdvancedVI provides multiple variational inference (VI) algorithms. Each algorithm defines its subtype of AdvancedVI.AbstractVariationalAlgorithm with some corresponding methods (see this section). Then the algorithm can be executed by invoking optimize. (See this section).","category":"section"},{"location":"general/#optimize","page":"General Usage","title":"Running Variational Inference","text":"Given a subtype of AbstractVariationalAlgorithm associated with each algorithm, it suffices to call the function optimize:\n\nEach algorithm may interact differently with the arguments of optimize. Therefore, please refer to the documentation of each different algorithm for a detailed description on their behavior and their requirements.","category":"section"},{"location":"general/#estimate_objective","page":"General Usage","title":"Monitoring the Objective Value","text":"Furthermore, each algorithm has an associated variational objective subject to minimization. (By convention, we assume all objectives are minimized rather than maximized.) The progress made by each optimization algorithm can be diagnosed by monitoring the variational objective value. This can be done by calling the following method.","category":"section"},{"location":"general/#algorithm","page":"General Usage","title":"Algorithm Interface","text":"A variational inference algorithm supported by AdvancedVI should define its own subtype of AbstractVariationalAlgorithm:\n\nThe functionality of each algorithm is then implemented through the following methods:\n\nThe role of each method should be self-explanatory and should be clear once we take a look at how optimize interacts with each algorithm. The operation of optimize can be simplified as follows:\n\nfunction optimize(rng, algorithm, max_iter, q_init, objargs; kwargs...)\n    info_total = NamedTuple[]\n    state = init(rng, algorithm, q_init, prob)\n    for t in 1:max_iter\n        info = (iteration=t,)\n        state, terminate, info′ = step(\n            rng, algorithm, state, callback, objargs...; kwargs...\n        )\n        info = merge(info′, info)\n\n        if terminate\n            break\n        end\n\n        push!(info_total, info)\n    end\n    out = output(algorithm, state)\n    return out, info_total, state\nend","category":"section"},{"location":"general/#AdvancedVI.optimize","page":"General Usage","title":"AdvancedVI.optimize","text":"optimize(\n    [rng::Random.AbstractRNG = Random.default_rng(),]\n    algorithm::AbstractVariationalAlgorithm,\n    max_iter::Int,\n    prob,\n    q_init,\n    args...;\n    kwargs...\n)\n\nRun variational inference algorithm on the problem implementing the LogDensityProblems interface. For more details on the usage, refer to the documentation corresponding to algorithm.\n\nArguments\n\nrng: Random number generator.\nalgorithm: Variational inference algorithm.\nmax_iter::Int: Maximum number of iterations.\nprob: Target LogDensityProblem \nq_init: Initial variational distribution.\nargs...: Arguments to be passed to algorithm.\n\nKeyword Arguments\n\nshow_progress::Bool: Whether to show the progress bar. (Default: true.)\nstate::Union{<:Any,Nothing}: Initial value for the internal state of optimization. Used to warm-start from the state of a previous run. (See the returned values below.)\ncallback: Callback function called after every iteration. See further information below. (Default: nothing.)\nprogress::ProgressMeter.AbstractProgress: Progress bar configuration. (Default: ProgressMeter.Progress(n_max_iter; desc=\"Optimizing\", barlen=31, showspeed=true, enabled=prog).)\nkwargs...: Keyword arguments to be passed to algorithm.\n\nReturns\n\noutput: The output of the variational inference algorithm.\ninfo: Array of NamedTuples, where each NamedTuple contains information generated at each iteration.\nstate: Collection of the final internal states of optimization. This can used later to warm-start from the last iteration of the corresponding run.\n\nCallback\n\nThe signature of the callback function depends on the algorithm in use. Thus, see the documentation for each algorithm. However, a callback should return either a nothing or a NamedTuple containing information generated during the current iteration. The content of the NamedTuple will be concatenated into the corresponding entry in the info array returns in the end of the call to optimize and will be displayed on the progress meter.\n\n\n\n\n\n","category":"function"},{"location":"general/#AdvancedVI.estimate_objective-Tuple{AbstractRNG, AdvancedVI.AbstractVariationalAlgorithm, Any, Any}","page":"General Usage","title":"AdvancedVI.estimate_objective","text":"estimate_objective([rng,] alg, q, prob; kwargs...)\n\nEstimate the variational objective to be minimized by the algorithm alg for approximating the target prob with the variational approximation q.\n\nArguments\n\nrng::Random.AbstractRNG: Random number generator.\nalg::AbstractVariationalAlgorithm: Variational inference algorithm.\nprob: The target log-joint likelihood implementing the LogDensityProblem interface.\nq: Variational approximation.\n\nKeyword Arguments\n\nDepending on the algorithm, additional keyword arguments may apply. Please refer to the respective documentation of each algorithm for more info.\n\nReturns\n\nobj_est: Estimate of the objective value.\n\n\n\n\n\n","category":"method"},{"location":"general/#AdvancedVI.AbstractVariationalAlgorithm","page":"General Usage","title":"AdvancedVI.AbstractVariationalAlgorithm","text":"AbstractVariationalAlgorithm\n\nAbstract type for a variational inference algorithm.\n\n\n\n\n\n","category":"type"},{"location":"general/#AdvancedVI.init-Tuple{AbstractRNG, AdvancedVI.AbstractVariationalAlgorithm, Any, Any}","page":"General Usage","title":"AdvancedVI.init","text":"init(rng, alg, q_init, prob)\n\nInitialize alg given the initial variational approximation q_init and the target prob.\n\nArguments\n\nrng::Random.AbstractRNG: Random number generator.\nalg::AbstractVariationalAlgorithm: Variational inference algorithm.\nq_init: Initial variational approximation.\nprob: Target problem.\n\nReturns\n\nstate: Initial state of the algorithm.\n\n\n\n\n\n","category":"method"},{"location":"general/#AdvancedVI.step","page":"General Usage","title":"AdvancedVI.step","text":"step(rng, alg, state, callback, objargs...; kwargs...)\n\nPerform a single step of alg given the previous state.\n\nArguments\n\nrng::Random.AbstractRNG: Random number generator.\nalg::AbstractVariationalAlgorithm: Variational inference algorithm.\nstate: Previous state of the algorithm.\ncallback: Callback function to be called during the step.\n\nReturns\n\nstate: New state generated by performing the step.\nterminate::Bool: Whether to terminate the algorithm after the step.\ninfo::NamedTuple: Information generated during the step. \n\n\n\n\n\n","category":"function"},{"location":"general/#AdvancedVI.output","page":"General Usage","title":"AdvancedVI.output","text":"output(alg, state)\n\nOutput a variational approximation from the last state of alg.\n\nArguments\n\nalg::AbstractVariationalAlgorithm: Variational inference algorithm used to compute the state.\nstate: The last state generated by the algorithm.\n\nReturns\n\nout: The output of the algorithm. \n\n\n\n\n\n","category":"function"},{"location":"klminsqrtnaturalgraddescent/#klminsqrtnaturalgraddescent","page":"KLMinSqrtNaturalGradDescent","title":"KLMinSqrtNaturalGradDescent","text":"","category":"section"},{"location":"klminsqrtnaturalgraddescent/#Description","page":"KLMinSqrtNaturalGradDescent","title":"Description","text":"This algorithm aims to minimize the exclusive (or reverse) Kullback-Leibler (KL) divergence by running natural gradient descent. KLMinSqrtNaturalGradDescent is a specific implementation of natural gradient variational inference (NGVI) also known as square-root variational Newton[KMKL2025][LDEBTM2024][LDLNKS2023][T2025]. This algorithm operates under the square-root or Cholesky factorization of the covariance matrix parameterization. This contrasts with KLMinNaturalGradDescent, which operates in the precision matrix parameterization, requiring a matrix inverse at each step. As a result, the cost of KLMinSqrtNaturalGradDescent should be relatively cheaper. Since KLMinSqrtNaturalGradDescent is a measure-space algorithm, its use is restricted to full-rank Gaussian variational families (FullRankGaussian) that make the updates tractable.\n\nThe associated objective value can be estimated through the following:\n\n[KMKL2025]: Kumar, N., Möllenhoff, T., Khan, M. E., & Lucchi, A. (2025). Optimization Guarantees for Square-Root Natural-Gradient Variational Inference. Transactions of Machine Learning Research.\n\n[LDEBTM2024]: Lin, W., Dangel, F., Eschenhagen, R., Bae, J., Turner, R. E., & Makhzani, A. (2024). Can We Remove the Square-Root in Adaptive Gradient Methods? A Second-Order Perspective. In International Conference on Machine Learning.\n\n[LDLNKS2023]: Lin, W., Duruisseaux, V., Leok, M., Nielsen, F., Khan, M. E., & Schmidt, M. (2023). Simplifying momentum-based positive-definite submanifold optimization with applications to deep learning. In International Conference on Machine Learning.\n\n[T2025]: Tan, L. S. (2025). Analytic natural gradient updates for Cholesky factor in Gaussian variational approximation. Journal of the Royal Statistical Society: Series B.","category":"section"},{"location":"klminsqrtnaturalgraddescent/#klminsqrtnaturalgraddescent_method","page":"KLMinSqrtNaturalGradDescent","title":"Methodology","text":"This algorithm aims to solve the problem\n\n  mathrmminimize_q_lambda in mathcalQquad mathrmKLleft(q_lambda piright)\n\nwhere mathcalQ is some family of distributions, often called the variational family, by running stochastic gradient descent in the (Euclidean) space of parameters. That is, for all q_lambda in mathcalQ, we assume q_lambda there is a corresponding vector of parameters lambda in Lambda, where the space of parameters is Euclidean such that Lambda subset mathbbR^p.\n\nSince we usually only have access to the unnormalized densities of the target distribution pi, we don't have direct access to the KL divergence. Instead, the ELBO maximization strategy minimizes a surrogate objective, the negative evidence lower bound[JGJS1999]\n\n  mathcalLleft(qright) triangleq mathbbE_theta sim q -log pileft(thetaright) - mathbbHleft(qright)\n\nwhich is equivalent to the KL up to an additive constant (the evidence).\n\nWhile KLMinSqrtNaturalGradDescent is close to a natural gradient variational inference algorithm, it can be derived in a variety of different ways. In fact, the update rule has been concurrently developed by several research groups[KMKL2025][LDEBTM2024][LDLNKS2023][T2025]. Here, we will present the derivation by Kumar et al. [KMKL2025]. Consider the ideal natural gradient descent algorithm discussed here. This can be viewed as a discretization of the continuous-time dynamics given by the differential equation\n\ndotlambda_t\n=\nF(lambda)^-1 nabla_lambda mathcalLleft(q_lambdaright) \n\nThis is also known as the natural gradient flow. Notice that the flow is over the parameters lambda_t. Therefore, the natural gradient flow depends on the way we parametrize q_lambda. For Gaussian variational families, if we specifically choose the square-root (or Cholesky) parametrization such that q_lambda_t = mathrmNormal(m_t C_t C_t), the flow of lambda_t = (m_t C_t) given as\n\nbeginalign*\ndotm_t = C_t C_t^top mathbbE_q_lambda_t left nabla log pi right \n\ndotC_t = C_t Mleft( mathrmI_d + C_t^top mathbbEleft nabla^2 log pi right C_t right) \nendalign*  \n\nwhere M is a mathrmtril-like function defined as\n\n M(A) _ij = begincases\n    0  textif i  j \n    frac12 A_ii  textif i = j \n    A_ij  textif i  j \nendcases\n\nKLMinSqrtNaturalGradDescent corresponds to the forward Euler discretization of this flow.\n\n[JGJS1999]: Jordan, M. I., Ghahramani, Z., Jaakkola, T. S., & Saul, L. K. (1999). An introduction to variational methods for graphical models. Machine learning, 37, 183-233.","category":"section"},{"location":"klminsqrtnaturalgraddescent/#AdvancedVI.KLMinSqrtNaturalGradDescent","page":"KLMinSqrtNaturalGradDescent","title":"AdvancedVI.KLMinSqrtNaturalGradDescent","text":"KLMinSqrtNaturalGradDescent(stepsize, n_samples, subsampling)\nKLMinSqrtNaturalGradDescent(; stepsize, n_samples, subsampling)\n\nKL divergence minimization algorithm obtained by discretizing the natural gradient flow (the Riemannian gradient flow with the Fisher information matrix as the metric tensor) under the square-root parameterization[KMKL2025][LDENKTM2024][LDLNKS2023][T2025].\n\nThis algorithm requires second-order information about the target. If the target LogDensityProblem has second-order differentiation capability, Hessians are used. Otherwise, if the target has only first-order capability, it will use only gradients but this will probably result in slower convergence and less robust behavior.\n\n(Keyword) Arguments\n\nstepsize::Float64: Step size.\nn_samples::Int: Number of samples used to estimate the natural gradient. (default: 1)\nsubsampling::Union{Nothing,<:AbstractSubsampling}: Optional subsampling strategy.\n\nnote: Note\nThe subsampling strategy is only applied to the target LogDensityProblem but not to the variational approximation q. That is, KLMinSqrtNaturalGradDescent does not support amortization or structured variational families.\n\nOutput\n\nq: The last iterate of the algorithm.\n\nCallback Signature\n\nThe callback function supplied to optimize needs to have the following signature:\n\ncallback(; rng, iteration, q, info)\n\nThe keyword arguments are as follows:\n\nrng: Random number generator internally used by the algorithm.\niteration: The index of the current iteration.\nq: Current variational approximation.\ninfo: NamedTuple containing the information generated during the current iteration.\n\nRequirements\n\nThe variational family is FullRankGaussian.\nThe target distribution has unconstrained support.\nThe target LogDensityProblems.logdensity(prob, x) has at least first-order differentiation capability.\n\n\n\n\n\n","category":"type"},{"location":"klminsqrtnaturalgraddescent/#AdvancedVI.estimate_objective-Tuple{AbstractRNG, KLMinWassFwdBwd, MvLocationScale, Any}-klminsqrtnaturalgraddescent","page":"KLMinSqrtNaturalGradDescent","title":"AdvancedVI.estimate_objective","text":"estimate_objective([rng,] alg, q, prob; kwargs...)\n\nEstimate the variational objective to be minimized by the algorithm alg for approximating the target prob with the variational approximation q.\n\nArguments\n\nrng::Random.AbstractRNG: Random number generator.\nalg::AbstractVariationalAlgorithm: Variational inference algorithm.\nprob: The target log-joint likelihood implementing the LogDensityProblem interface.\nq: Variational approximation.\n\nKeyword Arguments\n\nDepending on the algorithm, additional keyword arguments may apply. Please refer to the respective documentation of each algorithm for more info.\n\nReturns\n\nobj_est: Estimate of the objective value.\n\n\n\n\n\n","category":"method"},{"location":"#AdvancedVI","page":"AdvancedVI","title":"AdvancedVI","text":"AdvancedVI provides implementations of variational Bayesian inference (VI) algorithms. VI algorithms perform scalable and computationally efficient Bayesian inference at the cost of asymptotic exactness. AdvancedVI is part of the Turing probabilistic programming ecosystem.\n\nFor general usage, first refer to the following page:\n\nGeneral Usage\n\nFor using the algorithms implemented in AdvancedVI, refer to the corresponding documentations below:\n\nKLMinRepGradDescent (alias of ADVI)\nKLMinRepGradProxDescent\nKLMinScoreGradDescent  (alias of BBVI)\nKLMinNaturalGradDescent\nKLMinSqrtNaturalGradDescent\nKLMinWassFwdBwd\nFisherMinBatchMatch","category":"section"}]
}
