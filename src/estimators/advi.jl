
struct ADVI{TlogÏ€, B} <: AbstractGradientEstimator
    # Automatic differentiation variational inference
    # 
    # Kucukelbir, A., Tran, D., Ranganath, R., Gelman, A., & Blei, D. M. (2017).
    # Automatic differentiation variational inference.
    # Journal of machine learning research.

    â„“Ï€::TlogÏ€
    bâ»Â¹::B
    n_samples::Int

    function ADVI(prob, bâ»Â¹::B, n_samples; kwargs...) where {B <: Bijectors.Inverse{<:Bijectors.Bijector}}
        # Could check whether the support of bâ»Â¹ and â„“Ï€ match
        cap = LogDensityProblems.capabilities(prob)
        if cap === nothing
            throw(
                ArgumentError(
                    "The log density function does not support the LogDensityProblems.jl interface",
                ),
            )
        end
        â„“Ï€ = Base.Fix1(LogDensityProblems.logdensity, prob)
        new{typeof(â„“Ï€), typeof(bâ»Â¹)}(â„“Ï€, bâ»Â¹, n_samples)
    end
end

ADVI(prob, n_samples; kwargs...) = ADVI(prob, identity, n_samples; kwargs...)

objective(::ADVI) = "ELBO"

function estimate_gradient!(
    rng::Random.AbstractRNG,
    estimator::ADVI,
    Î»::Vector{<:Real},
    rebuild,
    out::DiffResults.MutableDiffResult)

    n_samples = estimator.n_samples

    grad!(ADBackend(), Î», out) do Î»â€²
        q_Î· = rebuild(Î»â€²)
        Î·s  = rand(rng, q_Î·, estimator.n_samples)

        zs, âˆ‘logdetjac = Bijectors.with_logabsdet_jacobian(estimator.bâ»Â¹, Î·s)

        ğ”¼logÏ€ = mapreduce(+, eachcol(zs)) do záµ¢
            estimator.â„“Ï€(záµ¢) / n_samples
        end
        ğ”¼logdetjac = âˆ‘logdetjac/n_samples

        elbo = ğ”¼logÏ€ + ğ”¼logdetjac + entropy(q_Î·)
        -elbo
    end
    nelbo = DiffResults.value(out)
    (elbo=-nelbo,)
end
