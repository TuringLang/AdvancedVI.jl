
struct ADVI{TlogÏ€, B} <: AbstractGradientEstimator
    # Automatic differentiation variational inference
    # 
    # Kucukelbir, A., Tran, D., Ranganath, R., Gelman, A., & Blei, D. M. (2017).
    # Automatic differentiation variational inference.
    # Journal of machine learning research.

    â„“Ï€::TlogÏ€
    bâ»Â¹::B
    n_samples::Int

    function ADVI(prob, bâ»Â¹, n_samples; kwargs...)
        # Could check whether the support of bâ»Â¹ and â„“Ï€ match
        cap = LogDensityProblems.capabilities(prob)
        if cap === nothing
            throw(
                ArgumentError(
                    "The log density function does not support the LogDensityProblems.jl interface",
                ),
            )
        end
        â„“Ï€ = Base.Fix1(LogDensityProblems.logdensity, prob)
        new{typeof(â„“Ï€), typeof(bâ»Â¹)}(â„“Ï€, bâ»Â¹, n_samples)
    end
end

ADVI(prob, n_samples; kwargs...) = ADVI(prob, identity, n_samples; kwargs...)

objective(::ADVI) = "ELBO"

function estimate_gradient!(
    rng::Random.AbstractRNG,
    estimator::ADVI,
    Î»::Vector{<:Real},
    rebuild,
    out::DiffResults.MutableDiffResult)

    n_samples = estimator.n_samples

    grad!(ADBackend(), Î», out) do Î»â€²
        q_Î· = rebuild(Î»â€²)
        Î·s  = rand(rng, q_Î·, estimator.n_samples)

        ğ”¼â„“ = mapreduce(+, eachcol(Î·s)) do Î·áµ¢
            záµ¢, logdetjacáµ¢ = Bijectors.with_logabsdet_jacobian(estimator.bâ»Â¹, Î·áµ¢)
            (estimator.â„“Ï€(záµ¢) + logdetjacáµ¢) / n_samples
        end

        elbo = ğ”¼â„“ + entropy(q_Î·)
        -elbo
    end
    nelbo = DiffResults.value(out)
    (elbo=-nelbo,)
end
