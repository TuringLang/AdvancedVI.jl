# [Optimization](@id optim)

## Parameter-Free Optimization Rules

We provide custom optimization rules that are not provided out-of-the-box by [Optimisers.jl](https://github.com/FluxML/Optimisers.jl).
The main theme of the provided optimizers is that they are parameter-free.
This means that these optimization rules shouldn't require (or barely) any tuning to obtain performance competitive with well-tuned alternatives.

```@docs
DoG
DoWG
COCOB
```

## Parameter Averaging Strategies

In some cases, the best optimization performance is obtained by averaging the sequence of parameters generated by the optimization algorithm.
For instance, the `DoG`[^IHC2023] and `DoWG`[^KMJ2024] papers report their best performance through averaging.
The benefits of parameter averaging have been specifically confirmed for ELBO maximization[^DCAMHV2020].

```@docs
NoAveraging
PolynomialAveraging
```

[^DCAMHV2020]: Dhaka, A. K., Catalina, A., Andersen, M. R., Magnusson, M., Huggins, J., & Vehtari, A. (2020). Robust, accurate stochastic optimization for variational inference. Advances in Neural Information Processing Systems, 33, 10961-10973.
[^KMJ2024]: Khaled, A., Mishchenko, K., & Jin, C. (2023). Dowg unleashed: An efficient universal parameter-free gradient descent method. Advances in Neural Information Processing Systems, 36, 6748-6769.
[^IHC2023]: Ivgi, M., Hinder, O., & Carmon, Y. (2023). Dog is sgd's best friend: A parameter-free dynamic step size schedule. In International Conference on Machine Learning (pp. 14465-14499). PMLR.
## Operators

Depending on the variational family, variational objective, and optimization strategy, it might be necessary to modify the variational parameters after performing a gradient-based update.
For this, an operator acting on the parameters can be supplied via the  `operator` keyword argument of `AdvancedVI.optimize`.

### [`ClipScale`](@id clipscale)

For the location scale, it is often the case that optimization is stable only when the smallest eigenvalue of the scale matrix is strictly positive[^D2020].
To ensure this, we provide the following projection operator:

```@docs
ClipScale
```

[^D2020]: Domke, J. (2020). Provable smoothness guarantees for black-box variational inference. In *International Conference on Machine Learning*.
