<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Reparameterization Gradient Estimator · AdvancedVI.jl</title><meta name="title" content="Reparameterization Gradient Estimator · AdvancedVI.jl"/><meta property="og:title" content="Reparameterization Gradient Estimator · AdvancedVI.jl"/><meta property="twitter:title" content="Reparameterization Gradient Estimator · AdvancedVI.jl"/><meta name="description" content="Documentation for AdvancedVI.jl."/><meta property="og:description" content="Documentation for AdvancedVI.jl."/><meta property="twitter:description" content="Documentation for AdvancedVI.jl."/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../../">AdvancedVI.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">AdvancedVI</a></li><li><a class="tocitem" href="../../general/">General Usage</a></li><li><a class="tocitem" href="../../examples/">Examples</a></li><li><span class="tocitem">ELBO Maximization</span><ul><li><a class="tocitem" href="../overview/">Overview</a></li><li class="is-active"><a class="tocitem" href>Reparameterization Gradient Estimator</a><ul class="internal"><li><a class="tocitem" href="#Overview"><span>Overview</span></a></li><li><a class="tocitem" href="#The-RepGradELBO-Objective"><span>The <code>RepGradELBO</code> Objective</span></a></li><li><a class="tocitem" href="#bijectors"><span>Handling Constraints with <code>Bijectors</code></span></a></li><li><a class="tocitem" href="#entropygrad"><span>Entropy Estimators</span></a></li><li><a class="tocitem" href="#Advanced-Usage"><span>Advanced Usage</span></a></li></ul></li></ul></li><li><a class="tocitem" href="../../families/">Variational Families</a></li><li><a class="tocitem" href="../../optimization/">Optimization</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">ELBO Maximization</a></li><li class="is-active"><a href>Reparameterization Gradient Estimator</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Reparameterization Gradient Estimator</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/TuringLang/AdvancedVI.jl/blob/master/docs/src/elbo/repgradelbo.md#" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="repgradelbo"><a class="docs-heading-anchor" href="#repgradelbo">Reparameterization Gradient Estimator</a><a id="repgradelbo-1"></a><a class="docs-heading-anchor-permalink" href="#repgradelbo" title="Permalink"></a></h1><h2 id="Overview"><a class="docs-heading-anchor" href="#Overview">Overview</a><a id="Overview-1"></a><a class="docs-heading-anchor-permalink" href="#Overview" title="Permalink"></a></h2><p>The reparameterization gradient<sup class="footnote-reference"><a id="citeref-TL2014" href="#footnote-TL2014">[TL2014]</a></sup><sup class="footnote-reference"><a id="citeref-RMW2014" href="#footnote-RMW2014">[RMW2014]</a></sup><sup class="footnote-reference"><a id="citeref-KW2014" href="#footnote-KW2014">[KW2014]</a></sup> is an unbiased gradient estimator of the ELBO. Consider some variational family</p><p class="math-container">\[\mathcal{Q} = \{q_{\lambda} \mid \lambda \in \Lambda \},\]</p><p>where <span>$\lambda$</span> is the <em>variational parameters</em> of <span>$q_{\lambda}$</span>. If its sampling process can be described by some differentiable reparameterization function <span>$\mathcal{T}_{\lambda}$</span> and a <em>base distribution</em> <span>$\varphi$</span> independent of <span>$\lambda$</span> such that</p><p class="math-container">\[z \sim  q_{\lambda} \qquad\Leftrightarrow\qquad
z \stackrel{d}{=} \mathcal{T}_{\lambda}\left(\epsilon\right);\quad \epsilon \sim \varphi\]</p><p>we can effectively estimate the gradient of the ELBO by directly differentiating</p><p class="math-container">\[  \widehat{\mathrm{ELBO}}\left(\lambda\right) = \frac{1}{M}\sum^M_{m=1} \log \pi\left(\mathcal{T}_{\lambda}\left(\epsilon_m\right)\right) + \mathbb{H}\left(q_{\lambda}\right),\]</p><p>where <span>$\epsilon_m \sim \varphi$</span> are Monte Carlo samples, with respect to <span>$\lambda$</span>. This estimator is called the reparameterization gradient estimator.</p><p>In addition to the reparameterization gradient, <code>AdvancedVI</code> provides the following features:</p><ol><li><strong>Posteriors with constrained supports</strong> are handled through <a href="https://github.com/TuringLang/Bijectors.jl"><code>Bijectors</code></a>, which is known as the automatic differentiation VI (ADVI; <sup class="footnote-reference"><a id="citeref-KTRGB2017" href="#footnote-KTRGB2017">[KTRGB2017]</a></sup>) formulation. (See <a href="#bijectors">this section</a>.)</li><li><strong>The gradient of the entropy</strong> can be estimated through various strategies depending on the capabilities of the variational family. (See <a href="#entropygrad">this section</a>.)</li></ol><h2 id="The-RepGradELBO-Objective"><a class="docs-heading-anchor" href="#The-RepGradELBO-Objective">The <code>RepGradELBO</code> Objective</a><a id="The-RepGradELBO-Objective-1"></a><a class="docs-heading-anchor-permalink" href="#The-RepGradELBO-Objective" title="Permalink"></a></h2><p>To use the reparameterization gradient, <code>AdvancedVI</code> provides the following variational objective:</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="AdvancedVI.RepGradELBO" href="#AdvancedVI.RepGradELBO"><code>AdvancedVI.RepGradELBO</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">RepGradELBO(n_samples; kwargs...)</code></pre><p>Evidence lower-bound objective with the reparameterization gradient formulation<sup class="footnote-reference"><a id="citeref-TL2014" href="#footnote-TL2014">[TL2014]</a></sup><sup class="footnote-reference"><a id="citeref-RMW2014" href="#footnote-RMW2014">[RMW2014]</a></sup><sup class="footnote-reference"><a id="citeref-KW2014" href="#footnote-KW2014">[KW2014]</a></sup>. This computes the evidence lower-bound (ELBO) through the formulation:</p><p class="math-container">\[\begin{aligned}
\mathrm{ELBO}\left(\lambda\right)
&amp;\triangleq
\mathbb{E}_{z \sim q_{\lambda}}\left[
  \log \pi\left(z\right)
\right]
+ \mathbb{H}\left(q_{\lambda}\right),
\end{aligned}\]</p><p><strong>Arguments</strong></p><ul><li><code>n_samples::Int</code>: Number of Monte Carlo samples used to estimate the ELBO.</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>entropy</code>: The estimator for the entropy term. (Type <code>&lt;: AbstractEntropyEstimator</code>; Default: <code>ClosedFormEntropy()</code>)</li></ul><p><strong>Requirements</strong></p><ul><li>The variational approximation <span>$q_{\lambda}$</span> implements <code>rand</code>.</li><li>The target distribution and the variational approximation have the same support.</li><li>The target <code>logdensity(prob, x)</code> must be differentiable with respect to <code>x</code> by the selected AD backend.</li></ul><p>Depending on the options, additional requirements on <span>$q_{\lambda}$</span> may apply.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/TuringLang/AdvancedVI.jl/blob/f801aed489bff0492c43347efd2f4c61c0fef700/src/objectives/elbo/repgradelbo.jl#L2-L30">source</a></section></article><h2 id="bijectors"><a class="docs-heading-anchor" href="#bijectors">Handling Constraints with <code>Bijectors</code></a><a id="bijectors-1"></a><a class="docs-heading-anchor-permalink" href="#bijectors" title="Permalink"></a></h2><p>As mentioned in the docstring, the <code>RepGradELBO</code> objective assumes that the variational approximation <span>$q_{\lambda}$</span> and the target distribution <span>$\pi$</span> have the same support for all <span>$\lambda \in \Lambda$</span>.</p><p>However, in general, it is most convenient to use variational families that have the whole Euclidean space <span>$\mathbb{R}^d$</span> as their support. This is the case for the <a href="../../families/#locscale">location-scale distributions</a> provided by <code>AdvancedVI</code>. For target distributions which the support is not the full <span>$\mathbb{R}^d$</span>, we can apply some transformation <span>$b$</span> to <span>$q_{\lambda}$</span> to match its support such that</p><p class="math-container">\[z \sim  q_{b,\lambda} \qquad\Leftrightarrow\qquad
z \stackrel{d}{=} b^{-1}\left(\eta\right);\quad \eta \sim q_{\lambda},\]</p><p>where <span>$b$</span> is often called a <em>bijector</em>, since it is often chosen among bijective transformations. This idea is known as automatic differentiation VI<sup class="footnote-reference"><a id="citeref-KTRGB2017" href="#footnote-KTRGB2017">[KTRGB2017]</a></sup> and has subsequently been improved by Tensorflow Probability<sup class="footnote-reference"><a id="citeref-DLTBV2017" href="#footnote-DLTBV2017">[DLTBV2017]</a></sup>. In Julia, <a href="https://github.com/TuringLang/Bijectors.jl">Bijectors.jl</a><sup class="footnote-reference"><a id="citeref-FXTYG2020" href="#footnote-FXTYG2020">[FXTYG2020]</a></sup> provides a comprehensive collection of bijections.</p><p>One caveat of ADVI is that, after applying the bijection, a Jacobian adjustment needs to be applied. That is, the objective is now</p><p class="math-container">\[\mathrm{ADVI}\left(\lambda\right)
\triangleq
\mathbb{E}_{\eta \sim q_{\lambda}}\left[
  \log \pi\left( b^{-1}\left( \eta \right) \right)
  + \log \lvert J_{b^{-1}}\left(\eta\right) \rvert
\right]
+ \mathbb{H}\left(q_{\lambda}\right)\]</p><p>This is automatically handled by <code>AdvancedVI</code> through <code>TransformedDistribution</code> provided by <code>Bijectors.jl</code>. See the following example:</p><pre><code class="language-julia hljs">using Bijectors
q = MeanFieldGaussian(μ, L)
b = Bijectors.bijector(dist)
binv = inverse(b)
q_transformed = Bijectors.TransformedDistribution(q, binv)</code></pre><p>By passing <code>q_transformed</code> to <code>optimize</code>, the Jacobian adjustment for the bijector <code>b</code> is automatically applied. (See <a href="../../examples/#examples">Examples</a> for a fully working example.)</p><h2 id="entropygrad"><a class="docs-heading-anchor" href="#entropygrad">Entropy Estimators</a><a id="entropygrad-1"></a><a class="docs-heading-anchor-permalink" href="#entropygrad" title="Permalink"></a></h2><p>For the gradient of the entropy term, we provide three choices with varying requirements. The user can select the entropy estimator by passing it as a keyword argument when constructing the <code>RepGradELBO</code> objective.</p><table><tr><th style="text-align: left">Estimator</th><th style="text-align: center"><code>entropy(q)</code></th><th style="text-align: center"><code>logpdf(q)</code></th><th style="text-align: left">Type</th></tr><tr><td style="text-align: left"><code>ClosedFormEntropy</code></td><td style="text-align: center">required</td><td style="text-align: center"></td><td style="text-align: left">Deterministic</td></tr><tr><td style="text-align: left"><code>MonteCarloEntropy</code></td><td style="text-align: center"></td><td style="text-align: center">required</td><td style="text-align: left">Monte Carlo</td></tr><tr><td style="text-align: left"><code>StickingTheLandingEntropy</code></td><td style="text-align: center"></td><td style="text-align: center">required</td><td style="text-align: left">Monte Carlo with control variate</td></tr></table><p>The requirements mean that either <code>Distributions.entropy</code> or <code>Distributions.logpdf</code> need to be implemented for the choice of variational family. In general, the use of <code>ClosedFormEntropy</code> is recommended whenever possible. If <code>entropy</code> is not available, then <code>StickingTheLandingEntropy</code> is recommended. See the following section for more details.</p><h3 id="The-StickingTheLandingEntropy-Estimator"><a class="docs-heading-anchor" href="#The-StickingTheLandingEntropy-Estimator">The <code>StickingTheLandingEntropy</code> Estimator</a><a id="The-StickingTheLandingEntropy-Estimator-1"></a><a class="docs-heading-anchor-permalink" href="#The-StickingTheLandingEntropy-Estimator" title="Permalink"></a></h3><p>The <code>StickingTheLandingEntropy</code>, or STL estimator, is a control variate approach <sup class="footnote-reference"><a id="citeref-RWD2017" href="#footnote-RWD2017">[RWD2017]</a></sup>.</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="AdvancedVI.StickingTheLandingEntropy" href="#AdvancedVI.StickingTheLandingEntropy"><code>AdvancedVI.StickingTheLandingEntropy</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">StickingTheLandingEntropy()</code></pre><p>The &quot;sticking the landing&quot; entropy estimator<sup class="footnote-reference"><a id="citeref-RWD2017" href="#footnote-RWD2017">[RWD2017]</a></sup>.</p><p><strong>Requirements</strong></p><ul><li>The variational approximation <code>q</code> implements <code>logpdf</code>.</li><li><code>logpdf(q, η)</code> must be differentiable by the selected AD framework.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/TuringLang/AdvancedVI.jl/blob/f801aed489bff0492c43347efd2f4c61c0fef700/src/objectives/elbo/entropy.jl#L18-L26">source</a></section></article><p>It occasionally results in lower variance when <span>$\pi \approx q_{\lambda}$</span>, and higher variance when <span>$\pi \not\approx q_{\lambda}$</span>. The conditions for which the STL estimator results in lower variance is still an active subject for research.</p><p>The main downside of the STL estimator is that it needs to evaluate and differentiate the log density of <span>$q_{\lambda}$</span>, <code>logpdf(q)</code>, in every iteration. Depending on the variational family, this might be computationally inefficient or even numerically unstable. For example, if <span>$q_{\lambda}$</span> is a Gaussian with a full-rank covariance, a back-substitution must be performed at every step, making the per-iteration complexity <span>$\mathcal{O}(d^3)$</span> and reducing numerical stability.</p><p>The STL control variate can be used by changing the entropy estimator using the following object:</p><p>Let us come back to the example in <a href="../../examples/#examples">Examples</a>, where a <code>LogDensityProblem</code> is given as <code>model</code>. In this example, the true posterior is contained within the variational family. This setting is known as &quot;perfect variational family specification.&quot; In this case, the <code>RepGradELBO</code> estimator with <code>StickingTheLandingEntropy</code> is the only estimator known to converge exponentially fast (&quot;linear convergence&quot;) to the true solution.</p><p>Recall that the original ADVI objective with a closed-form entropy (CFE) is given as follows:</p><pre><code class="language-julia hljs">n_montecarlo = 16;
b = Bijectors.bijector(model);
binv = inverse(b)

q0_trans = Bijectors.TransformedDistribution(q0, binv)

cfe = AdvancedVI.RepGradELBO(n_montecarlo)
nothing</code></pre><p>The repgradelbo estimator can instead be created as follows:</p><pre><code class="language-julia hljs">repgradelbo = AdvancedVI.RepGradELBO(
    n_montecarlo; entropy=AdvancedVI.StickingTheLandingEntropy()
);
nothing</code></pre><p><img src="../advi_stl_elbo.svg" alt/></p><p>We can see that the noise of the repgradelbo estimator becomes smaller as VI converges. However, the speed of convergence may not always be significantly different. Also, due to noise, just looking at the ELBO may not be sufficient to judge which algorithm is better. This can be made apparent if we measure convergence through the distance to the optimum:</p><p><img src="../advi_stl_dist.svg" alt/></p><p>We can see that STL kicks-in at later stages of optimization. Therefore, when STL &quot;works&quot;, it yields a higher accuracy solution even on large stepsizes. However, whether STL works or not highly depends on the problem<sup class="footnote-reference"><a id="citeref-KMG2024" href="#footnote-KMG2024">[KMG2024]</a></sup>. Furthermore, in a lot of cases, a low-accuracy solution may be sufficient.</p><h2 id="Advanced-Usage"><a class="docs-heading-anchor" href="#Advanced-Usage">Advanced Usage</a><a id="Advanced-Usage-1"></a><a class="docs-heading-anchor-permalink" href="#Advanced-Usage" title="Permalink"></a></h2><p>There are two major ways to customize the behavior of <code>RepGradELBO</code></p><ul><li>Customize the <code>Distributions</code> functions: <code>rand(q)</code>, <code>entropy(q)</code>, <code>logpdf(q)</code>.</li><li>Customize <code>AdvancedVI.reparam_with_entropy</code>.</li></ul><p>It is generally recommended to customize <code>rand(q)</code>, <code>entropy(q)</code>, <code>logpdf(q)</code>, since it will easily compose with other functionalities provided by <code>AdvancedVI</code>.</p><p>The most advanced way is to customize <code>AdvancedVI.reparam_with_entropy</code>. In particular, <code>reparam_with_entropy</code> is the function that invokes <code>rand(q)</code>, <code>entropy(q)</code>, <code>logpdf(q)</code>. Thus, it is the most general way to override the behavior of <code>RepGradELBO</code>.</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="AdvancedVI.reparam_with_entropy" href="#AdvancedVI.reparam_with_entropy"><code>AdvancedVI.reparam_with_entropy</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">reparam_with_entropy(rng, q, q_stop, n_samples, ent_est)</code></pre><p>Draw <code>n_samples</code> from <code>q</code> and compute its entropy.</p><p><strong>Arguments</strong></p><ul><li><code>rng::Random.AbstractRNG</code>: Random number generator.</li><li><code>q</code>: Variational approximation.</li><li><code>q_stop</code>: Same as <code>q</code>, but held constant during differentiation. Should only be used for computing the entropy.</li><li><code>n_samples::Int</code>: Number of Monte Carlo samples </li><li><code>ent_est</code>: The entropy estimation strategy. (See <code>estimate_entropy</code>.)</li></ul><p><strong>Returns</strong></p><ul><li><code>samples</code>: Monte Carlo samples generated through reparameterization. Their support matches that of the target distribution.</li><li><code>entropy</code>: An estimate (or exact value) of the differential entropy of <code>q</code>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/TuringLang/AdvancedVI.jl/blob/f801aed489bff0492c43347efd2f4c61c0fef700/src/objectives/elbo/repgradelbo.jl#L52-L67">source</a></section></article><p>To illustrate how we can customize the <code>rand(q)</code> function, we will implement quasi-Monte-Carlo variational inference<sup class="footnote-reference"><a id="citeref-BWM2018" href="#footnote-BWM2018">[BWM2018]</a></sup>. Consider the case where we use the <code>MeanFieldGaussian</code> variational family. In this case, it suffices to override its <code>rand</code> specialization as follows:</p><pre><code class="language-julia hljs">using QuasiMonteCarlo
using StatsFuns

qmcrng = SobolSample(; R=OwenScramble(; base=2, pad=32))

function Distributions.rand(
    rng::AbstractRNG, q::MvLocationScale{&lt;:Diagonal,D,L}, num_samples::Int
) where {L,D}
    (; location, scale, dist) = q
    n_dims = length(location)
    scale_diag = diag(scale)
    unif_samples = QuasiMonteCarlo.sample(num_samples, length(q), qmcrng)
    std_samples = norminvcdf.(unif_samples)
    return scale_diag .* std_samples .+ location
end
nothing</code></pre><p>(Note that this is a quick-and-dirty example, and there are more sophisticated ways to implement this.)</p><p>By plotting the ELBO, we can see the effect of quasi-Monte Carlo. <img src="../advi_qmc_elbo.svg" alt/> We can see that quasi-Monte Carlo results in much lower variance than naive Monte Carlo. However, similarly to the STL example, just looking at the ELBO is often insufficient to really judge performance. Instead, let&#39;s look at the distance to the global optimum:</p><p><img src="../advi_qmc_dist.svg" alt/></p><p>QMC yields an additional order of magnitude in accuracy. Also, unlike STL, it ever-so slightly accelerates convergence. This is because quasi-Monte Carlo uniformly reduces variance, unlike STL, which reduces variance only near the optimum.</p><section class="footnotes is-size-7"><ul><li class="footnote" id="footnote-TL2014"><a class="tag is-link" href="#citeref-TL2014">TL2014</a>Titsias, M., &amp; Lázaro-Gredilla, M. (2014). Doubly stochastic variational Bayes for non-conjugate inference. In <em>International Conference on Machine Learning</em>.</li><li class="footnote" id="footnote-RMW2014"><a class="tag is-link" href="#citeref-RMW2014">RMW2014</a>Rezende, D. J., Mohamed, S., &amp; Wierstra, D. (2014). Stochastic backpropagation and approximate inference in deep generative models. In <em>International Conference on Machine Learning</em>.</li><li class="footnote" id="footnote-KW2014"><a class="tag is-link" href="#citeref-KW2014">KW2014</a>Kingma, D. P., &amp; Welling, M. (2014). Auto-encoding variational bayes. In <em>International Conference on Learning Representations</em>.</li><li class="footnote" id="footnote-KTRGB2017"><a class="tag is-link" href="#citeref-KTRGB2017">KTRGB2017</a>Kucukelbir, A., Tran, D., Ranganath, R., Gelman, A., &amp; Blei, D. M. (2017). Automatic differentiation variational inference. <em>Journal of Machine Learning Research</em>.</li><li class="footnote" id="footnote-DLTBV2017"><a class="tag is-link" href="#citeref-DLTBV2017">DLTBV2017</a>Dillon, J. V., Langmore, I., Tran, D., Brevdo, E., Vasudevan, S., Moore, D., ... &amp; Saurous, R. A. (2017). Tensorflow distributions. arXiv.</li><li class="footnote" id="footnote-FXTYG2020"><a class="tag is-link" href="#citeref-FXTYG2020">FXTYG2020</a>Fjelde, T. E., Xu, K., Tarek, M., Yalburgi, S., &amp; Ge, H. (2020,. Bijectors. jl: Flexible transformations for probability distributions. In <em>Symposium on Advances in Approximate Bayesian Inference</em>.</li><li class="footnote" id="footnote-RWD2017"><a class="tag is-link" href="#citeref-RWD2017">RWD2017</a>Roeder, G., Wu, Y., &amp; Duvenaud, D. K. (2017). Sticking the landing: Simple, lower-variance gradient estimators for variational inference. Advances in Neural Information Processing Systems, 30.</li><li class="footnote" id="footnote-KMG2024"><a class="tag is-link" href="#citeref-KMG2024">KMG2024</a>Kim, K., Ma, Y., &amp; Gardner, J. (2024). Linear Convergence of Black-Box Variational Inference: Should We Stick the Landing?. In International Conference on Artificial Intelligence and Statistics (pp. 235-243). PMLR.</li><li class="footnote" id="footnote-BWM2018"><a class="tag is-link" href="#citeref-BWM2018">BWM2018</a>Buchholz, A., Wenzel, F., &amp; Mandt, S. (2018). Quasi-monte carlo variational inference. In <em>International Conference on Machine Learning</em>.</li></ul></section></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../overview/">« Overview</a><a class="docs-footer-nextpage" href="../../families/">Variational Families »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.7.0 on <span class="colophon-date" title="Monday 21 October 2024 22:37">Monday 21 October 2024</span>. Using Julia version 1.10.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
