var documenterSearchIndex = {"docs":
[{"location":"elbo/overview/#elbomax","page":"Overview","title":"Evidence Lower Bound Maximization","text":"","category":"section"},{"location":"elbo/overview/#Introduction","page":"Overview","title":"Introduction","text":"","category":"section"},{"location":"elbo/overview/","page":"Overview","title":"Overview","text":"Evidence lower bound (ELBO) maximization[JGJS1999] is a general family of algorithms that minimize the exclusive (or reverse) Kullback-Leibler (KL) divergence between the target distribution pi and a variational approximation q_lambda. More generally, they aim to solve the following problem:","category":"page"},{"location":"elbo/overview/","page":"Overview","title":"Overview","text":"  mathrmminimize_q in mathcalQquad mathrmKLleft(q piright)","category":"page"},{"location":"elbo/overview/","page":"Overview","title":"Overview","text":"where mathcalQ is some family of distributions, often called the variational family. Since the target distribution pi is intractable in general, the KL divergence is also intractable. Instead, the ELBO maximization strategy maximizes a surrogate objective, the ELBO:","category":"page"},{"location":"elbo/overview/","page":"Overview","title":"Overview","text":"  mathrmELBOleft(qright) triangleq mathbbE_theta sim q log pileft(thetaright) + mathbbHleft(qright)","category":"page"},{"location":"elbo/overview/","page":"Overview","title":"Overview","text":"which serves as a lower bound to the KL. The ELBO and its gradient can be readily estimated through various strategies. Overall, ELBO maximization algorithms aim to solve the problem:","category":"page"},{"location":"elbo/overview/","page":"Overview","title":"Overview","text":"  mathrmmaximize_q in mathcalQquad mathrmELBOleft(qright)","category":"page"},{"location":"elbo/overview/","page":"Overview","title":"Overview","text":"Multiple ways to solve this problem exist, each leading to a different variational inference algorithm.","category":"page"},{"location":"elbo/overview/#Algorithms","page":"Overview","title":"Algorithms","text":"","category":"section"},{"location":"elbo/overview/","page":"Overview","title":"Overview","text":"Currently, AdvancedVI only provides the approach known as black-box variational inference (also known as Monte Carlo VI, Stochastic Gradient VI). (Introduced independently by two groups [RGB2014][TL2014] in 2014.) In particular, AdvancedVI focuses on the reparameterization gradient estimator[TL2014][RMW2014][KW2014], which is generally superior compared to alternative strategies[XQKS2019], discussed in the following section:","category":"page"},{"location":"elbo/overview/","page":"Overview","title":"Overview","text":"RepGradELBO","category":"page"},{"location":"elbo/overview/","page":"Overview","title":"Overview","text":"[JGJS1999]: Jordan, M. I., Ghahramani, Z., Jaakkola, T. S., & Saul, L. K. (1999). An introduction to variational methods for graphical models. Machine learning, 37, 183-233.","category":"page"},{"location":"elbo/overview/","page":"Overview","title":"Overview","text":"[TL2014]: Titsias, M., & Lázaro-Gredilla, M. (2014). Doubly stochastic variational Bayes for non-conjugate inference. In International Conference on Machine Learning. ","category":"page"},{"location":"elbo/overview/","page":"Overview","title":"Overview","text":"[RMW2014]: Rezende, D. J., Mohamed, S., & Wierstra, D. (2014). Stochastic backpropagation and approximate inference in deep generative models. In International Conference on Machine Learning.","category":"page"},{"location":"elbo/overview/","page":"Overview","title":"Overview","text":"[KW2014]: Kingma, D. P., & Welling, M. (2014). Auto-encoding variational bayes. In International Conference on Learning Representations.","category":"page"},{"location":"elbo/overview/","page":"Overview","title":"Overview","text":"[XQKS2019]: Xu, M., Quiroz, M., Kohn, R., & Sisson, S. A. (2019). Variance reduction properties of the reparameterization trick. In *The International Conference on Artificial Intelligence and Statistics.","category":"page"},{"location":"elbo/overview/","page":"Overview","title":"Overview","text":"[RGB2014]: Ranganath, R., Gerrish, S., & Blei, D. (2014). Black box variational inference. In Artificial Intelligence and Statistics.","category":"page"},{"location":"optimization/#optim","page":"Optimization","title":"Optimization","text":"","category":"section"},{"location":"optimization/#Parameter-Free-Optimization-Rules","page":"Optimization","title":"Parameter-Free Optimization Rules","text":"","category":"section"},{"location":"optimization/","page":"Optimization","title":"Optimization","text":"We provide custom optimization rules that are not provided out-of-the-box by Optimisers.jl. The main theme of the provided optimizers is that they are parameter-free. This means that these optimization rules shouldn't require (or barely) any tuning to obtain performance competitive with well-tuned alternatives.","category":"page"},{"location":"optimization/","page":"Optimization","title":"Optimization","text":"DoG\nDoWG\nCOCOB","category":"page"},{"location":"optimization/#AdvancedVI.DoG","page":"Optimization","title":"AdvancedVI.DoG","text":"DoG(repsilon)\n\nDistance over gradient (DoG[IHC2023]) optimizer. It's only parameter is the initial guess of the Euclidean distance to the optimum repsilon. The original paper recommends $ 10^{-4} ( 1 + \\lVert \\lambda_0 \\rVert ) $, but the default value is $ 10^{-6} $.\n\nParameters\n\nrepsilon: Initial guess of the Euclidean distance between the initial point and the optimum. (default value: 1e-6)\n\n[IHC2023]: Ivgi, M., Hinder, O., & Carmon, Y. (2023). Dog is sgd's best friend: A parameter-free dynamic step size schedule. In International Conference on Machine Learning (pp. 14465-14499). PMLR.\n\n\n\n\n\n","category":"type"},{"location":"optimization/#AdvancedVI.DoWG","page":"Optimization","title":"AdvancedVI.DoWG","text":"DoWG(repsilon)\n\nDistance over weighted gradient (DoWG[KMJ2024]) optimizer. It's only parameter is the initial guess of the Euclidean distance to the optimum repsilon.\n\nParameters\n\nrepsilon: Initial guess of the Euclidean distance between the initial point and           the optimum. (default value: 1e-6)\n\n[KMJ2024]: Khaled, A., Mishchenko, K., & Jin, C. (2023). Dowg unleashed: An efficient universal parameter-free gradient descent method. Advances in Neural Information Processing Systems, 36, 6748-6769.\n\n\n\n\n\n","category":"type"},{"location":"optimization/#AdvancedVI.COCOB","page":"Optimization","title":"AdvancedVI.COCOB","text":"COCOB(alpha)\n\nContinuous Coin Betting (COCOB[OT2017]) optimizer. We use the \"COCOB-Backprop\" variant, which is closer to the Adam optimizer. It's only parameter is the maximum change per parameter α, which shouldn't need much tuning.\n\nParameters\n\nalpha: Scaling parameter. (default value: 100)\n\n[OT2017]: Orabona, F., & Tommasi, T. (2017). Training deep networks without learning rates through coin betting. Advances in Neural Information Processing Systems, 30.\n\n\n\n\n\n","category":"type"},{"location":"optimization/#Parameter-Averaging-Strategies","page":"Optimization","title":"Parameter Averaging Strategies","text":"","category":"section"},{"location":"optimization/","page":"Optimization","title":"Optimization","text":"In some cases, the best optimization performance is obtained by averaging the sequence of parameters generated by the optimization algorithm. For instance, the DoG[IHC2023] and DoWG[KMJ2024] papers report their best performance through averaging. The benefits of parameter averaging have been specifically confirmed for ELBO maximization[DCAMHV2020].","category":"page"},{"location":"optimization/","page":"Optimization","title":"Optimization","text":"NoAveraging\nPolynomialAveraging","category":"page"},{"location":"optimization/#AdvancedVI.NoAveraging","page":"Optimization","title":"AdvancedVI.NoAveraging","text":"NoAveraging()\n\nNo averaging. This returns the last-iterate of the optimization rule.\n\n\n\n\n\n","category":"type"},{"location":"optimization/#AdvancedVI.PolynomialAveraging","page":"Optimization","title":"AdvancedVI.PolynomialAveraging","text":"PolynomialAveraging(eta)\n\nPolynomial averaging rule proposed Shamir and Zhang[SZ2013]. At iteration t, the parameter average $ \\bar{\\lambda}_t $ according to the polynomial averaging rule is given as\n\n    barlambda_t = (1 - w_t) barlambda_t-1 + w_t lambda_t  \n\nwhere the averaging weight is \n\n    w_t = fraceta + 1t + eta  \n\nHigher eta (eta) down-weights earlier iterations. When eta=0, this is equivalent to uniformly averaging the iterates in an online fashion. The DoG paper[IHC2023] suggests eta=8.\n\nParameters\n\neta: Regularization term. (default: 8)\n\n[SZ2013]: Shamir, O., & Zhang, T. (2013). Stochastic gradient descent for non-smooth optimization: Convergence results and optimal averaging schemes. In International conference on machine learning (pp. 71-79). PMLR.\n\n\n\n\n\n","category":"type"},{"location":"optimization/","page":"Optimization","title":"Optimization","text":"[DCAMHV2020]: Dhaka, A. K., Catalina, A., Andersen, M. R., Magnusson, M., Huggins, J., & Vehtari, A. (2020). Robust, accurate stochastic optimization for variational inference. Advances in Neural Information Processing Systems, 33, 10961-10973.","category":"page"},{"location":"examples/#examples","page":"Examples","title":"Evidence Lower Bound Maximization","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"In this tutorial, we will work with a normal-log-normal model.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"beginaligned\nx sim mathrmLogNormalleft(mu_x sigma_x^2right) \ny sim mathcalNleft(mu_y sigma_y^2right)\nendaligned","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"BBVI with Bijectors.Exp bijectors is able to infer this model exactly.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"Using the LogDensityProblems interface, we the model can be defined as follows:","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"using LogDensityProblems\nusing SimpleUnPack\n\nstruct NormalLogNormal{MX,SX,MY,SY}\n    μ_x::MX\n    σ_x::SX\n    μ_y::MY\n    Σ_y::SY\nend\n\nfunction LogDensityProblems.logdensity(model::NormalLogNormal, θ)\n    @unpack μ_x, σ_x, μ_y, Σ_y = model\n    return logpdf(LogNormal(μ_x, σ_x), θ[1]) + logpdf(MvNormal(μ_y, Σ_y), θ[2:end])\nend\n\nfunction LogDensityProblems.dimension(model::NormalLogNormal)\n    return length(model.μ_y) + 1\nend\n\nfunction LogDensityProblems.capabilities(::Type{<:NormalLogNormal})\n    return LogDensityProblems.LogDensityOrder{0}()\nend","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"Let's now instantiate the model","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"using LinearAlgebra\n\nn_dims = 10\nμ_x = randn()\nσ_x = exp.(randn())\nμ_y = randn(n_dims)\nσ_y = exp.(randn(n_dims))\nmodel = NormalLogNormal(μ_x, σ_x, μ_y, Diagonal(σ_y .^ 2));\nnothing","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"Since the y follows a log-normal prior, its support is bounded to be the positive half-space mathbbR_+. Thus, we will use Bijectors to match the support of our target posterior and the variational approximation.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"using Bijectors\n\nfunction Bijectors.bijector(model::NormalLogNormal)\n    @unpack μ_x, σ_x, μ_y, Σ_y = model\n    return Bijectors.Stacked(\n        Bijectors.bijector.([LogNormal(μ_x, σ_x), MvNormal(μ_y, Σ_y)]),\n        [1:1, 2:(1 + length(μ_y))],\n    )\nend\n\nb = Bijectors.bijector(model);\nbinv = inverse(b)\nnothing","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"Let's now load AdvancedVI. Since BBVI relies on automatic differentiation (AD), we need to load an AD library, before loading AdvancedVI. Also, the selected AD framework needs to be communicated to AdvancedVI using the ADTypes interface. Here, we will use ForwardDiff, which can be selected by later passing ADTypes.AutoForwardDiff().","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"using Optimisers\nusing ADTypes, ForwardDiff\nusing AdvancedVI","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"We now need to select 1. a variational objective, and 2. a variational family. Here, we will use the RepGradELBO objective, which expects an object implementing the LogDensityProblems interface, and the inverse bijector.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"n_montecaro = 10;\nobjective = RepGradELBO(n_montecaro)","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"For the variational family, we will use the classic mean-field Gaussian family.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"d = LogDensityProblems.dimension(model);\nμ = randn(d);\nL = Diagonal(ones(d));\nq0 = AdvancedVI.MeanFieldGaussian(μ, L)\nnothing","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"And then, we now apply the bijector to the variational family.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"q0_trans = Bijectors.TransformedDistribution(q0, binv)\nnothing","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"Passing objective and the initial variational approximation q to optimize performs inference.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"n_max_iter = 10^4\nq_avg_trans, q_trans, stats, _ = AdvancedVI.optimize(\n    model,\n    objective,\n    q0_trans,\n    n_max_iter;\n    show_progress=false,\n    adtype=AutoForwardDiff(),\n    optimizer=Optimisers.Adam(1e-3),\n);\nnothing","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"q_avg_trans is the final output of the optimization procedure. If a parameter averaging strategy is used through the keyword argument averager, q_avg_trans is be the output of the averaging strategy, while q_trans is the last iterate.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"The selected inference procedure stores per-iteration statistics into stats. For instance, the ELBO can be ploted as follows:","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"using Plots\n\nt = [stat.iteration for stat in stats]\ny = [stat.elbo for stat in stats]\nplot(t, y; label=\"BBVI\", xlabel=\"Iteration\", ylabel=\"ELBO\")\nsavefig(\"bbvi_example_elbo.svg\")\nnothing","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"(Image: )","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"Further information can be gathered by defining your own callback!.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"The final ELBO can be estimated by calling the objective directly with a different number of Monte Carlo samples as follows:","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"estimate_objective(objective, q_avg_trans, model; n_samples=10^4)","category":"page"},{"location":"locscale/#locscale","page":"Location-Scale Variational Family","title":"Location-Scale Variational Family","text":"","category":"section"},{"location":"locscale/#Introduction","page":"Location-Scale Variational Family","title":"Introduction","text":"","category":"section"},{"location":"locscale/","page":"Location-Scale Variational Family","title":"Location-Scale Variational Family","text":"The location-scale variational family is a family of probability distributions, where their sampling process can be represented as","category":"page"},{"location":"locscale/","page":"Location-Scale Variational Family","title":"Location-Scale Variational Family","text":"z sim  q_lambda qquadLeftrightarrowqquad\nz stackreld= C u + mquad u sim varphi","category":"page"},{"location":"locscale/","page":"Location-Scale Variational Family","title":"Location-Scale Variational Family","text":"where C is the scale, m is the location, and varphi is the base distribution. m and C form the variational parameters lambda = (m C) of q_lambda.  The location-scale family encompases many practical variational families, which can be instantiated by setting the base distribution of u and the structure of C.","category":"page"},{"location":"locscale/","page":"Location-Scale Variational Family","title":"Location-Scale Variational Family","text":"The probability density is given by","category":"page"},{"location":"locscale/","page":"Location-Scale Variational Family","title":"Location-Scale Variational Family","text":"  q_lambda(z) = C^-1 varphi(C^-1(z - m))","category":"page"},{"location":"locscale/","page":"Location-Scale Variational Family","title":"Location-Scale Variational Family","text":"and the entropy is given as","category":"page"},{"location":"locscale/","page":"Location-Scale Variational Family","title":"Location-Scale Variational Family","text":"  mathbbH(q_lambda) = mathbbH(varphi) + log C","category":"page"},{"location":"locscale/","page":"Location-Scale Variational Family","title":"Location-Scale Variational Family","text":"where mathbbH(varphi) is the entropy of the base distribution. Notice the mathbbH(varphi) does not depend on log C. The derivative of the entropy with respect to lambda is thus independent of the base distribution.","category":"page"},{"location":"locscale/#Constructors","page":"Location-Scale Variational Family","title":"Constructors","text":"","category":"section"},{"location":"locscale/","page":"Location-Scale Variational Family","title":"Location-Scale Variational Family","text":"note: Note\nFor stable convergence, the initial scale needs to be sufficiently large and well-conditioned.  Initializing scale to have small eigenvalues will often result in initial divergences and numerical instabilities.","category":"page"},{"location":"locscale/","page":"Location-Scale Variational Family","title":"Location-Scale Variational Family","text":"MvLocationScale","category":"page"},{"location":"locscale/#AdvancedVI.MvLocationScale","page":"Location-Scale Variational Family","title":"AdvancedVI.MvLocationScale","text":"MvLocationScale(location, scale, dist) <: ContinuousMultivariateDistribution\n\nThe location scale variational family broadly represents various variational families using location and scale variational parameters.\n\nIt generally represents any distribution for which the sampling path can be represented as follows:\n\n  d = length(location)\n  u = rand(dist, d)\n  z = scale*u + location\n\n\n\n\n\n","category":"type"},{"location":"locscale/","page":"Location-Scale Variational Family","title":"Location-Scale Variational Family","text":"FullRankGaussian\nMeanFieldGaussian","category":"page"},{"location":"locscale/#AdvancedVI.FullRankGaussian","page":"Location-Scale Variational Family","title":"AdvancedVI.FullRankGaussian","text":"FullRankGaussian(location, scale; check_args = true)\n\nConstruct a Gaussian variational approximation with a dense covariance matrix.\n\nArguments\n\nlocation::AbstractVector{T}: Mean of the Gaussian.\nscale::LinearAlgebra.AbstractTriangular{T}: Cholesky factor of the covariance of the Gaussian.\n\nKeyword Arguments\n\ncheck_args: Check the conditioning of the initial scale (default: true).\n\n\n\n\n\n","category":"function"},{"location":"locscale/#AdvancedVI.MeanFieldGaussian","page":"Location-Scale Variational Family","title":"AdvancedVI.MeanFieldGaussian","text":"MeanFieldGaussian(location, scale; check_args = true)\n\nConstruct a Gaussian variational approximation with a diagonal covariance matrix.\n\nArguments\n\nlocation::AbstractVector{T}: Mean of the Gaussian.\nscale::Diagonal{T}: Diagonal Cholesky factor of the covariance of the Gaussian.\n\nKeyword Arguments\n\ncheck_args: Check the conditioning of the initial scale (default: true).\n\n\n\n\n\n","category":"function"},{"location":"locscale/#Gaussian-Variational-Families","page":"Location-Scale Variational Family","title":"Gaussian Variational Families","text":"","category":"section"},{"location":"locscale/","page":"Location-Scale Variational Family","title":"Location-Scale Variational Family","text":"using AdvancedVI, LinearAlgebra, Distributions;\nμ = zeros(2);\n\nL = diagm(ones(2)) |> LowerTriangular;\nq = FullRankGaussian(μ, L)\n\nL = ones(2) |> Diagonal;\nq = MeanFieldGaussian(μ, L)","category":"page"},{"location":"locscale/#Sudent-t-Variational-Families","page":"Location-Scale Variational Family","title":"Sudent-t Variational Families","text":"","category":"section"},{"location":"locscale/","page":"Location-Scale Variational Family","title":"Location-Scale Variational Family","text":"using AdvancedVI, LinearAlgebra, Distributions;\nμ = zeros(2);\nν = 3;\n\n# Full-Rank \nL = diagm(ones(2)) |> LowerTriangular;\nq = MvLocationScale(μ, L, TDist(ν))\n\n# Mean-Field\nL = ones(2) |> Diagonal;\nq = MvLocationScale(μ, L, TDist(ν))","category":"page"},{"location":"locscale/#Laplace-Variational-families","page":"Location-Scale Variational Family","title":"Laplace Variational families","text":"","category":"section"},{"location":"locscale/","page":"Location-Scale Variational Family","title":"Location-Scale Variational Family","text":"using AdvancedVI, LinearAlgebra, Distributions;\nμ = zeros(2);\n\n# Full-Rank \nL = diagm(ones(2)) |> LowerTriangular;\nq = MvLocationScale(μ, L, Laplace())\n\n# Mean-Field\nL = ones(2) |> Diagonal;\nq = MvLocationScale(μ, L, Laplace())","category":"page"},{"location":"general/#general","page":"General Usage","title":"General Usage","text":"","category":"section"},{"location":"general/","page":"General Usage","title":"General Usage","text":"Each VI algorithm provides the followings:","category":"page"},{"location":"general/","page":"General Usage","title":"General Usage","text":"Variational families supported by each VI algorithm.\nA variational objective corresponding to the VI algorithm.","category":"page"},{"location":"general/","page":"General Usage","title":"General Usage","text":"Note that each variational family is subject to its own constraints. Thus, please refer to the documentation of the variational inference algorithm of interest. ","category":"page"},{"location":"general/#Optimizing-a-Variational-Objective","page":"General Usage","title":"Optimizing a Variational Objective","text":"","category":"section"},{"location":"general/","page":"General Usage","title":"General Usage","text":"After constructing a variational objective objective and initializing a variational approximation, one can optimize objective by calling optimize:","category":"page"},{"location":"general/","page":"General Usage","title":"General Usage","text":"optimize","category":"page"},{"location":"general/#AdvancedVI.optimize","page":"General Usage","title":"AdvancedVI.optimize","text":"optimize(problem, objective, q_init, max_iter, objargs...; kwargs...)\n\nOptimize the variational objective objective targeting the problem problem by estimating (stochastic) gradients.\n\nThe trainable parameters in the variational approximation are expected to be extractable through Optimisers.destructure. This requires the variational approximation to be marked as a functor through Functors.@functor.\n\nArguments\n\nobjective::AbstractVariationalObjective: Variational Objective.\nq_init: Initial variational distribution. The variational parameters must be extractable through Optimisers.destructure.\nmax_iter::Int: Maximum number of iterations.\nobjargs...: Arguments to be passed to objective.\n\nKeyword Arguments\n\nadtype::ADtypes.AbstractADType: Automatic differentiation backend. \noptimizer::Optimisers.AbstractRule: Optimizer used for inference. (Default: Adam.)\naverager::AbstractAverager : Parameter averaging strategy. (Default: NoAveraging())\nrng::AbstractRNG: Random number generator. (Default: Random.default_rng().)\nshow_progress::Bool: Whether to show the progress bar. (Default: true.)\ncallback: Callback function called after every iteration. See further information below. (Default: nothing.)\nprog: Progress bar configuration. (Default: ProgressMeter.Progress(n_max_iter; desc=\"Optimizing\", barlen=31, showspeed=true, enabled=prog).)\nstate::NamedTuple: Initial value for the internal state of optimization. Used to warm-start from the state of a previous run. (See the returned values below.)\n\nReturns\n\naveraged_params: Variational parameters generated by the algorithm averaged according to averager.\nparams: Last variational parameters generated by the algorithm.\nstats: Statistics gathered during optimization.\nstate: Collection of the final internal states of optimization. This can used later to warm-start from the last iteration of the corresponding run.\n\nCallback\n\nThe callback function callback has a signature of\n\ncallback(; stat, state, params, averaged_params, restructure, gradient)\n\nThe arguments are as follows:\n\nstat: Statistics gathered during the current iteration. The content will vary depending on objective.\nstate: Collection of the internal states used for optimization.\nparams: Variational parameters.\naveraged_params: Variational parameters averaged according to the averaging strategy.\nrestructure: Function that restructures the variational approximation from the variational parameters. Calling restructure(param) reconstructs the variational approximation. \ngradient: The estimated (possibly stochastic) gradient.\n\ncb can return a NamedTuple containing some additional information computed within cb. This will be appended to the statistic of the current corresponding iteration. Otherwise, just return nothing.\n\n\n\n\n\n","category":"function"},{"location":"general/#Estimating-the-Objective","page":"General Usage","title":"Estimating the Objective","text":"","category":"section"},{"location":"general/","page":"General Usage","title":"General Usage","text":"In some cases, it is useful to directly estimate the objective value. This can be done by the following funciton:","category":"page"},{"location":"general/","page":"General Usage","title":"General Usage","text":"estimate_objective","category":"page"},{"location":"general/#AdvancedVI.estimate_objective","page":"General Usage","title":"AdvancedVI.estimate_objective","text":"estimate_objective([rng,] obj, q, prob; kwargs...)\n\nEstimate the variational objective obj targeting prob with respect to the variational approximation q.\n\nArguments\n\nrng::Random.AbstractRNG: Random number generator.\nobj::AbstractVariationalObjective: Variational objective.\nprob: The target log-joint likelihood implementing the LogDensityProblem interface.\nq: Variational approximation.\n\nKeyword Arguments\n\nDepending on the objective, additional keyword arguments may apply. Please refer to the respective documentation of each variational objective for more info.\n\nReturns\n\nobj_est: Estimate of the objective value.\n\n\n\n\n\n","category":"function"},{"location":"general/","page":"General Usage","title":"General Usage","text":"info: Info\nNote that estimate_objective is not expected to be differentiated through, and may not result in optimal statistical performance.","category":"page"},{"location":"general/#Advanced-Usage","page":"General Usage","title":"Advanced Usage","text":"","category":"section"},{"location":"general/","page":"General Usage","title":"General Usage","text":"Each variational objective is a subtype of the following abstract type:","category":"page"},{"location":"general/","page":"General Usage","title":"General Usage","text":"AdvancedVI.AbstractVariationalObjective","category":"page"},{"location":"general/#AdvancedVI.AbstractVariationalObjective","page":"General Usage","title":"AdvancedVI.AbstractVariationalObjective","text":"AbstractVariationalObjective\n\nAbstract type for the VI algorithms supported by AdvancedVI.\n\nImplementations\n\nTo be supported by AdvancedVI, a VI algorithm must implement AbstractVariationalObjective and estimate_objective. Also, it should provide gradients by implementing the function estimate_gradient!. If the estimator is stateful, it can implement init to initialize the state.\n\n\n\n\n\n","category":"type"},{"location":"general/","page":"General Usage","title":"General Usage","text":"Furthermore, AdvancedVI only interacts with each variational objective by querying gradient estimates. Therefore, to create a new custom objective to be optimized through AdvancedVI, it suffices to implement the following function:","category":"page"},{"location":"general/","page":"General Usage","title":"General Usage","text":"AdvancedVI.estimate_gradient!","category":"page"},{"location":"general/#AdvancedVI.estimate_gradient!","page":"General Usage","title":"AdvancedVI.estimate_gradient!","text":"estimate_gradient!(rng, obj, adtype, out, prob, λ, restructure, obj_state)\n\nEstimate (possibly stochastic) gradients of the variational objective obj targeting prob with respect to the variational parameters λ\n\nArguments\n\nrng::Random.AbstractRNG: Random number generator.\nobj::AbstractVariationalObjective: Variational objective.\nadtype::ADTypes.AbstractADType: Automatic differentiation backend. \nout::DiffResults.MutableDiffResult: Buffer containing the objective value and gradient estimates. \nprob: The target log-joint likelihood implementing the LogDensityProblem interface.\nλ: Variational parameters to evaluate the gradient on.\nrestructure: Function that reconstructs the variational approximation from λ.\nobj_state: Previous state of the objective.\n\nReturns\n\nout::MutableDiffResult: Buffer containing the objective value and gradient estimates.\nobj_state: The updated state of the objective.\nstat::NamedTuple: Statistics and logs generated during estimation.\n\n\n\n\n\n","category":"function"},{"location":"general/","page":"General Usage","title":"General Usage","text":"If an objective needs to be stateful, one can implement the following function to inialize the state.","category":"page"},{"location":"general/","page":"General Usage","title":"General Usage","text":"AdvancedVI.init","category":"page"},{"location":"general/#AdvancedVI.init","page":"General Usage","title":"AdvancedVI.init","text":"init(rng, obj, prob, params, restructure)\n\nInitialize a state of the variational objective obj given the initial variational parameters λ. This function needs to be implemented only if obj is stateful.\n\nArguments\n\nrng::Random.AbstractRNG: Random number generator.\nobj::AbstractVariationalObjective: Variational objective.\nparams: Initial variational parameters.\nrestructure: Function that reconstructs the variational approximation from λ.\n\n\n\n\n\ninit(avg, params)\n\nInitialize the state of the averaging strategy avg with the initial parameters params.\n\nArguments\n\navg::AbstractAverager: Averaging strategy.\nparams: Initial variational parameters.\n\n\n\n\n\n","category":"function"},{"location":"elbo/repgradelbo/#repgradelbo","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"","category":"section"},{"location":"elbo/repgradelbo/#Overview","page":"Reparameterization Gradient Estimator","title":"Overview","text":"","category":"section"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"The reparameterization gradient[TL2014][RMW2014][KW2014] is an unbiased gradient estimator of the ELBO. Consider some variational family","category":"page"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"mathcalQ = q_lambda mid lambda in Lambda ","category":"page"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"where lambda is the variational parameters of q_lambda. If its sampling process can be described by some differentiable reparameterization function mathcalT_lambda and a base distribution varphi independent of lambda such that","category":"page"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"z sim  q_lambda qquadLeftrightarrowqquad\nz stackreld= mathcalT_lambdaleft(epsilonright)quad epsilon sim varphi","category":"page"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"we can effectively estimate the gradient of the ELBO by directly differentiating","category":"page"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"  widehatmathrmELBOleft(lambdaright) = frac1Msum^M_m=1 log pileft(mathcalT_lambdaleft(epsilon_mright)right) + mathbbHleft(q_lambdaright)","category":"page"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"where epsilon_m sim varphi are Monte Carlo samples, with respect to lambda. This estimator is called the reparameterization gradient estimator.","category":"page"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"In addition to the reparameterization gradient, AdvancedVI provides the following features:","category":"page"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"Posteriors with constrained supports are handled through Bijectors, which is known as the automatic differentiation VI (ADVI; [KTRGB2017]) formulation. (See this section.)\nThe gradient of the entropy can be estimated through various strategies depending on the capabilities of the variational family. (See this section.)","category":"page"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"[TL2014]: Titsias, M., & Lázaro-Gredilla, M. (2014). Doubly stochastic variational Bayes for non-conjugate inference. In International Conference on Machine Learning.","category":"page"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"[RMW2014]: Rezende, D. J., Mohamed, S., & Wierstra, D. (2014). Stochastic backpropagation and approximate inference in deep generative models. In International Conference on Machine Learning.","category":"page"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"[KW2014]: Kingma, D. P., & Welling, M. (2014). Auto-encoding variational bayes. In International Conference on Learning Representations.","category":"page"},{"location":"elbo/repgradelbo/#The-RepGradELBO-Objective","page":"Reparameterization Gradient Estimator","title":"The RepGradELBO Objective","text":"","category":"section"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"To use the reparameterization gradient, AdvancedVI provides the following variational objective:","category":"page"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"RepGradELBO","category":"page"},{"location":"elbo/repgradelbo/#AdvancedVI.RepGradELBO","page":"Reparameterization Gradient Estimator","title":"AdvancedVI.RepGradELBO","text":"RepGradELBO(n_samples; kwargs...)\n\nEvidence lower-bound objective with the reparameterization gradient formulation[TL2014][RMW2014][KW2014]. This computes the evidence lower-bound (ELBO) through the formulation:\n\nbeginaligned\nmathrmELBOleft(lambdaright)\ntriangleq\nmathbbE_z sim q_lambdaleft\n  log pileft(zright)\nright\n+ mathbbHleft(q_lambdaright)\nendaligned\n\nArguments\n\nn_samples::Int: Number of Monte Carlo samples used to estimate the ELBO.\n\nKeyword Arguments\n\nentropy: The estimator for the entropy term. (Type <: AbstractEntropyEstimator; Default: ClosedFormEntropy())\n\nRequirements\n\nThe variational approximation q_lambda implements rand.\nThe target distribution and the variational approximation have the same support.\nThe target logdensity(prob, x) must be differentiable with respect to x by the selected AD backend.\n\nDepending on the options, additional requirements on q_lambda may apply.\n\n\n\n\n\n","category":"type"},{"location":"elbo/repgradelbo/#bijectors","page":"Reparameterization Gradient Estimator","title":"Handling Constraints with Bijectors","text":"","category":"section"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"As mentioned in the docstring, the RepGradELBO objective assumes that the variational approximation q_lambda and the target distribution pi have the same support for all lambda in Lambda.","category":"page"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"However, in general, it is most convenient to use variational families that have the whole Euclidean space mathbbR^d as their support. This is the case for the location-scale distributions provided by AdvancedVI. For target distributions which the support is not the full mathbbR^d, we can apply some transformation b to q_lambda to match its support such that","category":"page"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"z sim  q_blambda qquadLeftrightarrowqquad\nz stackreld= b^-1left(etaright)quad eta sim q_lambda","category":"page"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"where b is often called a bijector, since it is often chosen among bijective transformations. This idea is known as automatic differentiation VI[KTRGB2017] and has subsequently been improved by Tensorflow Probability[DLTBV2017]. In Julia, Bijectors.jl[FXTYG2020] provides a comprehensive collection of bijections.","category":"page"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"One caveat of ADVI is that, after applying the bijection, a Jacobian adjustment needs to be applied. That is, the objective is now","category":"page"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"mathrmADVIleft(lambdaright)\ntriangleq\nmathbbE_eta sim q_lambdaleft\n  log pileft( b^-1left( eta right) right)\n  + log lvert J_b^-1left(etaright) rvert\nright\n+ mathbbHleft(q_lambdaright)","category":"page"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"This is automatically handled by AdvancedVI through TransformedDistribution provided by Bijectors.jl. See the following example:","category":"page"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"using Bijectors\nq = MeanFieldGaussian(μ, L)\nb = Bijectors.bijector(dist)\nbinv = inverse(b)\nq_transformed = Bijectors.TransformedDistribution(q, binv)","category":"page"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"By passing q_transformed to optimize, the Jacobian adjustment for the bijector b is automatically applied. (See Examples for a fully working example.)","category":"page"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"[KTRGB2017]: Kucukelbir, A., Tran, D., Ranganath, R., Gelman, A., & Blei, D. M. (2017). Automatic differentiation variational inference. Journal of Machine Learning Research.","category":"page"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"[DLTBV2017]: Dillon, J. V., Langmore, I., Tran, D., Brevdo, E., Vasudevan, S., Moore, D., ... & Saurous, R. A. (2017). Tensorflow distributions. arXiv.","category":"page"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"[FXTYG2020]: Fjelde, T. E., Xu, K., Tarek, M., Yalburgi, S., & Ge, H. (2020,. Bijectors. jl: Flexible transformations for probability distributions. In Symposium on Advances in Approximate Bayesian Inference.","category":"page"},{"location":"elbo/repgradelbo/#entropygrad","page":"Reparameterization Gradient Estimator","title":"Entropy Estimators","text":"","category":"section"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"For the gradient of the entropy term, we provide three choices with varying requirements. The user can select the entropy estimator by passing it as a keyword argument when constructing the RepGradELBO objective.","category":"page"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"Estimator entropy(q) logpdf(q) Type\nClosedFormEntropy required  Deterministic\nMonteCarloEntropy  required Monte Carlo\nStickingTheLandingEntropy  required Monte Carlo with control variate","category":"page"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"The requirements mean that either Distributions.entropy or Distributions.logpdf need to be implemented for the choice of variational family. In general, the use of ClosedFormEntropy is recommended whenever possible. If entropy is not available, then StickingTheLandingEntropy is recommended. See the following section for more details.","category":"page"},{"location":"elbo/repgradelbo/#The-StickingTheLandingEntropy-Estimator","page":"Reparameterization Gradient Estimator","title":"The StickingTheLandingEntropy Estimator","text":"","category":"section"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"The StickingTheLandingEntropy, or STL estimator, is a control variate approach [RWD2017].","category":"page"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"StickingTheLandingEntropy","category":"page"},{"location":"elbo/repgradelbo/#AdvancedVI.StickingTheLandingEntropy","page":"Reparameterization Gradient Estimator","title":"AdvancedVI.StickingTheLandingEntropy","text":"StickingTheLandingEntropy()\n\nThe \"sticking the landing\" entropy estimator[RWD2017].\n\nRequirements\n\nThe variational approximation q implements logpdf.\nlogpdf(q, η) must be differentiable by the selected AD framework.\n\n\n\n\n\n","category":"type"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"It occasionally results in lower variance when pi approx q_lambda, and higher variance when pi notapprox q_lambda. The conditions for which the STL estimator results in lower variance is still an active subject for research.","category":"page"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"The main downside of the STL estimator is that it needs to evaluate and differentiate the log density of q_lambda, logpdf(q), in every iteration. Depending on the variational family, this might be computationally inefficient or even numerically unstable. For example, if q_lambda is a Gaussian with a full-rank covariance, a back-substitution must be performed at every step, making the per-iteration complexity mathcalO(d^3) and reducing numerical stability.","category":"page"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"The STL control variate can be used by changing the entropy estimator using the following object:","category":"page"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"using Bijectors\nusing FillArrays\nusing LinearAlgebra\nusing LogDensityProblems\nusing Plots\nusing Random\nusing SimpleUnPack\n\nusing Optimisers\nusing ADTypes, ForwardDiff\nusing AdvancedVI\n\nstruct NormalLogNormal{MX,SX,MY,SY}\n    μ_x::MX\n    σ_x::SX\n    μ_y::MY\n    Σ_y::SY\nend\n\nfunction LogDensityProblems.logdensity(model::NormalLogNormal, θ)\n    @unpack μ_x, σ_x, μ_y, Σ_y = model\n    logpdf(LogNormal(μ_x, σ_x), θ[1]) + logpdf(MvNormal(μ_y, Σ_y), θ[2:end])\nend\n\nfunction LogDensityProblems.dimension(model::NormalLogNormal)\n    length(model.μ_y) + 1\nend\n\nfunction LogDensityProblems.capabilities(::Type{<:NormalLogNormal})\n    LogDensityProblems.LogDensityOrder{0}()\nend\n\nn_dims = 10\nμ_x    = 2.0\nσ_x    = 0.3\nμ_y    = Fill(2.0, n_dims)\nσ_y    = Fill(1.0, n_dims)\nmodel  = NormalLogNormal(μ_x, σ_x, μ_y, Diagonal(σ_y.^2));\n\nd  = LogDensityProblems.dimension(model);\nμ  = zeros(d);\nL  = Diagonal(ones(d));\nq0 = AdvancedVI.MeanFieldGaussian(μ, L)\n\nfunction Bijectors.bijector(model::NormalLogNormal)\n    @unpack μ_x, σ_x, μ_y, Σ_y = model\n    Bijectors.Stacked(\n        Bijectors.bijector.([LogNormal(μ_x, σ_x), MvNormal(μ_y, Σ_y)]),\n        [1:1, 2:1+length(μ_y)])\nend","category":"page"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"Let us come back to the example in Examples, where a LogDensityProblem is given as model. In this example, the true posterior is contained within the variational family. This setting is known as \"perfect variational family specification.\" In this case, the RepGradELBO estimator with StickingTheLandingEntropy is the only estimator known to converge exponentially fast (\"linear convergence\") to the true solution.","category":"page"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"Recall that the original ADVI objective with a closed-form entropy (CFE) is given as follows:","category":"page"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"n_montecarlo = 16;\nb = Bijectors.bijector(model);\nbinv = inverse(b)\n\nq0_trans = Bijectors.TransformedDistribution(q0, binv)\n\ncfe = AdvancedVI.RepGradELBO(n_montecarlo)\nnothing","category":"page"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"The repgradelbo estimator can instead be created as follows:","category":"page"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"repgradelbo = AdvancedVI.RepGradELBO(\n    n_montecarlo; entropy=AdvancedVI.StickingTheLandingEntropy()\n);\nnothing","category":"page"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"max_iter = 3*10^3\n\nfunction callback(; params, restructure, kwargs...)\n    q = restructure(params).dist\n    dist2 = sum(abs2, q.location - vcat([μ_x], μ_y)) \n        + sum(abs2, diag(q.scale) - vcat(σ_x, σ_y))\n    (dist = sqrt(dist2),)\nend\n\n_, _, stats_cfe, _ = AdvancedVI.optimize(\n    model,\n    cfe,\n    q0_trans,\n    max_iter;\n    show_progress = false,\n    adtype        = AutoForwardDiff(),\n    optimizer     = Optimisers.Adam(3e-3),\n    callback      = callback,\n); \n\n_, _, stats_stl, _ = AdvancedVI.optimize(\n    model,\n    repgradelbo,\n    q0_trans,\n    max_iter;\n    show_progress = false,\n    adtype        = AutoForwardDiff(),\n    optimizer     = Optimisers.Adam(3e-3),\n    callback      = callback,\n); \n\nt        = [stat.iteration for stat in stats_cfe]\nelbo_cfe = [stat.elbo      for stat in stats_cfe]\nelbo_stl = [stat.elbo      for stat in stats_stl]\ndist_cfe = [stat.dist      for stat in stats_cfe]\ndist_stl = [stat.dist      for stat in stats_stl]\nplot( t, elbo_cfe, label=\"BBVI CFE\", xlabel=\"Iteration\", ylabel=\"ELBO\")\nplot!(t, elbo_stl, label=\"BBVI STL\", xlabel=\"Iteration\", ylabel=\"ELBO\")\nsavefig(\"advi_stl_elbo.svg\")\n\nplot( t, dist_cfe, label=\"BBVI CFE\", xlabel=\"Iteration\", ylabel=\"distance to optimum\", yscale=:log10)\nplot!(t, dist_stl, label=\"BBVI STL\", xlabel=\"Iteration\", ylabel=\"distance to optimum\", yscale=:log10)\nsavefig(\"advi_stl_dist.svg\")\nnothing","category":"page"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"(Image: )","category":"page"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"We can see that the noise of the repgradelbo estimator becomes smaller as VI converges. However, the speed of convergence may not always be significantly different. Also, due to noise, just looking at the ELBO may not be sufficient to judge which algorithm is better. This can be made apparent if we measure convergence through the distance to the optimum:","category":"page"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"(Image: )","category":"page"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"We can see that STL kicks-in at later stages of optimization. Therefore, when STL \"works\", it yields a higher accuracy solution even on large stepsizes. However, whether STL works or not highly depends on the problem[KMG2024]. Furthermore, in a lot of cases, a low-accuracy solution may be sufficient.","category":"page"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"[RWD2017]: Roeder, G., Wu, Y., & Duvenaud, D. K. (2017). Sticking the landing: Simple, lower-variance gradient estimators for variational inference. Advances in Neural Information Processing Systems, 30.","category":"page"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"[KMG2024]: Kim, K., Ma, Y., & Gardner, J. (2024). Linear Convergence of Black-Box Variational Inference: Should We Stick the Landing?. In International Conference on Artificial Intelligence and Statistics (pp. 235-243). PMLR.","category":"page"},{"location":"elbo/repgradelbo/#Advanced-Usage","page":"Reparameterization Gradient Estimator","title":"Advanced Usage","text":"","category":"section"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"There are two major ways to customize the behavior of RepGradELBO","category":"page"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"Customize the Distributions functions: rand(q), entropy(q), logpdf(q).\nCustomize AdvancedVI.reparam_with_entropy.","category":"page"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"It is generally recommended to customize rand(q), entropy(q), logpdf(q), since it will easily compose with other functionalities provided by AdvancedVI.","category":"page"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"The most advanced way is to customize AdvancedVI.reparam_with_entropy. In particular, reparam_with_entropy is the function that invokes rand(q), entropy(q), logpdf(q). Thus, it is the most general way to override the behavior of RepGradELBO.","category":"page"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"AdvancedVI.reparam_with_entropy","category":"page"},{"location":"elbo/repgradelbo/#AdvancedVI.reparam_with_entropy","page":"Reparameterization Gradient Estimator","title":"AdvancedVI.reparam_with_entropy","text":"reparam_with_entropy(rng, q, q_stop, n_samples, ent_est)\n\nDraw n_samples from q and compute its entropy.\n\nArguments\n\nrng::Random.AbstractRNG: Random number generator.\nq: Variational approximation.\nq_stop: Same as q, but held constant during differentiation. Should only be used for computing the entropy.\nn_samples::Int: Number of Monte Carlo samples \nent_est: The entropy estimation strategy. (See estimate_entropy.)\n\nReturns\n\nsamples: Monte Carlo samples generated through reparameterization. Their support matches that of the target distribution.\nentropy: An estimate (or exact value) of the differential entropy of q.\n\n\n\n\n\n","category":"function"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"To illustrate how we can customize the rand(q) function, we will implement quasi-Monte-Carlo variational inference[BWM2018]. Consider the case where we use the MeanFieldGaussian variational family. In this case, it suffices to override its rand specialization as follows:","category":"page"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"using QuasiMonteCarlo\nusing StatsFuns\n\nqmcrng = SobolSample(; R=OwenScramble(; base=2, pad=32))\n\nfunction Distributions.rand(\n    rng::AbstractRNG, q::MvLocationScale{<:Diagonal,D,L}, num_samples::Int\n) where {L,D}\n    @unpack location, scale, dist = q\n    n_dims = length(location)\n    scale_diag = diag(scale)\n    unif_samples = QuasiMonteCarlo.sample(num_samples, length(q), qmcrng)\n    std_samples = norminvcdf.(unif_samples)\n    return scale_diag .* std_samples .+ location\nend\nnothing","category":"page"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"(Note that this is a quick-and-dirty example, and there are more sophisticated ways to implement this.)","category":"page"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"repgradelbo = AdvancedVI.RepGradELBO(n_montecarlo);\n\n_, _, stats_qmc, _ = AdvancedVI.optimize(\n    model,\n    repgradelbo,\n    q0_trans,\n    max_iter;\n    show_progress = false,\n    adtype        = AutoForwardDiff(),\n    optimizer     = Optimisers.Adam(3e-3),\n    callback      = callback,\n); \n\nt        = [stat.iteration for stat in stats_qmc]\nelbo_qmc = [stat.elbo      for stat in stats_qmc]\ndist_qmc = [stat.dist      for stat in stats_qmc]\nplot( t, elbo_cfe, label=\"BBVI CFE\",     xlabel=\"Iteration\", ylabel=\"ELBO\")\nplot!(t, elbo_qmc, label=\"BBVI CFE QMC\", xlabel=\"Iteration\", ylabel=\"ELBO\")\nsavefig(\"advi_qmc_elbo.svg\")\n\nplot( t, dist_cfe, label=\"BBVI CFE\",     xlabel=\"Iteration\", ylabel=\"distance to optimum\", yscale=:log10)\nplot!(t, dist_qmc, label=\"BBVI CFE QMC\", xlabel=\"Iteration\", ylabel=\"distance to optimum\", yscale=:log10)\nsavefig(\"advi_qmc_dist.svg\")\n\n# The following definition is necessary to revert the behavior of `rand` so that \n# the example in example.md works with the regular non-QMC estimator.\nfunction Distributions.rand(\n    rng::AbstractRNG, q::MvLocationScale{<:Diagonal, D, L}, num_samples::Int\n) where {L, D}\n    @unpack location, scale, dist = q \n    n_dims       = length(location)\n    scale_diag   = diag(scale)\n    scale_diag.*rand(rng, dist, n_dims, num_samples) .+ location\nend\nnothing","category":"page"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"By plotting the ELBO, we can see the effect of quasi-Monte Carlo. (Image: ) We can see that quasi-Monte Carlo results in much lower variance than naive Monte Carlo. However, similarly to the STL example, just looking at the ELBO is often insufficient to really judge performance. Instead, let's look at the distance to the global optimum:","category":"page"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"(Image: )","category":"page"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"QMC yields an additional order of magnitude in accuracy. Also, unlike STL, it ever-so slightly accelerates convergence. This is because quasi-Monte Carlo uniformly reduces variance, unlike STL, which reduces variance only near the optimum.","category":"page"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"[BWM2018]: Buchholz, A., Wenzel, F., & Mandt, S. (2018). Quasi-monte carlo variational inference. In International Conference on Machine Learning.","category":"page"},{"location":"","page":"AdvancedVI","title":"AdvancedVI","text":"CurrentModule = AdvancedVI","category":"page"},{"location":"#AdvancedVI","page":"AdvancedVI","title":"AdvancedVI","text":"","category":"section"},{"location":"#Introduction","page":"AdvancedVI","title":"Introduction","text":"","category":"section"},{"location":"","page":"AdvancedVI","title":"AdvancedVI","text":"AdvancedVI provides implementations of variational Bayesian inference (VI) algorithms. VI algorithms perform scalable and computationally efficient Bayesian inference at the cost of asymptotic exactness. AdvancedVI is part of the Turing probabilistic programming ecosystem.","category":"page"},{"location":"#Provided-Algorithms","page":"AdvancedVI","title":"Provided Algorithms","text":"","category":"section"},{"location":"","page":"AdvancedVI","title":"AdvancedVI","text":"AdvancedVI currently provides the following algorithm for evidence lower bound maximization:","category":"page"},{"location":"","page":"AdvancedVI","title":"AdvancedVI","text":"Evidence Lower-Bound Maximization","category":"page"}]
}
