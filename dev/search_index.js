var documenterSearchIndex = {"docs":
[{"location":"elbo/overview/#elbomax","page":"Overview","title":"Evidence Lower Bound Maximization","text":"","category":"section"},{"location":"elbo/overview/#Introduction","page":"Overview","title":"Introduction","text":"","category":"section"},{"location":"elbo/overview/","page":"Overview","title":"Overview","text":"Evidence lower bound (ELBO) maximization[JGJS1999] is a general family of algorithms that minimize the exclusive (or reverse) Kullback-Leibler (KL) divergence between the target distribution pi and a variational approximation q_lambda. More generally, they aim to solve the following problem:","category":"page"},{"location":"elbo/overview/","page":"Overview","title":"Overview","text":"  mathrmminimize_q in mathcalQquad mathrmKLleft(q piright)","category":"page"},{"location":"elbo/overview/","page":"Overview","title":"Overview","text":"where mathcalQ is some family of distributions, often called the variational family. Since the target distribution pi is intractable in general, the KL divergence is also intractable. Instead, the ELBO maximization strategy maximizes a surrogate objective, the ELBO:","category":"page"},{"location":"elbo/overview/","page":"Overview","title":"Overview","text":"  mathrmELBOleft(qright) triangleq mathbbE_theta sim q log pileft(thetaright) + mathbbHleft(qright)","category":"page"},{"location":"elbo/overview/","page":"Overview","title":"Overview","text":"which serves as a lower bound to the KL. The ELBO and its gradient can be readily estimated through various strategies. Overall, ELBO maximization algorithms aim to solve the problem:","category":"page"},{"location":"elbo/overview/","page":"Overview","title":"Overview","text":"  mathrmmaximize_q in mathcalQquad mathrmELBOleft(qright)","category":"page"},{"location":"elbo/overview/","page":"Overview","title":"Overview","text":"Multiple ways to solve this problem exist, each leading to a different variational inference algorithm.","category":"page"},{"location":"elbo/overview/#Algorithms","page":"Overview","title":"Algorithms","text":"","category":"section"},{"location":"elbo/overview/","page":"Overview","title":"Overview","text":"Currently, AdvancedVI only provides the approach known as black-box variational inference (also known as Monte Carlo VI, Stochastic Gradient VI). (Introduced independently by two groups [RGB2014][TL2014] in 2014.) In particular, AdvancedVI focuses on the reparameterization gradient estimator[TL2014][RMW2014][KW2014], which is generally superior compared to alternative strategies[XQKS2019], discussed in the following section:","category":"page"},{"location":"elbo/overview/","page":"Overview","title":"Overview","text":"RepGradELBO","category":"page"},{"location":"elbo/overview/","page":"Overview","title":"Overview","text":"[JGJS1999]: Jordan, M. I., Ghahramani, Z., Jaakkola, T. S., & Saul, L. K. (1999). An introduction to variational methods for graphical models. Machine learning, 37, 183-233.","category":"page"},{"location":"elbo/overview/","page":"Overview","title":"Overview","text":"[TL2014]: Titsias, M., & Lázaro-Gredilla, M. (2014). Doubly stochastic variational Bayes for non-conjugate inference. In International Conference on Machine Learning.","category":"page"},{"location":"elbo/overview/","page":"Overview","title":"Overview","text":"[RMW2014]: Rezende, D. J., Mohamed, S., & Wierstra, D. (2014). Stochastic backpropagation and approximate inference in deep generative models. In International Conference on Machine Learning.","category":"page"},{"location":"elbo/overview/","page":"Overview","title":"Overview","text":"[KW2014]: Kingma, D. P., & Welling, M. (2014). Auto-encoding variational bayes. In International Conference on Learning Representations.","category":"page"},{"location":"elbo/overview/","page":"Overview","title":"Overview","text":"[XQKS2019]: Xu, M., Quiroz, M., Kohn, R., & Sisson, S. A. (2019). Variance reduction properties of the reparameterization trick. In *The International Conference on Artificial Intelligence and Statistics.","category":"page"},{"location":"elbo/overview/","page":"Overview","title":"Overview","text":"[RGB2014]: Ranganath, R., Gerrish, S., & Blei, D. (2014). Black box variational inference. In Artificial Intelligence and Statistics.","category":"page"},{"location":"optimization/#optim","page":"Optimization","title":"Optimization","text":"","category":"section"},{"location":"optimization/#Parameter-Free-Optimization-Rules","page":"Optimization","title":"Parameter-Free Optimization Rules","text":"","category":"section"},{"location":"optimization/","page":"Optimization","title":"Optimization","text":"We provide custom optimization rules that are not provided out-of-the-box by Optimisers.jl. The main theme of the provided optimizers is that they are parameter-free. This means that these optimization rules shouldn't require (or barely) any tuning to obtain performance competitive with well-tuned alternatives.","category":"page"},{"location":"optimization/#AdvancedVI.DoG","page":"Optimization","title":"AdvancedVI.DoG","text":"DoG(repsilon)\n\nDistance over gradient (DoG[IHC2023]) optimizer. It's only parameter is the initial guess of the Euclidean distance to the optimum repsilon. The original paper recommends $ 10^{-4} ( 1 + \\lVert \\lambda_0 \\rVert ) $, but the default value is $ 10^{-6} $.\n\nParameters\n\nrepsilon: Initial guess of the Euclidean distance between the initial point and the optimum. (default value: 1e-6)\n\n\n\n\n\n","category":"type"},{"location":"optimization/#AdvancedVI.DoWG","page":"Optimization","title":"AdvancedVI.DoWG","text":"DoWG(repsilon)\n\nDistance over weighted gradient (DoWG[KMJ2024]) optimizer. It's only parameter is the initial guess of the Euclidean distance to the optimum repsilon.\n\nParameters\n\nrepsilon: Initial guess of the Euclidean distance between the initial point and           the optimum. (default value: 1e-6)\n\n\n\n\n\n","category":"type"},{"location":"optimization/#AdvancedVI.COCOB","page":"Optimization","title":"AdvancedVI.COCOB","text":"COCOB(alpha)\n\nContinuous Coin Betting (COCOB[OT2017]) optimizer. We use the \"COCOB-Backprop\" variant, which is closer to the Adam optimizer. It's only parameter is the maximum change per parameter α, which shouldn't need much tuning.\n\nParameters\n\nalpha: Scaling parameter. (default value: 100)\n\n[OT2017]: Orabona, F., & Tommasi, T. (2017). Training deep networks without learning rates through coin betting. Advances in Neural Information Processing Systems, 30.\n\n\n\n\n\n","category":"type"},{"location":"optimization/#Parameter-Averaging-Strategies","page":"Optimization","title":"Parameter Averaging Strategies","text":"","category":"section"},{"location":"optimization/","page":"Optimization","title":"Optimization","text":"In some cases, the best optimization performance is obtained by averaging the sequence of parameters generated by the optimization algorithm. For instance, the DoG[IHC2023] and DoWG[KMJ2024] papers report their best performance through averaging. The benefits of parameter averaging have been specifically confirmed for ELBO maximization[DCAMHV2020].","category":"page"},{"location":"optimization/#AdvancedVI.NoAveraging","page":"Optimization","title":"AdvancedVI.NoAveraging","text":"NoAveraging()\n\nNo averaging. This returns the last-iterate of the optimization rule.\n\n\n\n\n\n","category":"type"},{"location":"optimization/#AdvancedVI.PolynomialAveraging","page":"Optimization","title":"AdvancedVI.PolynomialAveraging","text":"PolynomialAveraging(eta)\n\nPolynomial averaging rule proposed Shamir and Zhang[SZ2013]. At iteration t, the parameter average $ \\bar{\\lambda}_t $ according to the polynomial averaging rule is given as\n\n    barlambda_t = (1 - w_t) barlambda_t-1 + w_t lambda_t  \n\nwhere the averaging weight is \n\n    w_t = fraceta + 1t + eta  \n\nHigher eta (eta) down-weights earlier iterations. When eta=0, this is equivalent to uniformly averaging the iterates in an online fashion. The DoG paper[IHC2023] suggests eta=8.\n\nParameters\n\neta: Regularization term. (default: 8)\n\n[SZ2013]: Shamir, O., & Zhang, T. (2013). Stochastic gradient descent for non-smooth optimization: Convergence results and optimal averaging schemes. In International conference on machine learning (pp. 71-79). PMLR.\n\n\n\n\n\n","category":"type"},{"location":"optimization/","page":"Optimization","title":"Optimization","text":"[DCAMHV2020]: Dhaka, A. K., Catalina, A., Andersen, M. R., Magnusson, M., Huggins, J., & Vehtari, A. (2020). Robust, accurate stochastic optimization for variational inference. Advances in Neural Information Processing Systems, 33, 10961-10973.","category":"page"},{"location":"optimization/","page":"Optimization","title":"Optimization","text":"[KMJ2024]: Khaled, A., Mishchenko, K., & Jin, C. (2023). Dowg unleashed: An efficient universal parameter-free gradient descent method. Advances in Neural Information Processing Systems, 36, 6748-6769.","category":"page"},{"location":"optimization/","page":"Optimization","title":"Optimization","text":"[IHC2023]: Ivgi, M., Hinder, O., & Carmon, Y. (2023). Dog is sgd's best friend: A parameter-free dynamic step size schedule. In International Conference on Machine Learning (pp. 14465-14499). PMLR.","category":"page"},{"location":"optimization/#Operators","page":"Optimization","title":"Operators","text":"","category":"section"},{"location":"optimization/","page":"Optimization","title":"Optimization","text":"Depending on the variational family, variational objective, and optimization strategy, it might be necessary to modify the variational parameters after performing a gradient-based update. For this, an operator acting on the parameters can be supplied via the  operator keyword argument of AdvancedVI.optimize.","category":"page"},{"location":"optimization/#clipscale","page":"Optimization","title":"ClipScale","text":"","category":"section"},{"location":"optimization/","page":"Optimization","title":"Optimization","text":"For the location-scale family, it is often the case that optimization is stable only when the smallest eigenvalue of the scale matrix is strictly positive[D2020]. To ensure this, we provide the following projection operator:","category":"page"},{"location":"optimization/#AdvancedVI.ClipScale","page":"Optimization","title":"AdvancedVI.ClipScale","text":"ClipScale(ϵ = 1e-5)\n\nProjection operator ensuring that an MvLocationScale or MvLocationScaleLowRank has a scale with eigenvalues larger than ϵ. ClipScale also supports by operating on MvLocationScale and MvLocationScaleLowRank wrapped by a Bijectors.TransformedDistribution object. \n\n\n\n\n\n","category":"type"},{"location":"optimization/#proximalocationscaleentropy","page":"Optimization","title":"ProximalLocationScaleEntropy","text":"","category":"section"},{"location":"optimization/","page":"Optimization","title":"Optimization","text":"ELBO maximization with the location-scale family tends to be unstable when the scale has small eigenvalues or the stepsize is large. To remedy this, a proximal operator of the entropy[D2020] can be used.","category":"page"},{"location":"optimization/#AdvancedVI.ProximalLocationScaleEntropy","page":"Optimization","title":"AdvancedVI.ProximalLocationScaleEntropy","text":"ProximalLocationScaleEntropy()\n\nProximal operator for the entropy of a location-scale distribution, which is defined as\n\n    mathrmprox(lambda) = argmin_lambda^prime - mathbbH(q_lambda^prime) + frac12 gamma_t leftlVert lambda - lambda^prime rightrVert \n\nwhere gamma_t is the stepsize the optimizer used with the proximal operator. This assumes the variational family is <:VILocationScale and the optimizer is one of the following:\n\nDoG\nDoWG\nDescent\n\nFor ELBO maximization, since this proximal operator handles the entropy, the gradient estimator for the ELBO must ignore the entropy term. That is, the entropy keyword argument of RepGradELBO muse be one of the following:\n\nClosedFormEntropyZeroGradient\nStickingTheLandingEntropyZeroGradient\n\n\n\n\n\n","category":"type"},{"location":"optimization/","page":"Optimization","title":"Optimization","text":"[D2020]: Domke, J. (2020). Provable smoothness guarantees for black-box variational inference. In International Conference on Machine Learning.","category":"page"},{"location":"families/#families","page":"Variational Families","title":"Reparameterizable Variational Families","text":"","category":"section"},{"location":"families/","page":"Variational Families","title":"Variational Families","text":"The RepGradELBO objective assumes that the members of the variational family have a differentiable sampling path. We provide multiple pre-packaged variational families that can be readily used.","category":"page"},{"location":"families/#locscale","page":"Variational Families","title":"The LocationScale Family","text":"","category":"section"},{"location":"families/","page":"Variational Families","title":"Variational Families","text":"The location-scale variational family is a family of probability distributions, where their sampling process can be represented as","category":"page"},{"location":"families/","page":"Variational Families","title":"Variational Families","text":"z sim  q_lambda qquadLeftrightarrowqquad\nz stackreld= C u + mquad u sim varphi","category":"page"},{"location":"families/","page":"Variational Families","title":"Variational Families","text":"where C is the scale, m is the location, and varphi is the base distribution. m and C form the variational parameters lambda = (m C) of q_lambda. The location-scale family encompases many practical variational families, which can be instantiated by setting the base distribution of u and the structure of C.","category":"page"},{"location":"families/","page":"Variational Families","title":"Variational Families","text":"The probability density is given by","category":"page"},{"location":"families/","page":"Variational Families","title":"Variational Families","text":"  q_lambda(z) = C^-1 varphi(C^-1(z - m))","category":"page"},{"location":"families/","page":"Variational Families","title":"Variational Families","text":"the covariance is given as","category":"page"},{"location":"families/","page":"Variational Families","title":"Variational Families","text":"  mathrmVarleft(q_lambdaright) = C mathrmVar(q_lambda) C^top","category":"page"},{"location":"families/","page":"Variational Families","title":"Variational Families","text":"and the entropy is given as","category":"page"},{"location":"families/","page":"Variational Families","title":"Variational Families","text":"  mathbbH(q_lambda) = mathbbH(varphi) + log C","category":"page"},{"location":"families/","page":"Variational Families","title":"Variational Families","text":"where mathbbH(varphi) is the entropy of the base distribution. Notice the mathbbH(varphi) does not depend on log C. The derivative of the entropy with respect to lambda is thus independent of the base distribution.","category":"page"},{"location":"families/#API","page":"Variational Families","title":"API","text":"","category":"section"},{"location":"families/","page":"Variational Families","title":"Variational Families","text":"note: Note\nFor stable convergence, the initial scale needs to be sufficiently large and well-conditioned. Initializing scale to have small eigenvalues will often result in initial divergences and numerical instabilities.","category":"page"},{"location":"families/#AdvancedVI.MvLocationScale","page":"Variational Families","title":"AdvancedVI.MvLocationScale","text":"MvLocationScale(location, scale, dist)\n\nThe location scale variational family broadly represents various variational families using location and scale variational parameters.\n\nIt generally represents any distribution for which the sampling path can be represented as follows:\n\n  d = length(location)\n  u = rand(dist, d)\n  z = scale*u + location\n\n\n\n\n\n","category":"type"},{"location":"families/","page":"Variational Families","title":"Variational Families","text":"The following are specialized constructors for convenience:","category":"page"},{"location":"families/#AdvancedVI.FullRankGaussian","page":"Variational Families","title":"AdvancedVI.FullRankGaussian","text":"FullRankGaussian(μ, L)\n\nConstruct a Gaussian variational approximation with a dense covariance matrix.\n\nArguments\n\nμ::AbstractVector{T}: Mean of the Gaussian.\nL::LinearAlgebra.AbstractTriangular{T}: Cholesky factor of the covariance of the Gaussian.\n\n\n\n\n\n","category":"function"},{"location":"families/#AdvancedVI.MeanFieldGaussian","page":"Variational Families","title":"AdvancedVI.MeanFieldGaussian","text":"MeanFieldGaussian(μ, L)\n\nConstruct a Gaussian variational approximation with a diagonal covariance matrix.\n\nArguments\n\nμ::AbstractVector{T}: Mean of the Gaussian.\nL::Diagonal{T}: Diagonal Cholesky factor of the covariance of the Gaussian.\n\n\n\n\n\n","category":"function"},{"location":"families/#Gaussian-Variational-Families","page":"Variational Families","title":"Gaussian Variational Families","text":"","category":"section"},{"location":"families/","page":"Variational Families","title":"Variational Families","text":"using AdvancedVI, LinearAlgebra, Distributions;\nμ = zeros(2);\n\nL = LowerTriangular(diagm(ones(2)));\nq = FullRankGaussian(μ, L)\n\nL = Diagonal(ones(2));\nq = MeanFieldGaussian(μ, L)","category":"page"},{"location":"families/#Student-t-Variational-Families","page":"Variational Families","title":"Student-t Variational Families","text":"","category":"section"},{"location":"families/","page":"Variational Families","title":"Variational Families","text":"using AdvancedVI, LinearAlgebra, Distributions;\nμ = zeros(2);\nν = 3;\n\n# Full-Rank \nL = LowerTriangular(diagm(ones(2)));\nq = MvLocationScale(μ, L, TDist(ν))\n\n# Mean-Field\nL = Diagonal(ones(2));\nq = MvLocationScale(μ, L, TDist(ν))","category":"page"},{"location":"families/#Laplace-Variational-families","page":"Variational Families","title":"Laplace Variational families","text":"","category":"section"},{"location":"families/","page":"Variational Families","title":"Variational Families","text":"using AdvancedVI, LinearAlgebra, Distributions;\nμ = zeros(2);\n\n# Full-Rank \nL = LowerTriangular(diagm(ones(2)));\nq = MvLocationScale(μ, L, Laplace())\n\n# Mean-Field\nL = Diagonal(ones(2));\nq = MvLocationScale(μ, L, Laplace())","category":"page"},{"location":"families/#The-LocationScaleLowRank-Family","page":"Variational Families","title":"The LocationScaleLowRank Family","text":"","category":"section"},{"location":"families/","page":"Variational Families","title":"Variational Families","text":"In practice, LocationScale families with full-rank scale matrices are known to converge slowly as they require a small SGD stepsize. Low-rank variational families can be an effective alternative[ONS2018]. LocationScaleLowRank generally represent any d-dimensional distribution which its sampling path can be represented as","category":"page"},{"location":"families/","page":"Variational Families","title":"Variational Families","text":"z sim  q_lambda qquadLeftrightarrowqquad\nz stackreld= D u_1 + U u_2  + mquad u_1 u_2 sim varphi","category":"page"},{"location":"families/","page":"Variational Families","title":"Variational Families","text":"where D in mathbbR^d times d is a diagonal matrix, U in mathbbR^d times r is a dense low-rank matrix for the rank r  0, m in mathbbR^d is the location, and varphi is the base distribution. m, D, and U form the variational parameters lambda = (m D U).","category":"page"},{"location":"families/","page":"Variational Families","title":"Variational Families","text":"The covariance of this distribution is given as","category":"page"},{"location":"families/","page":"Variational Families","title":"Variational Families","text":"  mathrmVarleft(q_lambdaright) = D mathrmVar(varphi) D + U mathrmVar(varphi) U^top","category":"page"},{"location":"families/","page":"Variational Families","title":"Variational Families","text":"and the entropy is given by the matrix determinant lemma as","category":"page"},{"location":"families/","page":"Variational Families","title":"Variational Families","text":"  mathbbH(q_lambda) \n  = mathbbH(varphi) + log Sigma\n  = mathbbH(varphi) + 2 log D + log I + U^top D^-2 U","category":"page"},{"location":"families/","page":"Variational Families","title":"Variational Families","text":"where mathbbH(varphi) is the entropy of the base distribution.","category":"page"},{"location":"families/","page":"Variational Families","title":"Variational Families","text":"Consider a 30-dimensional Gaussian with a diagonal plus low-rank covariance structure, where the true rank is 3. Then, we can compare the convergence speed of LowRankGaussian versus FullRankGaussian:","category":"page"},{"location":"families/","page":"Variational Families","title":"Variational Families","text":"(Image: )","category":"page"},{"location":"families/","page":"Variational Families","title":"Variational Families","text":"As we can see, LowRankGaussian converges faster than FullRankGaussian. While FullRankGaussian can converge to the true solution since it is a more expressive variational family, LowRankGaussian gets there faster.","category":"page"},{"location":"families/","page":"Variational Families","title":"Variational Families","text":"info: Info\nMvLocationScaleLowRank tend to work better with the Optimisers.Adam optimizer due to non-smoothness. Other optimisers may experience divergences.","category":"page"},{"location":"families/#API-2","page":"Variational Families","title":"API","text":"","category":"section"},{"location":"families/#AdvancedVI.MvLocationScaleLowRank","page":"Variational Families","title":"AdvancedVI.MvLocationScaleLowRank","text":"MvLocationLowRankScale(location, scale_diag, scale_factors, dist)\n\nVariational family with a covariance in the form of a diagonal matrix plus a squared low-rank matrix. The rank is given by size(scale_factors, 2).\n\nIt generally represents any distribution for which the sampling path can be represented as follows:\n\n  d = length(location)\n  r = size(scale_factors, 2)\n  u_diag = rand(dist, d)\n  u_factors = rand(dist, r)\n  z = scale_diag.*u_diag + scale_factors*u_factors + location\n\n\n\n\n\n","category":"type"},{"location":"families/","page":"Variational Families","title":"Variational Families","text":"The logpdf of  MvLocationScaleLowRank has an optional argument non_differentiable::Bool (default: false). If set as true, a more efficient Oleft(r d^2right) implementation is used to evaluate the density. This, however, is not differentiable under most AD frameworks due to the use of Cholesky lowrankupdate. The default value is false, which uses a Oleft(d^3right) implementation, is differentiable and therefore compatible with the StickingTheLandingEntropy estimator.","category":"page"},{"location":"families/","page":"Variational Families","title":"Variational Families","text":"The following is a specialized constructor for convenience:","category":"page"},{"location":"families/#AdvancedVI.LowRankGaussian","page":"Variational Families","title":"AdvancedVI.LowRankGaussian","text":"LowRankGaussian(μ, D, U)\n\nConstruct a Gaussian variational approximation with a diagonal plus low-rank covariance matrix.\n\nArguments\n\nμ::AbstractVector{T}: Mean of the Gaussian.\nD::Vector{T}: Diagonal of the scale.\nU::Matrix{T}: Low-rank factors of the scale, where size(U,2) is the rank.\n\n\n\n\n\n","category":"function"},{"location":"families/","page":"Variational Families","title":"Variational Families","text":"[ONS2018]: Ong, V. M. H., Nott, D. J., & Smith, M. S. (2018). Gaussian variational approximation with a factor covariance structure. Journal of Computational and Graphical Statistics, 27(3), 465-478.","category":"page"},{"location":"examples/#examples","page":"Examples","title":"Evidence Lower Bound Maximization","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"In this tutorial, we will work with a normal-log-normal model.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"beginaligned\nx sim mathrmLogNormalleft(mu_x sigma_x^2right) \ny sim mathcalNleft(mu_y sigma_y^2right)\nendaligned","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"BBVI with Bijectors.Exp bijectors is able to infer this model exactly.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"Using the LogDensityProblems interface, we the model can be defined as follows:","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"using LogDensityProblems\n\nstruct NormalLogNormal{MX,SX,MY,SY}\n    μ_x::MX\n    σ_x::SX\n    μ_y::MY\n    Σ_y::SY\nend\n\nfunction LogDensityProblems.logdensity(model::NormalLogNormal, θ)\n    (; μ_x, σ_x, μ_y, Σ_y) = model\n    return logpdf(LogNormal(μ_x, σ_x), θ[1]) + logpdf(MvNormal(μ_y, Σ_y), θ[2:end])\nend\n\nfunction LogDensityProblems.dimension(model::NormalLogNormal)\n    return length(model.μ_y) + 1\nend\n\nfunction LogDensityProblems.capabilities(::Type{<:NormalLogNormal})\n    return LogDensityProblems.LogDensityOrder{0}()\nend","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"Let's now instantiate the model","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"using LinearAlgebra\n\nn_dims = 10\nμ_x = randn()\nσ_x = exp.(randn())\nμ_y = randn(n_dims)\nσ_y = exp.(randn(n_dims))\nmodel = NormalLogNormal(μ_x, σ_x, μ_y, Diagonal(σ_y .^ 2));\nnothing","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"Since the y follows a log-normal prior, its support is bounded to be the positive half-space mathbbR_+. Thus, we will use Bijectors to match the support of our target posterior and the variational approximation.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"using Bijectors\n\nfunction Bijectors.bijector(model::NormalLogNormal)\n    (; μ_x, σ_x, μ_y, Σ_y) = model\n    return Bijectors.Stacked(\n        Bijectors.bijector.([LogNormal(μ_x, σ_x), MvNormal(μ_y, Σ_y)]),\n        [1:1, 2:(1 + length(μ_y))],\n    )\nend\n\nb = Bijectors.bijector(model);\nbinv = inverse(b)\nnothing","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"Let's now load AdvancedVI. Since BBVI relies on automatic differentiation (AD), we need to load an AD library, before loading AdvancedVI. Also, the selected AD framework needs to be communicated to AdvancedVI using the ADTypes interface. Here, we will use ForwardDiff, which can be selected by later passing ADTypes.AutoForwardDiff().","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"using Optimisers\nusing ADTypes, ForwardDiff\nusing AdvancedVI","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"We now need to select 1. a variational objective, and 2. a variational family. Here, we will use the RepGradELBO objective, which expects an object implementing the LogDensityProblems interface, and the inverse bijector.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"n_montecaro = 10;\nobjective = RepGradELBO(n_montecaro)","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"For the variational family, we will use the classic mean-field Gaussian family.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"d = LogDensityProblems.dimension(model);\nμ = randn(d);\nL = Diagonal(ones(d));\nq0 = AdvancedVI.MeanFieldGaussian(μ, L)\nnothing","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"And then, we now apply the bijector to the variational family.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"q0_trans = Bijectors.TransformedDistribution(q0, binv)\nnothing","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"Passing objective and the initial variational approximation q to optimize performs inference.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"n_max_iter = 10^4\nq_avg_trans, q_trans, stats, _ = AdvancedVI.optimize(\n    model,\n    objective,\n    q0_trans,\n    n_max_iter;\n    show_progress=false,\n    adtype=AutoForwardDiff(),\n    optimizer=Optimisers.Adam(1e-3),\n    operator=ClipScale(),\n);\nnothing","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"ClipScale is a projection operator, which ensures that the variational approximation stays within a stable region of the variational family. For more information see this section.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"q_avg_trans is the final output of the optimization procedure. If a parameter averaging strategy is used through the keyword argument averager, q_avg_trans is be the output of the averaging strategy, while q_trans is the last iterate.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"The selected inference procedure stores per-iteration statistics into stats. For instance, the ELBO can be ploted as follows:","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"using Plots\n\nt = [stat.iteration for stat in stats]\ny = [stat.elbo for stat in stats]\nplot(t, y; label=\"BBVI\", xlabel=\"Iteration\", ylabel=\"ELBO\")\nsavefig(\"bbvi_example_elbo.svg\")\nnothing","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"(Image: )","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"Further information can be gathered by defining your own callback!.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"The final ELBO can be estimated by calling the objective directly with a different number of Monte Carlo samples as follows:","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"estimate_objective(objective, q_avg_trans, model; n_samples=10^4)","category":"page"},{"location":"general/#general","page":"General Usage","title":"General Usage","text":"","category":"section"},{"location":"general/","page":"General Usage","title":"General Usage","text":"Each VI algorithm provides the followings:","category":"page"},{"location":"general/","page":"General Usage","title":"General Usage","text":"Variational families supported by each VI algorithm.\nA variational objective corresponding to the VI algorithm. Note that each variational family is subject to its own constraints. Thus, please refer to the documentation of the variational inference algorithm of interest.","category":"page"},{"location":"general/#Optimizing-a-Variational-Objective","page":"General Usage","title":"Optimizing a Variational Objective","text":"","category":"section"},{"location":"general/","page":"General Usage","title":"General Usage","text":"After constructing a variational objective objective and initializing a variational approximation, one can optimize objective by calling optimize:","category":"page"},{"location":"general/#AdvancedVI.optimize","page":"General Usage","title":"AdvancedVI.optimize","text":"optimize(problem, objective, q_init, max_iter, objargs...; kwargs...)\n\nOptimize the variational objective objective targeting the problem problem by estimating (stochastic) gradients.\n\nThe trainable parameters in the variational approximation are expected to be extractable through Optimisers.destructure. This requires the variational approximation to be marked as a functor through Functors.@functor.\n\nArguments\n\nobjective::AbstractVariationalObjective: Variational Objective.\nq_init: Initial variational distribution. The variational parameters must be extractable through Optimisers.destructure.\nmax_iter::Int: Maximum number of iterations.\nobjargs...: Arguments to be passed to objective.\n\nKeyword Arguments\n\nadtype::ADtypes.AbstractADType: Automatic differentiation backend. \noptimizer::Optimisers.AbstractRule: Optimizer used for inference. (Default: Adam.)\naverager::AbstractAverager : Parameter averaging strategy. (Default: NoAveraging())\noperator::AbstractOperator : Operator applied to the parameters after each optimization step. (Default: IdentityOperator())\nrng::AbstractRNG: Random number generator. (Default: Random.default_rng().)\nshow_progress::Bool: Whether to show the progress bar. (Default: true.)\ncallback: Callback function called after every iteration. See further information below. (Default: nothing.)\nprog: Progress bar configuration. (Default: ProgressMeter.Progress(n_max_iter; desc=\"Optimizing\", barlen=31, showspeed=true, enabled=prog).)\nstate::NamedTuple: Initial value for the internal state of optimization. Used to warm-start from the state of a previous run. (See the returned values below.)\n\nReturns\n\naveraged_params: Variational parameters generated by the algorithm averaged according to averager.\nparams: Last variational parameters generated by the algorithm.\nstats: Statistics gathered during optimization.\nstate: Collection of the final internal states of optimization. This can used later to warm-start from the last iteration of the corresponding run.\n\nCallback\n\nThe callback function callback has a signature of\n\ncallback(; stat, state, params, averaged_params, restructure, gradient)\n\nThe arguments are as follows:\n\nstat: Statistics gathered during the current iteration. The content will vary depending on objective.\nstate: Collection of the internal states used for optimization.\nparams: Variational parameters.\naveraged_params: Variational parameters averaged according to the averaging strategy.\nrestructure: Function that restructures the variational approximation from the variational parameters. Calling restructure(param) reconstructs the variational approximation. \ngradient: The estimated (possibly stochastic) gradient.\n\ncallback can return a NamedTuple containing some additional information computed within cb. This will be appended to the statistic of the current corresponding iteration. Otherwise, just return nothing.\n\n\n\n\n\n","category":"function"},{"location":"general/#Estimating-the-Objective","page":"General Usage","title":"Estimating the Objective","text":"","category":"section"},{"location":"general/","page":"General Usage","title":"General Usage","text":"In some cases, it is useful to directly estimate the objective value. This can be done by the following funciton:","category":"page"},{"location":"general/#AdvancedVI.estimate_objective","page":"General Usage","title":"AdvancedVI.estimate_objective","text":"estimate_objective([rng,] obj, q, prob; kwargs...)\n\nEstimate the variational objective obj targeting prob with respect to the variational approximation q.\n\nArguments\n\nrng::Random.AbstractRNG: Random number generator.\nobj::AbstractVariationalObjective: Variational objective.\nprob: The target log-joint likelihood implementing the LogDensityProblem interface.\nq: Variational approximation.\n\nKeyword Arguments\n\nDepending on the objective, additional keyword arguments may apply. Please refer to the respective documentation of each variational objective for more info.\n\nReturns\n\nobj_est: Estimate of the objective value.\n\n\n\n\n\n","category":"function"},{"location":"general/","page":"General Usage","title":"General Usage","text":"info: Info\nNote that estimate_objective is not expected to be differentiated through, and may not result in optimal statistical performance.","category":"page"},{"location":"general/#Advanced-Usage","page":"General Usage","title":"Advanced Usage","text":"","category":"section"},{"location":"general/","page":"General Usage","title":"General Usage","text":"Each variational objective is a subtype of the following abstract type:","category":"page"},{"location":"general/#AdvancedVI.AbstractVariationalObjective","page":"General Usage","title":"AdvancedVI.AbstractVariationalObjective","text":"AbstractVariationalObjective\n\nAbstract type for the VI algorithms supported by AdvancedVI.\n\nImplementations\n\nTo be supported by AdvancedVI, a VI algorithm must implement AbstractVariationalObjective and estimate_objective. Also, it should provide gradients by implementing the function estimate_gradient!. If the estimator is stateful, it can implement init to initialize the state.\n\n\n\n\n\n","category":"type"},{"location":"general/","page":"General Usage","title":"General Usage","text":"Furthermore, AdvancedVI only interacts with each variational objective by querying gradient estimates. Therefore, to create a new custom objective to be optimized through AdvancedVI, it suffices to implement the following function:","category":"page"},{"location":"general/#AdvancedVI.estimate_gradient!","page":"General Usage","title":"AdvancedVI.estimate_gradient!","text":"estimate_gradient!(rng, obj, adtype, out, prob, params, restructure, obj_state)\n\nEstimate (possibly stochastic) gradients of the variational objective obj targeting prob with respect to the variational parameters λ\n\nArguments\n\nrng::Random.AbstractRNG: Random number generator.\nobj::AbstractVariationalObjective: Variational objective.\nadtype::ADTypes.AbstractADType: Automatic differentiation backend. \nout::DiffResults.MutableDiffResult: Buffer containing the objective value and gradient estimates. \nprob: The target log-joint likelihood implementing the LogDensityProblem interface.\nparams: Variational parameters to evaluate the gradient on.\nrestructure: Function that reconstructs the variational approximation from params.\nobj_state: Previous state of the objective.\n\nReturns\n\nout::MutableDiffResult: Buffer containing the objective value and gradient estimates.\nobj_state: The updated state of the objective.\nstat::NamedTuple: Statistics and logs generated during estimation.\n\n\n\n\n\n","category":"function"},{"location":"general/","page":"General Usage","title":"General Usage","text":"If an objective needs to be stateful, one can implement the following function to inialize the state.","category":"page"},{"location":"general/#AdvancedVI.init","page":"General Usage","title":"AdvancedVI.init","text":"init(rng, obj, adtype, prob, params, restructure)\n\nInitialize a state of the variational objective obj given the initial variational parameters λ. This function needs to be implemented only if obj is stateful.\n\nArguments\n\nrng::Random.AbstractRNG: Random number generator.\nobj::AbstractVariationalObjective: Variational objective.\n\nadtype::ADTypes.AbstractADType`: Automatic differentiation backend.\n\nparams: Initial variational parameters.\nrestructure: Function that reconstructs the variational approximation from λ.\n\n\n\n\n\ninit(avg, params)\n\nInitialize the state of the averaging strategy avg with the initial parameters params.\n\nArguments\n\navg::AbstractAverager: Averaging strategy.\nparams: Initial variational parameters.\n\n\n\n\n\n","category":"function"},{"location":"elbo/repgradelbo/#repgradelbo","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"","category":"section"},{"location":"elbo/repgradelbo/#Overview","page":"Reparameterization Gradient Estimator","title":"Overview","text":"","category":"section"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"The reparameterization gradient[TL2014][RMW2014][KW2014] is an unbiased gradient estimator of the ELBO. Consider some variational family","category":"page"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"mathcalQ = q_lambda mid lambda in Lambda ","category":"page"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"where lambda is the variational parameters of q_lambda. If its sampling process can be described by some differentiable reparameterization function mathcalT_lambda and a base distribution varphi independent of lambda such that","category":"page"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"z sim  q_lambda qquadLeftrightarrowqquad\nz stackreld= mathcalT_lambdaleft(epsilonright)quad epsilon sim varphi","category":"page"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"we can effectively estimate the gradient of the ELBO by directly differentiating","category":"page"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"  widehatmathrmELBOleft(lambdaright) = frac1Msum^M_m=1 log pileft(mathcalT_lambdaleft(epsilon_mright)right) + mathbbHleft(q_lambdaright)","category":"page"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"where epsilon_m sim varphi are Monte Carlo samples, with respect to lambda. This estimator is called the reparameterization gradient estimator.","category":"page"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"In addition to the reparameterization gradient, AdvancedVI provides the following features:","category":"page"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"Posteriors with constrained supports are handled through Bijectors, which is known as the automatic differentiation VI (ADVI; [KTRGB2017]) formulation. (See this section.)\nThe gradient of the entropy can be estimated through various strategies depending on the capabilities of the variational family. (See this section.)","category":"page"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"[TL2014]: Titsias, M., & Lázaro-Gredilla, M. (2014). Doubly stochastic variational Bayes for non-conjugate inference. In International Conference on Machine Learning.","category":"page"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"[RMW2014]: Rezende, D. J., Mohamed, S., & Wierstra, D. (2014). Stochastic backpropagation and approximate inference in deep generative models. In International Conference on Machine Learning.","category":"page"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"[KW2014]: Kingma, D. P., & Welling, M. (2014). Auto-encoding variational bayes. In International Conference on Learning Representations.","category":"page"},{"location":"elbo/repgradelbo/#The-RepGradELBO-Objective","page":"Reparameterization Gradient Estimator","title":"The RepGradELBO Objective","text":"","category":"section"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"To use the reparameterization gradient, AdvancedVI provides the following variational objective:","category":"page"},{"location":"elbo/repgradelbo/#AdvancedVI.RepGradELBO","page":"Reparameterization Gradient Estimator","title":"AdvancedVI.RepGradELBO","text":"RepGradELBO(n_samples; kwargs...)\n\nEvidence lower-bound objective with the reparameterization gradient formulation[TL2014][RMW2014][KW2014]. This computes the evidence lower-bound (ELBO) through the formulation:\n\nbeginaligned\nmathrmELBOleft(lambdaright)\ntriangleq\nmathbbE_z sim q_lambdaleft\n  log pileft(zright)\nright\n+ mathbbHleft(q_lambdaright)\nendaligned\n\nArguments\n\nn_samples::Int: Number of Monte Carlo samples used to estimate the ELBO.\n\nKeyword Arguments\n\nentropy: The estimator for the entropy term. (Type <: AbstractEntropyEstimator; Default: ClosedFormEntropy())\n\nRequirements\n\nThe variational approximation q_lambda implements rand.\nThe target distribution and the variational approximation have the same support.\nThe target logdensity(prob, x) must be differentiable with respect to x by the selected AD backend.\n\nDepending on the options, additional requirements on q_lambda may apply.\n\n\n\n\n\n","category":"type"},{"location":"elbo/repgradelbo/#bijectors","page":"Reparameterization Gradient Estimator","title":"Handling Constraints with Bijectors","text":"","category":"section"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"As mentioned in the docstring, the RepGradELBO objective assumes that the variational approximation q_lambda and the target distribution pi have the same support for all lambda in Lambda.","category":"page"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"However, in general, it is most convenient to use variational families that have the whole Euclidean space mathbbR^d as their support. This is the case for the location-scale distributions provided by AdvancedVI. For target distributions which the support is not the full mathbbR^d, we can apply some transformation b to q_lambda to match its support such that","category":"page"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"z sim  q_blambda qquadLeftrightarrowqquad\nz stackreld= b^-1left(etaright)quad eta sim q_lambda","category":"page"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"where b is often called a bijector, since it is often chosen among bijective transformations. This idea is known as automatic differentiation VI[KTRGB2017] and has subsequently been improved by Tensorflow Probability[DLTBV2017]. In Julia, Bijectors.jl[FXTYG2020] provides a comprehensive collection of bijections.","category":"page"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"One caveat of ADVI is that, after applying the bijection, a Jacobian adjustment needs to be applied. That is, the objective is now","category":"page"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"mathrmADVIleft(lambdaright)\ntriangleq\nmathbbE_eta sim q_lambdaleft\n  log pileft( b^-1left( eta right) right)\n  + log lvert J_b^-1left(etaright) rvert\nright\n+ mathbbHleft(q_lambdaright)","category":"page"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"This is automatically handled by AdvancedVI through TransformedDistribution provided by Bijectors.jl. See the following example:","category":"page"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"using Bijectors\nq = MeanFieldGaussian(μ, L)\nb = Bijectors.bijector(dist)\nbinv = inverse(b)\nq_transformed = Bijectors.TransformedDistribution(q, binv)","category":"page"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"By passing q_transformed to optimize, the Jacobian adjustment for the bijector b is automatically applied. (See Examples for a fully working example.)","category":"page"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"[KTRGB2017]: Kucukelbir, A., Tran, D., Ranganath, R., Gelman, A., & Blei, D. M. (2017). Automatic differentiation variational inference. Journal of Machine Learning Research.","category":"page"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"[DLTBV2017]: Dillon, J. V., Langmore, I., Tran, D., Brevdo, E., Vasudevan, S., Moore, D., ... & Saurous, R. A. (2017). Tensorflow distributions. arXiv.","category":"page"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"[FXTYG2020]: Fjelde, T. E., Xu, K., Tarek, M., Yalburgi, S., & Ge, H. (2020,. Bijectors. jl: Flexible transformations for probability distributions. In Symposium on Advances in Approximate Bayesian Inference.","category":"page"},{"location":"elbo/repgradelbo/#entropygrad","page":"Reparameterization Gradient Estimator","title":"Entropy Estimators","text":"","category":"section"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"For the gradient of the entropy term, we provide three choices with varying requirements. The user can select the entropy estimator by passing it as a keyword argument when constructing the RepGradELBO objective.","category":"page"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"Estimator entropy(q) logpdf(q) Type\nClosedFormEntropy required  Deterministic\nMonteCarloEntropy  required Monte Carlo\nStickingTheLandingEntropy  required Monte Carlo with control variate","category":"page"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"The requirements mean that either Distributions.entropy or Distributions.logpdf need to be implemented for the choice of variational family. In general, the use of ClosedFormEntropy is recommended whenever possible. If entropy is not available, then StickingTheLandingEntropy is recommended. See the following section for more details.","category":"page"},{"location":"elbo/repgradelbo/#The-StickingTheLandingEntropy-Estimator","page":"Reparameterization Gradient Estimator","title":"The StickingTheLandingEntropy Estimator","text":"","category":"section"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"The StickingTheLandingEntropy, or STL estimator, is a control variate approach [RWD2017].","category":"page"},{"location":"elbo/repgradelbo/#AdvancedVI.StickingTheLandingEntropy","page":"Reparameterization Gradient Estimator","title":"AdvancedVI.StickingTheLandingEntropy","text":"StickingTheLandingEntropy()\n\nThe \"sticking the landing\" entropy estimator[RWD2017].\n\nRequirements\n\nThe variational approximation q implements logpdf.\nlogpdf(q, η) must be differentiable by the selected AD framework.\n\n\n\n\n\n","category":"type"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"It occasionally results in lower variance when pi approx q_lambda, and higher variance when pi notapprox q_lambda. The conditions for which the STL estimator results in lower variance is still an active subject for research.","category":"page"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"The main downside of the STL estimator is that it needs to evaluate and differentiate the log density of q_lambda, logpdf(q), in every iteration. Depending on the variational family, this might be computationally inefficient or even numerically unstable. For example, if q_lambda is a Gaussian with a full-rank covariance, a back-substitution must be performed at every step, making the per-iteration complexity mathcalO(d^3) and reducing numerical stability.","category":"page"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"The STL control variate can be used by changing the entropy estimator using the following object:","category":"page"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"Let us come back to the example in Examples, where a LogDensityProblem is given as model. In this example, the true posterior is contained within the variational family. This setting is known as \"perfect variational family specification.\" In this case, the RepGradELBO estimator with StickingTheLandingEntropy is the only estimator known to converge exponentially fast (\"linear convergence\") to the true solution.","category":"page"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"Recall that the original ADVI objective with a closed-form entropy (CFE) is given as follows:","category":"page"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"n_montecarlo = 16;\nb = Bijectors.bijector(model);\nbinv = inverse(b)\n\nq0_trans = Bijectors.TransformedDistribution(q0, binv)\n\ncfe = AdvancedVI.RepGradELBO(n_montecarlo)\nnothing","category":"page"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"The repgradelbo estimator can instead be created as follows:","category":"page"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"repgradelbo = AdvancedVI.RepGradELBO(\n    n_montecarlo; entropy=AdvancedVI.StickingTheLandingEntropy()\n);\nnothing","category":"page"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"(Image: )","category":"page"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"We can see that the noise of the repgradelbo estimator becomes smaller as VI converges. However, the speed of convergence may not always be significantly different. Also, due to noise, just looking at the ELBO may not be sufficient to judge which algorithm is better. This can be made apparent if we measure convergence through the distance to the optimum:","category":"page"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"(Image: )","category":"page"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"We can see that STL kicks-in at later stages of optimization. Therefore, when STL \"works\", it yields a higher accuracy solution even on large stepsizes. However, whether STL works or not highly depends on the problem[KMG2024]. Furthermore, in a lot of cases, a low-accuracy solution may be sufficient.","category":"page"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"[RWD2017]: Roeder, G., Wu, Y., & Duvenaud, D. K. (2017). Sticking the landing: Simple, lower-variance gradient estimators for variational inference. Advances in Neural Information Processing Systems, 30.","category":"page"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"[KMG2024]: Kim, K., Ma, Y., & Gardner, J. (2024). Linear Convergence of Black-Box Variational Inference: Should We Stick the Landing?. In International Conference on Artificial Intelligence and Statistics (pp. 235-243). PMLR.","category":"page"},{"location":"elbo/repgradelbo/#Advanced-Usage","page":"Reparameterization Gradient Estimator","title":"Advanced Usage","text":"","category":"section"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"There are two major ways to customize the behavior of RepGradELBO","category":"page"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"Customize the Distributions functions: rand(q), entropy(q), logpdf(q).\nCustomize AdvancedVI.reparam_with_entropy.","category":"page"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"It is generally recommended to customize rand(q), entropy(q), logpdf(q), since it will easily compose with other functionalities provided by AdvancedVI.","category":"page"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"The most advanced way is to customize AdvancedVI.reparam_with_entropy. In particular, reparam_with_entropy is the function that invokes rand(q), entropy(q), logpdf(q). Thus, it is the most general way to override the behavior of RepGradELBO.","category":"page"},{"location":"elbo/repgradelbo/#AdvancedVI.reparam_with_entropy","page":"Reparameterization Gradient Estimator","title":"AdvancedVI.reparam_with_entropy","text":"reparam_with_entropy(rng, q, q_stop, n_samples, ent_est)\n\nDraw n_samples from q and compute its entropy.\n\nArguments\n\nrng::Random.AbstractRNG: Random number generator.\nq: Variational approximation.\nq_stop: Same as q, but held constant during differentiation. Should only be used for computing the entropy.\nn_samples::Int: Number of Monte Carlo samples \nent_est: The entropy estimation strategy. (See estimate_entropy.)\n\nReturns\n\nsamples: Monte Carlo samples generated through reparameterization. Their support matches that of the target distribution.\nentropy: An estimate (or exact value) of the differential entropy of q.\n\n\n\n\n\n","category":"function"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"To illustrate how we can customize the rand(q) function, we will implement quasi-Monte-Carlo variational inference[BWM2018]. Consider the case where we use the MeanFieldGaussian variational family. In this case, it suffices to override its rand specialization as follows:","category":"page"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"using QuasiMonteCarlo\nusing StatsFuns\n\nqmcrng = SobolSample(; R=OwenScramble(; base=2, pad=32))\n\nfunction Distributions.rand(\n    rng::AbstractRNG, q::MvLocationScale{<:Diagonal,D,L}, num_samples::Int\n) where {L,D}\n    (; location, scale, dist) = q\n    n_dims = length(location)\n    scale_diag = diag(scale)\n    unif_samples = QuasiMonteCarlo.sample(num_samples, length(q), qmcrng)\n    std_samples = norminvcdf.(unif_samples)\n    return scale_diag .* std_samples .+ location\nend\nnothing","category":"page"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"(Note that this is a quick-and-dirty example, and there are more sophisticated ways to implement this.)","category":"page"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"By plotting the ELBO, we can see the effect of quasi-Monte Carlo. (Image: ) We can see that quasi-Monte Carlo results in much lower variance than naive Monte Carlo. However, similarly to the STL example, just looking at the ELBO is often insufficient to really judge performance. Instead, let's look at the distance to the global optimum:","category":"page"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"(Image: )","category":"page"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"QMC yields an additional order of magnitude in accuracy. Also, unlike STL, it ever-so slightly accelerates convergence. This is because quasi-Monte Carlo uniformly reduces variance, unlike STL, which reduces variance only near the optimum.","category":"page"},{"location":"elbo/repgradelbo/","page":"Reparameterization Gradient Estimator","title":"Reparameterization Gradient Estimator","text":"[BWM2018]: Buchholz, A., Wenzel, F., & Mandt, S. (2018). Quasi-monte carlo variational inference. In International Conference on Machine Learning.","category":"page"},{"location":"#AdvancedVI","page":"AdvancedVI","title":"AdvancedVI","text":"","category":"section"},{"location":"#Introduction","page":"AdvancedVI","title":"Introduction","text":"","category":"section"},{"location":"","page":"AdvancedVI","title":"AdvancedVI","text":"AdvancedVI provides implementations of variational Bayesian inference (VI) algorithms. VI algorithms perform scalable and computationally efficient Bayesian inference at the cost of asymptotic exactness. AdvancedVI is part of the Turing probabilistic programming ecosystem.","category":"page"},{"location":"#Provided-Algorithms","page":"AdvancedVI","title":"Provided Algorithms","text":"","category":"section"},{"location":"","page":"AdvancedVI","title":"AdvancedVI","text":"AdvancedVI currently provides the following algorithm for evidence lower bound maximization:","category":"page"},{"location":"","page":"AdvancedVI","title":"AdvancedVI","text":"Evidence Lower-Bound Maximization","category":"page"}]
}
